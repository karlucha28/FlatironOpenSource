window.COURSE_DATA = {"language":"en","lastDownload":"2022-10-31T12:02:46-04:00","title":"Phase 3","modules":[{"id":46936,"name":"Table of Contents","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g173bd31b8e9046963f1c590b42c58ad2","items":[{"id":455614,"title":"Table of Contents (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 21: Object-Oriented Programming and Scikit-Learn\" href=\"modules/g8aab1c17ecda1520340c32e7b2ecd632\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/45903\" data-api-returntype=\"Module\"\u003eTopic 21: Object-Oriented Programming and Scikit-Learn\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 21 Lesson Priorities (Live)\" href=\"pages/topic-21-lesson-priorities-live\"\u003eTopic 21 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 22: Linear Algebra and Calculus\" href=\"modules/ga1ec22255791208102870b00c9597631\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43417\" data-api-returntype=\"Module\"\u003eTopic 22: Linear Algebra and Calculus\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 22 Lesson Priorities (Live)\" href=\"pages/topic-22-lesson-priorities-live\"\u003eTopic 22 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 23: Machine Learning Fundamentals\" href=\"modules/g1853751b3adb03f437e97cf14f6a7a8b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/45904\" data-api-returntype=\"Module\"\u003eTopic 23: Machine Learning Fundamentals\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 23 Lesson Priorities (Live)\" href=\"pages/topic-23-lesson-priorities-live\"\u003eTopic 23 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 24: Logistic Regression\" href=\"modules/g3ca6a0af38c0b8fea8e33b3bcd24d8e1\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43420\" data-api-returntype=\"Module\"\u003eTopic 24: Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 24 Lesson Priorities (Live)\" href=\"pages/topic-24-lesson-priorities-live\"\u003eTopic 24 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 25: Classification Metrics\" href=\"modules/gec1b618791ced2ade978d083978119b0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43421\" data-api-returntype=\"Module\"\u003eTopic 25: Classification Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 25 Lesson Priorities (Live)\" href=\"pages/topic-25-lesson-priorities-live\"\u003eTopic 25 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 26: Decision Trees\" href=\"modules/g82789eaec1c2f4bc30fcb1651aaa6e3f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43422\" data-api-returntype=\"Module\"\u003eTopic 26: Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 26 Lesson Priorities (Live)\" href=\"pages/topic-26-lesson-priorities-live\"\u003eTopic 26 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 27: K Nearest Neighbors\" href=\"modules/g2a0d32eb8567428df5a74a26b5cbba7d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43423\" data-api-returntype=\"Module\"\u003eTopic 27: K Nearest Neighbors\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 27 Lesson Priorities (Live)\" href=\"pages/topic-27-lesson-priorities-live\"\u003eTopic 27 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 28: Bayes Classification\" href=\"modules/gccd76d19614efb1b6f9d4bc63d404deb\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43424\" data-api-returntype=\"Module\"\u003eTopic 28: Bayes Classification\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 28 Lesson Priorities (Live)\" href=\"pages/topic-28-lesson-priorities-live\"\u003eTopic 28 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 29: Model Tuning and Pipelines\" href=\"modules/gaab62e319239fc8a2aa6ed300350f65e\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43425\" data-api-returntype=\"Module\"\u003eTopic 29: Model Tuning and Pipelines\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 29 Lesson Priorities (Live)\" href=\"pages/topic-29-lesson-priorities-live\"\u003eTopic 29 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 30: Ensemble Methods\" href=\"modules/g1296f971fd4327def9ab6c54b8f6e0a4\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43426\" data-api-returntype=\"Module\"\u003eTopic 30: Ensemble Methods\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 30 Lesson Priorities (Live)\" href=\"pages/topic-30-lesson-priorities-live\"\u003eTopic 30 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca title=\"üèÜ Milestones\" href=\"modules/gbcbec1317916c833fa0f56e4889aaaaa\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43414\" data-api-returntype=\"Module\"\u003eüèÜ Milestones\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca title=\"APPENDIX: More OOP\" href=\"modules/gfd45e1b463d698d320c9d9f01d71f34b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43427\" data-api-returntype=\"Module\"\u003eAPPENDIX\u003c/a\u003e\u003c/p\u003e","exportId":"table-of-contents-live"}]},{"id":46937,"name":"Topic 21: Object-Oriented Programming and Scikit-Learn","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g8aab1c17ecda1520340c32e7b2ecd632","items":[{"id":455618,"title":"Topic 21 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan style=\"font-size: 18pt;\"\u003eWelcome to Phase 3!\u003c/span\u003e\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98.0716%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003ePython OOP\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth style=\"width: 82.0057%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 17.7609%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Object-Oriented Programming - Introduction\" href=\"pages/object-oriented-programming-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/object-oriented-programming-introduction\" data-api-returntype=\"Page\"\u003eObject-Oriented Programming - Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Classes and Instances\" href=\"assignments/g62af0481da1247e246f90c684e1972d0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197091\" data-api-returntype=\"Assignment\"\u003eClasses and Instances\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Classes and Instances - Lab\" href=\"assignments/g119766e414e353de35cb25c1e78483a1\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197092\" data-api-returntype=\"Assignment\"\u003eClasses and Instances - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Instance Methods\" href=\"assignments/g69af0808045f764ffe43b978fa3ccaf4\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197097\" data-api-returntype=\"Assignment\"\u003eInstance Methods\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Instance Methods - Lab\" href=\"assignments/gaf65bb50e8026994e8ca240f8354c3d2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197098\" data-api-returntype=\"Assignment\"\u003eInstance Methods - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"A Deeper Dive into \u0026quot;self\u0026quot;\" href=\"assignments/g381033ca9bd27a5adad694abdecab727\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197088\" data-api-returntype=\"Assignment\"\u003eA Deeper Dive into \"self\"\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Object Attributes - Lab\" href=\"assignments/gfbd4429ffbc0336acf92ade5373a1701\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197103\" data-api-returntype=\"Assignment\"\u003eObject Attributes - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Object Initialization\" href=\"assignments/g4d076a4b2a78d13ff48248cac54840a5\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197104\" data-api-returntype=\"Assignment\"\u003eObject Initialization\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Object Initialization - Lab\" href=\"assignments/ge3bb362ff99b1f27ecce7f46eab3b72c\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197105\" data-api-returntype=\"Assignment\"\u003eObject Initialization - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Quiz: Object-Oriented Programming\" href=\"quizzes/gcade38300ac325cdd762ba3921374c78\"\u003e\u003cstrong\u003eQuiz: Object-Oriented Programming\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98.0716%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003ePython OOP\u003c/em\u003e Lecture, Before \u003cem\u003eOOP with Scikit-Learn\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth style=\"width: 82.0057%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 17.7609%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Python OOP Exit Ticket\" href=\"quizzes/g099556e8c6fde026836f7808d2964805\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33592\" data-api-returntype=\"Quiz\"\u003e\u003cstrong\u003ePython OOP Exit Ticket\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Inheritance\" href=\"assignments/ge5a4685436b8c307dbd0e02842aa8bb3\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197095\" data-api-returntype=\"Assignment\"\u003eInheritance\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Inheritance - Lab\" href=\"assignments/ga7235058a8fe896a98b13eac29d0a628\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197096\" data-api-returntype=\"Assignment\"\u003eInheritance - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"OOP with Scikit-Learn\" href=\"assignments/gf90dbcad5fd270a68f87e199399d7cf4\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197101\" data-api-returntype=\"Assignment\"\u003eOOP with Scikit-Learn\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"OOP with Scikit-Learn - Lab\" href=\"assignments/gc6044b3c144d2460d591e93a47b50523\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197102\" data-api-returntype=\"Assignment\"\u003eOOP with Scikit-Learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98.0716%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eOOP with Scikit-Learn\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth style=\"width: 82.0057%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 17.7609%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"OOP with Scikit-Learn Exit Ticket\" href=\"quizzes/g2dc61175cd9d50f14d0654fc91001d78\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33588\" data-api-returntype=\"Quiz\"\u003eOOP with Scikit-Learn Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Short Video: Debugging with OOP\" href=\"pages/short-video-debugging-with-oop\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/short-video-debugging-with-oop\" data-api-returntype=\"Page\"\u003eShort Video: Debugging with OOP\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\" href=\"quizzes/g0c9a5bdde5f8af00ba4e30f35458c335\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33590\" data-api-returntype=\"Quiz\"\u003e‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Object-Oriented Programming - Recap\" href=\"pages/object-oriented-programming-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/object-oriented-programming-recap\" data-api-returntype=\"Page\"\u003eObject-Oriented Programming - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","exportId":"topic-21-lesson-priorities-live"},{"id":455619,"title":"Object-Oriented Programming - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-oop-intro-v2-4\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll be introduced to the concept of object-oriented programming (OOP) in Python. OOP has become a foundational practice in much of software development and programming, allowing developers to build upon each other's code in a fluent manner.\u003c/p\u003e\n\n\u003ch2\u003eProgramming Paradigms\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProgramming paradigms\u003c/em\u003e\u003c/strong\u003e are formal approaches for structuring code to achieve the desired results.\u003c/p\u003e\n\n\u003ch3\u003eWhy Do We Need Them?\u003c/h3\u003e\n\n\u003cp\u003eFor very simple programming tasks, there is essentially only one \"correct\" way to structure the code. For example, if you needed to print the string \"Hello, world!\", this is how you would do it:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Hello, world!\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eHello, world!\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBut once your code starts to get more complex, the structure gets less intuitive and obvious. For example, if you needed to reshape some data then display a bar graph, or fit a model then use it to make predictions, how would you design that?\u003c/p\u003e\n\n\u003cp\u003eDeciding on a paradigm and sticking to it helps to guide your code design choices, and helps others to understand what your code is doing.\u003c/p\u003e\n\n\u003ch3\u003eProcedural Programming\u003c/h3\u003e\n\n\u003cp\u003eThe oldest (and probably most intuitive) modern programming paradigm is procedural programming. This involves writing a series of sequential steps to be executed, possibly with the use of techniques for \u003cstrong\u003e\u003cem\u003econtrol flow\u003c/em\u003e\u003c/strong\u003e (e.g. \u003ccode\u003eif\u003c/code\u003e statements) and \u003cstrong\u003e\u003cem\u003emodular procedures\u003c/em\u003e\u003c/strong\u003e (e.g. functions).\u003c/p\u003e\n\n\u003cp\u003eData science code written in a \u003cstrong\u003enotebook\u003c/strong\u003e is almost always following a procedural programming paradigm. It is useful for telling a story with a single thread, but less useful for building libraries or software that runs without human intervention. Once code starts to get more complicated, we start incorporating more-complex paradigms such as functional programming or OOP.\u003c/p\u003e\n\n\u003ch3\u003eFunctional Programming\u003c/h3\u003e\n\n\u003cp\u003e\"Purely functional\" programming, using a language like Haskell or Clojure, means that procedural programming is abandoned entirely -- rather than a series of steps, the program consists only of functions, which in turn can be composed of functions or apply functions.\u003c/p\u003e\n\n\u003cp\u003eIn the development of data science libraries, they tended not to use purely functional programming, but nevertheless incorporated some functional principles.\u003c/p\u003e\n\n\u003cp\u003eFor example, here is the functional interface to Matplotlib:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eplt\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efigure\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"My Graph\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003exlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"x Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eylabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"y Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/raw/master/index_files/index_6_0.png\" alt=\"png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNote that we created this graph without instantiating any variables. We just imported the library, then called a series of functions to create the desired graph. We could rewrite that code snippet like this, to make that aspect even clearer:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003efigure\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003exlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eylabel\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003efigure\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"My Graph\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003exlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"x Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eylabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"y Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/raw/master/index_files/index_8_0.png\" alt=\"png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis approach is still preferred by some \"old school\" data science practitioners, but it has some issues.\u003c/p\u003e\n\n\u003cp\u003eIt uses \u003cstrong\u003e\u003cem\u003eglobal variables\u003c/em\u003e\u003c/strong\u003e, which can get messy as code gets more complex. When the \u003ccode\u003etitle()\u003c/code\u003e function is called in the above snippet, for example, the internal logic first has to find the current global axes object, then apply the label to that object. For a programmer to understand what axes object that is, they would need to closely follow the steps of the code, since there is no unique variable assigned to it. With no variable assigned, that also means that the code is less flexible and steps must be performed \u003cstrong\u003e\u003cem\u003eone at a time\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eObject-Oriented Programming (OOP)\u003c/h3\u003e\n\n\u003cp\u003eObject-oriented programming takes these global variables and functions and makes them into \"member variables\" (AKA \u003cstrong\u003e\u003cem\u003eattributes\u003c/em\u003e\u003c/strong\u003e) and \"member functions\" (AKA \u003cstrong\u003e\u003cem\u003emethods\u003c/em\u003e\u003c/strong\u003e). This allows code to be more organized and clear.\u003c/p\u003e\n\n\u003cp\u003eFor example, in the previous functional Matplotlib example, you might ask \u003cem\u003eWhat is \u003ccode\u003etitle()\u003c/code\u003e being called on? Is it the figure or the axes?\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eTo answer this, we could look at the \u003ca href=\"https://github.com/matplotlib/matplotlib/blob/v3.5.1/lib/matplotlib/pyplot.py#L3024-L3027\"\u003eMatplotlib source code\u003c/a\u003e, which shows this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efontdict\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epad\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003egca\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eset_title\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efontdict\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003efontdict\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epad\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003epad\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003egca()\u003c/code\u003e means \"get current axes\", so we can tell that this is being applied to the axes.\u003c/p\u003e\n\n\u003cp\u003eOr if we use the object-oriented Matplotlib interface instead, the answer becomes much clearer, just by looking at our code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eplt\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003efig\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eax\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubplots\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_title\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"My Graph\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_xlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"x Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_ylabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"y Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/raw/master/index_files/index_11_0.png\" alt=\"png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see, the title is being applied to the axes, not the figure. We can tell because the method call is structured like \u003ccode\u003eax.\u0026lt;method name\u0026gt;()\u003c/code\u003e and \u003ccode\u003eax\u003c/code\u003e is our axes variable.\u003c/p\u003e\n\n\u003cp\u003eA key takeaway here is that \u003cstrong\u003e\u003cem\u003eyou can often do the exact same thing using different paradigms\u003c/em\u003e\u003c/strong\u003e. They are just different approaches to structuring code, and different people might prefer different approaches.\u003c/p\u003e\n\n\u003ch2\u003eOOP Topics\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we will cover:\u003c/p\u003e\n\n\u003ch3\u003eClasses and Instances\u003c/h3\u003e\n\n\u003cp\u003eA Python class can be thought of as the blueprint for creating a code object. These objects are known as an instance objects or instances. We'll go over how to create classes as well as instances.\u003c/p\u003e\n\n\u003ch3\u003eMethods and Attributes\u003c/h3\u003e\n\n\u003cp\u003eNext, we'll dive deeper into how to specify and invoke the functions and variables that are \"bound\" to instance objects. This includes the \u003cstrong\u003e\u003cem\u003eencapsulation\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eabstraction\u003c/em\u003e\u003c/strong\u003e principles of OOP.\u003c/p\u003e\n\n\u003ch3\u003eInheritance\u003c/h3\u003e\n\n\u003cp\u003eInheritance means that classes can be defined that take on the traits of other classes. This is especially useful when interacting with complex code libraries.\u003c/p\u003e\n\n\u003ch3\u003eOOP and Scikit-Learn\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"https://github.com/scikit-learn/scikit-learn/raw/main/doc/logos/scikit-learn-logo.png\" alt=\"scikit-learn logo\"\u003e\u003c/p\u003e\n\n\u003cp\u003eScikit-learn is the most popular machine learning library in use today, and its organization relies heavily on object-oriented programming. We'll go over the types of classes used and some of the most common methods and attributes you should know about.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eObject-oriented programming (OOP) is a way of organizing your code that can make many types of applications easier to write by combining related variables/properties and functions/methods into objects containing both behavior and state.\u003c/p\u003e","exportId":"object-oriented-programming-introduction"},{"id":455622,"title":"Classes and Instances","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classes-and-instances\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll take a look at class and instance objects in Python and how to create them. A Python class can be thought of as the blueprint for creating a code object (or \u003cstrong\u003einstance object\u003c/strong\u003e). It has both the layout for new objects as well as the ability to create those objects. When you \u003cstrong\u003einitialize\u003c/strong\u003e or make a new instance object from a class, you are essentially pressing a button on an assembly line that instantly rolls out a new instance object. For example, if you were dealing with a \u003ccode\u003eCar\u003c/code\u003e class, you would get a brand new car from the assembly line. In cases where you want to create multiple objects, you can see how this functionality would be extremely useful.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe a class and how it can be used to create objects \u003c/li\u003e\n\u003cli\u003eDescribe an instance object \u003c/li\u003e\n\u003cli\u003eCreate an instance of a class \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g62af0481da1247e246f90c684e1972d0"},{"id":455623,"title":"Classes and Instances - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classes-and-instances-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eOkay, you've learned how to declare classes and create instances in the last lesson. Now it's time to put these new skills to the test!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate an instance of a class\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g119766e414e353de35cb25c1e78483a1"},{"id":455624,"title":"Instance Methods","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-instance-methods\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you know what classes and instances are, we can talk about instance methods. Instance methods are almost the same as regular functions in Python. The key difference is that an instance method is defined inside of a class and bound to instance objects of that class. Instance methods can be thought of as an attribute of an instance object. The difference between an instance method and another attribute of an instance is that instance methods are \u003cem\u003ecallable\u003c/em\u003e, meaning they execute a block of code. This may seem a bit confusing, but try to think about instance methods as functions defined in a class that are really just attributes of an instance object from that class.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare instance methods and attributes\u003c/li\u003e\n\u003cli\u003eDefine and call an instance method\u003c/li\u003e\n\u003cli\u003eDefine instance attributes\u003c/li\u003e\n\u003cli\u003eExplain the \u003ccode\u003eself\u003c/code\u003e variable and its relation to instance objects\u003c/li\u003e\n\u003cli\u003eCreate an instance of a class \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g69af0808045f764ffe43b978fa3ccaf4"},{"id":455626,"title":"Instance Methods - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-instance-methods-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson, you learned about instance methods -- what they are and how to define them. In this lab, you are going to flesh out the \u003ccode\u003eDriver\u003c/code\u003e and \u003ccode\u003ePassenger\u003c/code\u003e classes by writing your own instance methods for these classes.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate an instance of a class \u003c/li\u003e\n\u003cli\u003eDefine and call an instance method\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gaf65bb50e8026994e8ca240f8354c3d2"},{"id":455629,"title":"A Deeper Dive into self","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-understanding-self\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn a little more about \u003ccode\u003eself\u003c/code\u003e in object-oriented programming (OOP) in Python. You've seen a little bit about \u003ccode\u003eself\u003c/code\u003e when you learned about defining and calling instance methods. So far you've seen that \u003ccode\u003eself\u003c/code\u003e is always explicitly defined as the instance method's \u003cstrong\u003efirst parameter\u003c/strong\u003e. You've also seen that instance methods implicitly use the instance object as the \u003cstrong\u003efirst argument\u003c/strong\u003e when you call the method. By convention, you name this first parameter \u003ccode\u003eself\u003c/code\u003e since it is a reference to the object on which you are operating. Let's take a look at some code that uses \u003ccode\u003eself\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the \u003ccode\u003eself\u003c/code\u003e variable and its relation to instance objects\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g381033ca9bd27a5adad694abdecab727"},{"id":455631,"title":"Object Attributes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-object-attributes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice defining classes and instance methods. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine and call an instance method\u003c/li\u003e\n\u003cli\u003eDefine and access instance attributes\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gfbd4429ffbc0336acf92ade5373a1701"},{"id":455632,"title":"Object Initialization","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-object-initialization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you've begun to see OOP and class structures, it's time to investigate the \u003ccode\u003e__init__\u003c/code\u003e method more. The \u003ccode\u003e__init__\u003c/code\u003e method allows classes to have default behaviors and attributes. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate instance variables in the \u003ccode\u003e__init__\u003c/code\u003e method\u003c/li\u003e\n\u003cli\u003eUse default arguments in the \u003ccode\u003e__init__\u003c/code\u003e method \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g4d076a4b2a78d13ff48248cac54840a5"},{"id":455634,"title":"Object Initialization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-object-initialization-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice defining classes with \u003ccode\u003e__init__\u003c/code\u003e methods. You'll define two classes, \u003ccode\u003eDriver\u003c/code\u003e and \u003ccode\u003ePassenger\u003c/code\u003e in the cells below. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate instance variables in the \u003ccode\u003e__init__\u003c/code\u003e method\u003c/li\u003e\n\u003cli\u003eUse default arguments in the \u003ccode\u003e__init__\u003c/code\u003e method\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","exportId":"ge3bb362ff99b1f27ecce7f46eab3b72c"},{"id":455635,"title":"Quiz: Object-Oriented Programming","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g78dbcc31faa2ce4fe50021d6665dec53","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"gcade38300ac325cdd762ba3921374c78"},{"id":455644,"title":"Inheritance","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-inheritance\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about how you can use inheritance to create relationships between \u003cstrong\u003e\u003cem\u003eSuperclasses\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eSubclasses\u003c/em\u003e\u003c/strong\u003e to further save you from writing redundant code!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a domain model using OOP \u003c/li\u003e\n\u003cli\u003eUse inheritance to write nonredundant code \u003c/li\u003e\n\u003cli\u003eDescribe the relationship between subclasses and superclasses \u003c/li\u003e\n\u003c/ul\u003e","exportId":"ge5a4685436b8c307dbd0e02842aa8bb3"},{"id":455646,"title":"Inheritance - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-inheritance-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll use what you've learned about inheritance to model a zoo using superclasses, subclasses, and maybe even an abstract superclass!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a domain model using OOP \u003c/li\u003e\n\u003cli\u003eUse inheritance to write nonredundant code \u003c/li\u003e\n\u003c/ul\u003e","exportId":"ga7235058a8fe896a98b13eac29d0a628"},{"id":455648,"title":"OOP with Scikit-Learn","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-oop-sklearn\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs you learn more about machine learning algorithms, there are typically two components. First, the conceptual underlying logic of the algorithm -- how it works to process inputs and generate outputs. Second, the scikit-learn implementation of the algorithm -- how to use it in practice.\u003c/p\u003e\n\n\u003cp\u003eBefore diving into specific examples of various scikit-learn models, it is helpful to understand the general structure they follow. Specifically, we'll go over some key classes, methods, and attributes common to scikit-learn.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecall the distinction between mutable and immutable types\u003c/li\u003e\n\u003cli\u003eDefine the four main inherited object types in scikit-learn\u003c/li\u003e\n\u003cli\u003eInstantiate scikit-learn transformers and models\u003c/li\u003e\n\u003cli\u003eInvoke scikit-learn methods\u003c/li\u003e\n\u003cli\u003eAccess scikit-learn attributes\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gf90dbcad5fd270a68f87e199399d7cf4"},{"id":455650,"title":"OOP with Scikit-Learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-oop-sklearn-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have learned some of the basics of object-oriented programming with scikit-learn, let's practice applying it!\u003c/p\u003e\n\n\u003ch2\u003eObjectives:\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you will practice:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecall the distinction between mutable and immutable types\u003c/li\u003e\n\u003cli\u003eDefine the four main inherited object types in scikit-learn\u003c/li\u003e\n\u003cli\u003eInstantiate scikit-learn transformers and models\u003c/li\u003e\n\u003cli\u003eInvoke scikit-learn methods\u003c/li\u003e\n\u003cli\u003eAccess scikit-learn attributes\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gc6044b3c144d2460d591e93a47b50523"},{"id":455657,"title":"OOP with Scikit-Learn Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g2a8a68e2b1d6f93063734ba508d417fa","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g2dc61175cd9d50f14d0654fc91001d78"},{"id":455659,"title":"Short Video: Debugging with OOP","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802485?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-debugging-with-oop"},{"id":455663,"title":"‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"gaefb27c067a8c8a58e822cf9b93b1236","questionCount":1,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_submit","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sklearn-preprocessing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sklearn-preprocessing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure how to clone the source code, refer to the instructions here: \u003ca href=\"https://github.com/learn-co-curriculum/dsc-running-jupyter-locally\" target=\"_blank\"\u003ehttps://github.com/learn-co-curriculum/dsc-running-jupyter-locally\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003eSubmission Instructions\u003c/h3\u003e\n\u003cp\u003eWhen you are finished with the lab, complete the following steps to submit your work:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSave the changes to the notebook by clicking the Save icon, shown below highlighted in red\u003cbr\u003e\u003cimg src=\"https://learning.flatironschool.com/courses/5539/file_contents/viewer/files/Uploaded%20Media/Screen%20Shot%202021-07-28%20at%205.41.06%20PM.png\" alt=\"Screen Shot 2021-07-28 at 5.41.06 PM.png\"\u003e\u0026nbsp;\u003c/li\u003e\n\u003cli\u003eClose the notebook browser tab(s)\u003c/li\u003e\n\u003cli\u003eShut down the notebook server by typing control-C in the terminal window where it is currently running\u003c/li\u003e\n\u003cli\u003eCommit your changes in Git by typing \u003cbr\u003e\u003ccode\u003egit commit -am \"Finished lab\"\u003c/code\u003e \u003cbr\u003ein the terminal and hitting Enter\u003c/li\u003e\n\u003cli\u003ePush your changes to GitHub by typing \u003cbr\u003e\u003ccode\u003egit push origin master\u003c/code\u003e \u003cbr\u003ein the terminal and hitting Enter\u003c/li\u003e\n\u003cli\u003eOpen the GitHub view of your fork of the lab in the browser. For example, if your username were \u003ccode\u003ehoffm386\u003c/code\u003e, you would go to \u003ca href=\"https://github.com/hoffm386/dsc-data-serialization-lab\" target=\"_blank\"\u003ehttps://github.com/hoffm386/dsc-data-serialization-lab\u003c/a\u003e in the browser for this particular lab. Click on \u003ccode\u003eindex.ipynb\u003c/code\u003e and double-check that your code updates are there. (The updates will not be in the README, only in the \u003ccode\u003e.ipynb\u003c/code\u003e file.)\u003c/li\u003e\n\u003cli\u003eSubmit the link to your fork of the lab in the textbox on Canvas\u003cbr\u003e\u0026nbsp;\u003cimg src=\"https://learning.flatironschool.com/courses/5539/file_contents/viewer/files/Uploaded%20Media/Screen%20Shot%202021-08-24%20at%206.42.54%20PM.png\" alt=\"Screen Shot 2021-08-24 at 6.42.54 PM.png\" width=\"319\" height=\"320\"\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eTroubleshooting\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"background-color: #fbeeb8;\"\u003eIf you are able to submit the URL successfully, you do not need to follow the below steps!\u003c/span\u003e\u003c/p\u003e\n\u003ch4\u003eNot a Git Repository\u003c/h4\u003e\n\u003cp\u003eIf you try to run \u003ccode\u003egit commit -am \"Finished lab\"\u003c/code\u003e and get the error message \u003ccode\u003efatal: not a git repository\u003c/code\u003e, double-check that you are running the code from the correct directory. If you type \u003ccode\u003epwd\u003c/code\u003e in the terminal and hit Enter, the path that is printed out should include the directory of the lab ‚Äî in this case, \u003ccode\u003edsc-data-serialization-lab\u003c/code\u003e. For example, a valid path would be \u003ccode\u003e/Users/myname/Development/DS/dsc-data-serialization-lab\u003c/code\u003e, since that ends with the lab directory, whereas \u003ccode\u003e/Users/myname/Development/DS/\u003c/code\u003e would not be a valid path. Use commands like \u003ccode\u003els\u003c/code\u003e and \u003ccode\u003ecd\u003c/code\u003e to navigate to the appropriate directory, then continue with the steps above, starting with step 4.\u003c/p\u003e\n\u003ch4\u003ePermission Denied\u003c/h4\u003e\n\u003cp\u003eIf you try to run \u003ccode\u003egit push origin master\u003c/code\u003e and get a \u003ccode\u003ePermission denied\u003c/code\u003e error message, you are likely trying to push to the curriculum version of the lab, not your personal fork. Follow these steps to fix this:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIn the browser, go to the curriculum repository for this lab by clicking the \u003cimg src=\"viewer/files/Uploaded%20Media/GitHub-Mark-32px.png\" alt=\"GitHub octocat logo\"\u003e\u0026nbsp;icon above\u003c/li\u003e\n\u003cli\u003eClick the Fork button. If you already have a fork, this will take you to it. If you haven't made a fork yet, this will make the fork and take you to it\u003c/li\u003e\n\u003cli\u003eOn the page of your fork, copy the clone link. For example, \u003ca href=\"https://github.com/hoffm386/dsc-data-serialization-lab.git\" target=\"_blank\"\u003ehttps://github.com/hoffm386/dsc-data-serialization-lab.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eBack in the terminal where you were trying to run \u003ccode\u003egit push\u003c/code\u003e, type \u003cbr\u003e\u003ccode\u003egit remote add myfork \u0026lt;URL\u0026gt;\u003c/code\u003e \u003cbr\u003eWhere \u003ccode\u003e\u0026lt;URL\u0026gt;\u003c/code\u003e is replaced with the clone link you copied. For example, \u003ccode\u003egit remote add myfork https://github.com/hoffm386/dsc-data-serialization-lab.git\u003c/code\u003e. Then hit Enter. This means you have created a connection between your local repository and your fork\u003c/li\u003e\n\u003cli\u003eNow, push your code to your fork by typing \u003cbr\u003e\u003ccode\u003egit push myfork master\u003c/code\u003e \u003cbr\u003ein the terminal and hitting Enter\u003c/li\u003e\n\u003cli\u003eProceed with the steps above, starting with step 6\u003c/li\u003e\n\u003c/ol\u003e","exportId":"g0c9a5bdde5f8af00ba4e30f35458c335"},{"id":455666,"title":"Object-Oriented Programming - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you learned about Object-oriented programming (OOP) as a foundational practice for software development and programming.\u003c/p\u003e\n\u003ch2\u003eOOP overview\u003c/h2\u003e\n\u003cp\u003eYou now know all about OOP and how to define classes. Like functions, using classes in your programming can save you a lot of time by eliminating repetitive tasks. Classes go further than functions by allowing you to persist data. After all, class methods are fairly analogous to functions, while attributes add functionality by acting as data storage containers.\u003c/p\u003e\n\u003ch2\u003eClass structure\u003c/h2\u003e\n\u003cp\u003eAs you saw, the most basic class definition starts off with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there, you then saw how you can further define class methods:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n    def method_1(self):\n        pass # Ideally a method does something, but you get the point\n    def method_2(self):\n        print('This is a pretty useless second method.')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou also learned about \u003ccode\u003eself\u003c/code\u003e. Specifically, that \u003ccode\u003eself\u003c/code\u003e is the default parameter used to define methods. This is necessary since instance methods implicitly pass in the object itself as an argument during execution.\u003c/p\u003e\n\u003ch2\u003eCreating instances\u003c/h2\u003e\n\u003cp\u003eRecall that after you define a class, you can then create instances of that class to bring it to life and use it! You're probably wondering what all of this has to do with data science. In turns out you'll use OOP principles when you start working with common data science libraries. For example, you might import the \u003ccode\u003eLinearRegression\u003c/code\u003e class from the scikit-learn library in order to create a regression model!\u003c/p\u003e\n\u003cp\u003eRemember, creating an instance of a class would look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.linear_model import LinearRegression() \n\nreg = LinearRegression() \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you create an instance object of the class, you can then use all the methods associated with that class!\u003c/p\u003e\n\u003ch2\u003eLevel up\u003c/h2\u003e\n\u003cp\u003eIf you would like to dive deeper into OOP and learn some advanced topics, you can check out the additional OOP lessons and labs in the Appendix.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eCongrats, you now have a solid foundation in OOP! You first learned how to define a class and methods. Then you learned how to create an instance of a class and define instance attributes. These skills will be useful for collaboration and writing concise, modular code!\u003c/p\u003e","exportId":"object-oriented-programming-recap"}]},{"id":46947,"name":"Topic 22: Linear Algebra and Calculus","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"ga1ec22255791208102870b00c9597631","items":[{"id":455673,"title":"Topic 22 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003eIn the Live program, there is no scheduled lecture on Calculus and Cost Functions; instead there is a recorded video lecture. Take the time to watch the lecture recording if you would like a deeper dive into or refresher on the calculus content.\u003c/span\u003e\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.719%; height: 305px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eLinear Algebra\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Linear Algebra and Calculus - Introduction\" href=\"pages/linear-algebra-and-calculus-introduction\"\u003eLinear Algebra and Calculus - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Motivation for Linear Algebra in Data Science\" href=\"pages/motivation-for-linear-algebra-in-data-science\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/motivation-for-linear-algebra-in-data-science\" data-api-returntype=\"Page\"\u003eMotivation for Linear Algebra in Data Science\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Systems of Linear Equations\" href=\"pages/systems-of-linear-equations\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/systems-of-linear-equations\" data-api-returntype=\"Page\"\u003eSystems of Linear Equations\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Systems of Linear Equations - Lab\" href=\"assignments/g6cafbb465925915959d5223bf0942753\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187049\" data-api-returntype=\"Assignment\"\u003eSystems of Linear Equations - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Scalars, Vectors, Matrices, and Tensors - Code Along\" href=\"assignments/gfea549a153a77fb9593814c7ea567a9d\"\u003eScalars, Vectors, Matrices, and Tensors - Code Along\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\"\u003e\u003cspan style=\"color: #000000;\"\u003e\u003cstrong\u003e 1st\u003c/strong\u003e\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Matrix Multiplication - Code Along\" href=\"assignments/g1f9f8e83538aa1ccd15d2d9b0de0394c\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187012\" data-api-returntype=\"Assignment\"\u003eMatrix Multiplication - Code Along\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\"\u003e\u003cspan data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\" data-sheets-userformat=\"{\u0026quot;2\u0026quot;:4224,\u0026quot;10\u0026quot;:2,\u0026quot;15\u0026quot;:\u0026quot;Arial\u0026quot;}\"\u003e 2nd\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003ca title=\"Short Video: Matrix Multiplication\" href=\"pages/short-video-matrix-multiplication\"\u003eShort Video: Matrix Multiplication\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cspan data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\" data-sheets-userformat=\"{\u0026quot;2\u0026quot;:4224,\u0026quot;10\u0026quot;:2,\u0026quot;15\u0026quot;:\u0026quot;Arial\u0026quot;}\"\u003e2nd\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Solving Systems of Linear Equations with NumPy - Code Along\" href=\"assignments/g8345898d9a24b82069d1f41709ee0fdd\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187047\" data-api-returntype=\"Assignment\"\u003eSolving Systems of Linear Equations with NumPy - Code Along\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Solving Systems of Linear Equations with NumPy - Lab\" href=\"assignments/g601c542580c65fb6de1d04b8bb0a30ef\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187048\" data-api-returntype=\"Assignment\"\u003eSolving Systems of Linear Equations with NumPy - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Regression Analysis using Linear Algebra and NumPy - Code Along\" href=\"assignments/gf03dc6212b7f20a3571de75b90fb5ccc\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187035\" data-api-returntype=\"Assignment\"\u003eRegression Analysis using Linear Algebra and NumPy - Code Along\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Regression with Linear Algebra - Lab\" href=\"assignments/gce6931adc68361b10a894c60cafbf6f7\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187039\" data-api-returntype=\"Assignment\"\u003eRegression with Linear Algebra - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Computational Complexity: From OLS to Gradient Descent\" href=\"pages/computational-complexity-from-ols-to-gradient-descent\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/computational-complexity-from-ols-to-gradient-descent\" data-api-returntype=\"Page\"\u003eComputational Complexity: From OLS to Gradient Descent\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003csup\u003e1\u003c/sup\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Linear Algebra\" href=\"quizzes/gc405785850874cf6a53b7923e4458e4d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30646\" data-api-returntype=\"Quiz\"\u003eQuiz: Linear Algebra\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003csup\u003e1\u003c/sup\u003eThis is rated as 2nd priority for progress within the DS program, but we encourage you to study this in preparation for technical interviews\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6253%; height: 71px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eLinear Algebra\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eCalculus and Cost Functions\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Linear Algebra Exit Ticket\" href=\"quizzes/g62b3e5e4cbc8970111617a88cdab7801\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30657\" data-api-returntype=\"Quiz\"\u003eLinear Algebra Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Derivatives\" href=\"assignments/g9777bfbacb7532d96452a0bc04a904b5\"\u003eIntroduction to Derivatives\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Introduction To Derivatives - Lab\" href=\"assignments/ga3c10c23d3d9e1151dccb8cb9437a1f4\"\u003eIntroduction To Derivatives - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003cstrong\u003e\u003ca title=\"Derivatives of Non-Linear Functions\" href=\"assignments/g3096b7ec24bf7a02135c7a720a776e2d\"\u003eDerivatives of Non-Linear Functions\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003ca title=\"Rules for Derivatives\" href=\"assignments/g240536dee629c9b857d210808dac71ec\"\u003eRules for Derivatives\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003cstrong\u003e\u003ca title=\"Derivatives: Conclusion\" href=\"assignments/gc0e76e1a6e29d946d6f18e9aacad1c3d\"\u003eDerivatives: Conclusion\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6253%; height: 290px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eCalculus and Cost Functions\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eGradient Descent\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Calculus and Cost Functions Exit Ticket\" href=\"quizzes/gb3db5cbf0bcdb11aabfa93891bfac5b0\"\u003eCalculus and Cost Functions Exit Ticket\u003c/a\u003e\u003ca title=\"Introduction to Gradient Descent\" href=\"assignments/g0eaaa285661d545beda76b6a1d97b302\"\u003e\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Gradient Descent\" href=\"assignments/g0eaaa285661d545beda76b6a1d97b302\"\u003eIntroduction to Gradient Descent\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient Descent: Step Sizes\" href=\"assignments/g508c94e5f4410b774066661cb6893554\"\u003eGradient Descent: Step Sizes\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient Descent: Step Sizes - Lab\" href=\"assignments/gafebbe6171e387ed7e205bba80badcc0\"\u003eGradient Descent: Step Sizes - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient Descent in 3D\" href=\"assignments/g3929ccf412b073ec731206e2dce84453\"\u003eGradient Descent in 3D\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\" The Gradient in Gradient Descent\" href=\"pages/the-gradient-in-gradient-descent\"\u003e The Gradient in Gradient Descent\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Short Video: The Gradient\" href=\"pages/short-video-the-gradient\"\u003eShort Video: The Gradient\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient to Cost Function\" href=\"assignments/gc5264c4c0c07fc228d3a316350dc8504\"\u003eGradient to Cost Function\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Applying Gradient Descent - Lab\" href=\"assignments/gf1d5c6a7ab52e0d588b4b7db4b395574\"\u003eApplying Gradient Descent - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6253%; height: 71px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eGradient Descent\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Gradient Descent Exit Ticket\" href=\"quizzes/gc53fdbad8c7fa83c61a7b59363bb80bf\"\u003eGradient Descent Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Linear Algebra and Calculus - Recap\" href=\"pages/linear-algebra-and-calculus-recap\"\u003eLinear Algebra and Calculus - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","exportId":"topic-22-lesson-priorities-live"},{"id":455676,"title":"Linear Algebra and Calculus - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-algebra-and-calculus-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to take a step back to learn some of the basics of the math that powers the most popular machine learning models. You may not need deep knowledge of linear algebra and calculus just to build a scikit-learn model, but this introduction should give you a better understanding of how your models are working \"under the hood\".\u003c/p\u003e\n\n\u003ch2\u003eLinear Algebra\u003c/h2\u003e\n\n\u003cp\u003eLinear Algebra is so foundational to machine learning that you're going to see it referenced many times as the course progresses. In this section, the goal is to give you both a theoretical introduction and some computational practice, solving a real-life problem by writing the code required to solve a linear regression using OLS.\u003c/p\u003e\n\n\u003cp\u003eWe're going to kick this section off by looking at some of the many places that linear algebra is used in machine learning - from deep learning to natural language processing (NLP) to dimensionality reduction techniques such as principal component analysis (PCA).\u003c/p\u003e\n\n\u003ch3\u003eSystems of Linear Equations\u003c/h3\u003e\n\n\u003cp\u003eWe then start to dig into the math! We look at the idea of linear simultaneous equations - a set of two or more equations each of which is linear (can be plotted on a graph as a straight line). We then see how such equations can be represented as vectors or matrices to represent such systems efficiently.\u003c/p\u003e\n\n\u003ch3\u003eScalars, Vectors, Matrices, and Tensors\u003c/h3\u003e\n\n\u003cp\u003eIn a code along, we'll introduce the concepts and concrete representations (in NumPy) of scalars, vectors, matrices, and tensors - why they are important and how to create them. \u003c/p\u003e\n\n\u003ch3\u003eVector/Matrix Operations\u003c/h3\u003e\n\n\u003cp\u003eWe then start to build up the basic skills required to perform matrix operations such as addition and multiplication.  You will also cover key techniques used by many machine learning models to perform their calculations covering both the Hadamard product and the (more common) dot product. \u003c/p\u003e\n\n\u003ch3\u003eSolving Systems of Linear Equations Using NumPy\u003c/h3\u003e\n\n\u003cp\u003eWe then bring the previous work together to look at how to use NumPy to solve systems of linear equations, introducing the identity and inverse matrices along the way.\u003c/p\u003e\n\n\u003ch3\u003eRegression Analysis Using Linear Algebra and NumPy\u003c/h3\u003e\n\n\u003cp\u003eHaving built up a basic mathematical and computational foundation for linear algebra, you will solve a real data problem - looking at how to use NumPy to solve a linear regression using the ordinary least squares (OLS) method.\u003c/p\u003e\n\n\u003ch3\u003eComputational Complexity\u003c/h3\u003e\n\n\u003cp\u003eIn the last linear algebra lesson, we look at the idea of computational complexity and Big O notation, showing why OLS is computationally inefficient, and that a gradient descent algorithm can instead be used to solve a linear regression much more efficiently.\u003c/p\u003e\n\n\u003ch2\u003eCalculus and Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eNext, you'll learn about the mechanism behind many machine learning optimization algorithms: gradient descent! Along the way, we'll also look at cost functions and will provide a foundation in calculus that will be valuable to you throughout your career as a data scientist.\u003c/p\u003e\n\n\u003cp\u003eJust as we used solving a linear regression using OLS as an excuse to introduce you to linear algebra, we're now using the idea of gradient descent to introduce enough calculus to both understand and have good intuitions about many of the machine learning models that you're going to learn throughout the rest of the course.\u003c/p\u003e\n\n\u003ch3\u003eAn Introduction to Calculus and Derivatives\u003c/h3\u003e\n\n\u003cp\u003eWe're going to start off by introducing derivatives - the \"instantaneous rate of change of a function\" or (more graphically) the \"slope of a curve\". We'll start off by looking at how to calculate the slope of a curve for a straight line, and then we'll explore how to calculate the rate of change for more complex (non-linear) functions.\u003c/p\u003e\n\n\u003ch3\u003eGradient Descent\u003c/h3\u003e\n\n\u003cp\u003eNow that we know how to calculate the slope of a curve - and, by extension, to find a local minimum (low point) or maximum (high point) where the curve is flat (the slope of the curve is zero), we'll look at the idea of a gradient descent to step from some random point on a cost curve to find the local optima to solve for a given linear equation. We'll also look at how best to select the step sizes for descending the cost function, and how to use partial derivatives to optimize both slope and offset to more effectively solve a linear regression using gradient descent.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIt is possible to use machine learning models such as scikit-learn without understanding the underlying math, but a basic understanding of this math will help you to make the most informed choices about which models to use based on your particular project context.\u003c/p\u003e","exportId":"linear-algebra-and-calculus-introduction"},{"id":455678,"title":"Motivation for Linear Algebra in Data Science","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about algebra as a foundational step for data science, and later on statistics. Linear algebra is also very important when moving on to machine learning models, where a solid understanding of linear equations plays a major role. This lesson will attempt to present some motivational examples of how and why a solid foundation of linear algebra is valuable for data scientists.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eState the importance of linear algebra in the fields of data science and machine learning \u003c/li\u003e\n\u003cli\u003eDescribe the areas in AI and machine learning where linear algebra might be used for advanced analytics \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eWhile having a deep understanding of linear algebra may not be mandatory, some basic knowledge is undoubtedly extremely helpful in your journey towards becoming a data scientist.\u003c/p\u003e\n\n\u003cp\u003eYou may already know a number of linear algebraic concepts without even knowing it. Examples are: matrix multiplication and dot-products. Later on, you'll learn more complex algebraic concepts like the calculation of matrix determinants, cross-products, and eigenvalues/eigenvectors. As a data scientist, it is important to know some of the theories as well as having a practical understanding of these concepts in a real-world setting.\u003c/p\u003e\n\n\u003ch2\u003eAn analogy\u003c/h2\u003e\n\n\u003cp\u003eThink of a simple example where you first learn about a sine function as an infinite polynomial while learning trigonometry. Students usually practice this function by passing different values to this function and getting the expected results and then manage to relate this to triangles and vertices. When learning advanced physics, students get to learn more applications of sine and other similar functions in the area of sound and light. In the domain of Signal Processing for unidimensional data, these functions pop up again to help you solve filtering, time-series related problems. An introduction to numeric computation around sine functions can not alone help you understand its wider application areas. In fact, sine functions are everywhere in the universe from music to light/sound/radio waves, from pendulum oscillations to alternating current.\u003c/p\u003e\n\n\u003ch2\u003e\u0026nbsp;Why Linear Algebra?\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eLinear algebra is the branch of mathematics concerning vector spaces and linear relationships between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAnalogous to the example we saw above, it's important that a data scientist understands how data structures are built with vectors and matrices following the geometric intuitions from linear algebra, in addition to the numeric calculations. A data-focused understanding of linear algebra can help machine learning practitioners decide what tools can be applied to a given problem and how to interpret the results of experiments. You'll see that a good understanding of linear algebra is particularly useful in many ML/AI algorithms, especially in deep learning, where a lot of the operations happen under the hood.\u003c/p\u003e\n\n\u003cp\u003eFollowing are some of the areas where linear algebra is commonly practiced in the domain of data science and machine learning:  \u003c/p\u003e\n\n\u003ch3\u003eComputer Vision / Image Processing\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/rgb.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eComputers are designed to process binary information only (only 0s and 1s). How can an image such as the dog shown here, with multiple attributes like color, be stored in a computer? This is achieved by storing the pixel intensities for red, blue and green colors in a matrix format. Color intensities can be coded into this matrix and can be processed further for analysis and other tasks. Any operation performed on this image would likely use some form of linear algebra with matrices as the back end.\u003c/p\u003e\n\n\u003ch3\u003eDeep Learning - Tensors\u003c/h3\u003e\n\n\u003cp\u003eDeep Learning is a sub-domain of machine learning, concerned with algorithms that can imitate the functions and structure of a biological brain as a computational algorithm. These are called artificial neural networks (ANNs). \u003c/p\u003e\n\n\u003cp\u003eThe algorithms usually store and process data in the form of mathematical entities called tensors. A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually such a tensor), a 2-D matrix (like a data frame), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/tensor.png\" width=\"850\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs shown in the image above where different input features are being extracted and stored as spatial locations inside a tensor which appears as a cube. A tensor encapsulates the scalar, vector, and the matrix characteristics. For deep learning, creating and processing tensors and operations that are performed on these also require knowledge of linear algebra. Don't worry if you don't fully understand this right now, you'll learn more about tensors later!\u003c/p\u003e\n\n\u003ch3\u003eNatural Language Processing\u003c/h3\u003e\n\n\u003cp\u003eNatural Language Processing (NLP) is another (very popular) area in Machine Learning dealing with text data. The most common techniques employed in NLP include BoW (Bag of Words) representation, Term Document Matrix etc. As shown in the image below, the idea is that words are being encoded as numbers and stored in a matrix format. Here, we just use 3 sentences to illustrate this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/NLPmatrix.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is just a short example, but you can store long documents in (giant) matrices like this. Using these counts in a matrix form can help perform tasks like semantic analysis, language translation, language generation etc.\u003c/p\u003e\n\n\u003ch3\u003eDimensionality Reduction\u003c/h3\u003e\n\n\u003cp\u003eDimensionality reduction techniques, which are heavily used when dealing with big datasets, use matrices to process data in order to reduce its dimensions. Principle Component Analysis (PCA) is a widely used dimensionality reduction technique that relies solely on calculating eigenvectors and eigenvalues to identify principal components as a set of highly reduced dimensions. The picture below is an example of a three-dimensional data being mapped into two dimensions using matrix manipulations. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/pca.png\" width=\"900\"\u003e\u003c/p\u003e\n\n\u003cp\u003eGreat, you now know about some key areas where linear algebra is used! In the following lessons, you'll go through an introductory series of lessons and labs that will cover basic ideas of linear algebra: an understanding of vectors and matrices with some basic operations that can be performed on these mathematical entities. We will implement these ideas in Python, in an attempt to give you the foundational knowledge to deal with these algebraic entities and their properties. These skills will be applied in advanced machine learning sections later in the course. \u003c/p\u003e\n\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=_MxCXGF9N-8\"\u003eYoutube: Why Linear Algebra\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/boost-your-data-sciences-skills-learn-linear-algebra-2c30fdd008cf\"\u003eBoost your data science skills. Learn linear algebra.\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.quora.com/What-are-the-applications-of-linear-algebra-in-machine-learning\"\u003eQuora: Applications of Linear Algebra in Deep Learning\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about some Data Science examples that heavily rely on linear algebra principles. You looked at some use cases in practical machine learning problems where linear algebra and matrix manipulation might come in handy. In the following lessons, you'll take a deeper dive into specific concepts in linear algebra, working your way towards solving a regression problem using linear algebraic operations only. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"motivation-for-linear-algebra-in-data-science"},{"id":455680,"title":"Systems of Linear Equations","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lingalg-linear-equations\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eLinear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms between them. \nThe first step towards developing a good understanding of linear algebra is to get a good sense of \u003cem\u003ewhat linear mappings and linear equations\u003c/em\u003e are, \u003cem\u003ehow these relate to vectors and matrices\u003c/em\u003e and \u003cem\u003ewhat this has to do with data analysis\u003c/em\u003e. Let's try to develop a basic intuition around these ideas by first understanding what linear equations are. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe a system of linear equations for solving analytical problems \u003c/li\u003e\n\u003cli\u003eDescribe how matrices and vectors can be used to solve linear equations \u003c/li\u003e\n\u003cli\u003eSolve a system of equations using elimination and substitution \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat are linear equations?\u003c/h2\u003e\n\n\u003cp\u003eIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables. For example, look at the following equations: \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" 3x + 2y - z = 0 \" src=\"https://learning.flatironschool.com/equation_images/%203x%20+%202y%20-%20z%20=%200\" alt=\"{\" data-equation-content=\" 3x + 2y - z = 0 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" 2x- 2y + 4z = -2 \" src=\"https://learning.flatironschool.com/equation_images/%202x-%202y%20+%204z%20=%20-2\" alt=\"{\" data-equation-content=\" 2x- 2y + 4z = -2 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" -x + 0.5y - z = 0 \" src=\"https://learning.flatironschool.com/equation_images/%20-x%20+%200.5y%20-%20z%20=%200\" alt=\"{\" data-equation-content=\" -x + 0.5y - z = 0 \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is a system of three equations in the three variables \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e, \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e, and \u003cimg class=\"equation_image\" title=\"z\" src=\"https://learning.flatironschool.com/equation_images/z\" alt=\"{\" data-equation-content=\"z\"\u003e. A solution to a linear system is an assignment of values to the variables in a way that \u003cem\u003eall the equations are simultaneously satisfied\u003c/em\u003e. A solution to the system above is given by:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" x = 1 \" src=\"https://learning.flatironschool.com/equation_images/%20x%20=%201\" alt=\"{\" data-equation-content=\" x = 1 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" y = -8/3 \" src=\"https://learning.flatironschool.com/equation_images/%20y%20=%20-8/3\" alt=\"{\" data-equation-content=\" y = -8/3 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" z = -7/3 \" src=\"https://learning.flatironschool.com/equation_images/%20z%20=%20-7/3\" alt=\"{\" data-equation-content=\" z = -7/3 \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eThese values make all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\u003c/p\u003e\n\n\u003ch2\u003eSolving linear equations\u003c/h2\u003e\n\n\u003cp\u003eA system of linear equations can always be expressed in a matrix form. Algebraically, both of these express the same thing. Let's work with an example to see how this works: \u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eLet's say you go to a market and buy 2 apples and 1 banana. For this, you end up paying 35 pence. If you denote apples by \u003cimg class=\"equation_image\" title=\"a\" src=\"https://learning.flatironschool.com/equation_images/a\" alt=\"{\" data-equation-content=\"a\"\u003e and bananas by \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e, the relationship between items bought and the price paid can be written down as an equation - let's call it Eq. A: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"2a + b = 35\" src=\"https://learning.flatironschool.com/equation_images/2a%20+%20b%20=%2035\" alt=\"{\" data-equation-content=\"2a + b = 35\"\u003e  - (Eq. A)\u003c/p\u003e\n\n\u003cp\u003eOn your next trip to the market, you buy 3 apples and 4 bananas, and the cost is 65 pence. Just like above, this can be written as Eq. B:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"3a + 4b = 65\" src=\"https://learning.flatironschool.com/equation_images/3a%20+%204b%20=%2065\" alt=\"{\" data-equation-content=\"3a + 4b = 65\"\u003e - (Eq. B)\u003c/p\u003e\n\n\u003cp\u003eThese two equations (known as a simultaneous equations) form a system that can be solved by hand for values of \u003cimg class=\"equation_image\" title=\"a\" src=\"https://learning.flatironschool.com/equation_images/a\" alt=\"{\" data-equation-content=\"a\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e i.e., price of a single apple and banana.\u003c/p\u003e\n\n\u003cp\u003eLet's solve this system for individual prices using a series of eliminations and substitutions:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStep 1:\u003c/strong\u003e Multiply Eq. A by 4\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"8a + 4b = 140\" src=\"https://learning.flatironschool.com/equation_images/8a%20+%204b%20=%20140\" alt=\"{\" data-equation-content=\"8a + 4b = 140\"\u003e - (Eq. C)\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStep 2 :\u003c/strong\u003e Subtract Eq. B from Eq. C\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"5a = 75\" src=\"https://learning.flatironschool.com/equation_images/5a%20=%2075\" alt=\"{\" data-equation-content=\"5a = 75\"\u003e which leads to \u003cimg class=\"equation_image\" title=\"a = 15\" src=\"https://learning.flatironschool.com/equation_images/a%20=%2015\" alt=\"{\" data-equation-content=\"a = 15\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStep 3:\u003c/strong\u003e Substitute the value of \u003cimg class=\"equation_image\" title=\"a\" src=\"https://learning.flatironschool.com/equation_images/a\" alt=\"{\" data-equation-content=\"a\"\u003e in Eq. A\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"30 + b = 35\" src=\"https://learning.flatironschool.com/equation_images/30%20+%20b%20=%2035\" alt=\"{\" data-equation-content=\"30 + b = 35\"\u003e which leads to \u003cimg class=\"equation_image\" title=\"b = 5\" src=\"https://learning.flatironschool.com/equation_images/b%20=%205\" alt=\"{\" data-equation-content=\"b = 5\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSo the price of an apple is 15 pence and the price of the banana is 5 pence. \u003c/p\u003e\n\n\u003ch2\u003eFrom equations to vectors and matrices\u003c/h2\u003e\n\n\u003cp\u003eNow, as your number of shopping trips increase along with the number of items you buy at each trip, the system of equations will become more complex and solving a system for individual price may become very expensive in terms of time and effort. In these cases, you can use a computer to find the solution.\u003c/p\u003e\n\n\u003cp\u003eThe above example is a classic linear algebra problem. The numbers 2 and 1 from Eq. A and 3 and 4 from Eq. B are linear coefficients that relate input variables a and b to the known output 15 and 5.\u003c/p\u003e\n\n\u003cp\u003eUsing linear algebra, we can write this system of equations as shown below: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-linear-equations/master/images/ss.png\" width=\"320\"\u003e\u003c/p\u003e\n\n\u003cp\u003eYou see that in order for a computational algorithm to solve this (and other similar) problems, we need to first convert the data we have into a set of matrix and vector objects. Machine learning involves building up these objects from the given data, understanding their relationships and how to process them for a particular problem. \u003c/p\u003e\n\n\u003cp\u003eSolving these equations requires knowledge of defining these vectors and matrices in a computational environment and of operations that can be performed on these entities to solve for unknown variables as we saw above. We'll look into how to do this in upcoming lessons. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned how a system of linear (simultaneous) equations can be solved using elimination and substitution, and also, how to covert these problems into matrices and vectors to be processed by computational algorithms. In the next couple of lessons, we'll look at how to describe these entities in Python and NumPy and also how to perform arithmetic operations to solve these types of equations.\u003c/p\u003e","exportId":"systems-of-linear-equations"},{"id":455684,"title":"Systems of Linear Equations - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lingalg-linear-equations-quiz\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThe following scenarios present problems that can be solved as a system of equations while performing substitutions and eliminations as you saw in the previous lesson.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSolve these problems by hand, showing all the steps to work out the unknown variable values \u003c/li\u003e\n\u003cli\u003eVerify your answers by showing the calculated values satisfy all equations\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g6cafbb465925915959d5223bf0942753"},{"id":455688,"title":"Scalars, Vectors, Matrices, and Tensors - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-scalars-vectors-matrices-tensors-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll be introduced to the basic mathematical entities used in linear algebra. We'll also look at how these entities are created (and later manipulated) in Python using NumPy. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare scalars, vectors, matrices, and tensors \u003c/li\u003e\n\u003cli\u003eCreate vectors and matrices using Numpy and Python\u003c/li\u003e\n\u003cli\u003eUse the transpose method to transpose Numpy matrices \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gfea549a153a77fb9593814c7ea567a9d"},{"id":455691,"title":"Matrix Multiplication - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linalg-mat-multiplication-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eUnderstanding matrix operations is very important for a deeper understanding of linear algebra. We know matrices are used throughout the field of machine learning in the description of algorithms and representation of data. In this lesson, we shall discover how to manipulate matrices in Python and Numpy.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompute the dot product for matrices and vectors \u003c/li\u003e\n\u003cli\u003eCalculate a cross product using Numpy \u003c/li\u003e\n\u003cli\u003eDefine a cross product\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g1f9f8e83538aa1ccd15d2d9b0de0394c"},{"id":455695,"title":"Short Video: Matrix Multiplication","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802388?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-matrix-multiplication"},{"id":455698,"title":"Solving Systems of Linear Equations with NumPy - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lineq-numpy-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn how to solve a system of linear equations using matrix algebra and Numpy.  You'll also learn about the identity matrix and inverse matrices, which have some unique properties that can be used to solve for unknown values in systems of linear equations. You'll also learn how to create these using Numpy. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine the identity matrix and its dot product \u003c/li\u003e\n\u003cli\u003eDefine the inverse of a matrix \u003c/li\u003e\n\u003cli\u003eCalculate the inverse of a matrix in order to solve linear problems \u003c/li\u003e\n\u003cli\u003eUse matrix algebra and Numpy to solve a system of linear equations given a real-life example \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g8345898d9a24b82069d1f41709ee0fdd"},{"id":455702,"title":"Solving Systems of Linear Equations with NumPy - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lineq-numpy-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow you've gathered all the required skills needed to solve systems of linear equations. You saw why there was a need to calculate inverses of matrices, followed by matrix multiplication to figure out the values of unknown variables. \u003c/p\u003e\n\n\u003cp\u003eThe exercises in this lab present some problems that can be converted into a system of linear equations. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse matrix algebra and NumPy to solve a system of linear equations given a real-life example \u003c/li\u003e\n\u003cli\u003eUse NumPy's linear algebra solver to solve for systems of linear equations\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g601c542580c65fb6de1d04b8bb0a30ef"},{"id":455707,"title":"Regression Analysis using Linear Algebra and NumPy - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linalg-regression-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist's toolbox. Here, you'll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions. \u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"http://math.mit.edu/%7Egs/linearalgebra/ila0403.pdf\"\u003eYou can find a deeper mathematical and geometric explanation of the topic here\u003c/a\u003e. In this lesson, we'll try to keep things more data-oriented.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eApply linear algebra to fit a function to data, describing linear mappings between input and output variables\u003c/li\u003e\n\u003cli\u003eIndicate how linear algebra is related to regression modeling\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gf03dc6212b7f20a3571de75b90fb5ccc"},{"id":455711,"title":"Regression with Linear Algebra - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linalg-regression-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You'll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the \u003cimg class=\"equation_image\" title=\"\\beta\" src=\"https://learning.flatironschool.com/equation_images/%255Cbeta\" alt=\"{\" data-equation-content=\"\\beta\"\u003e (unknown variables) vector. The beta vector will be used with test data to make new predictions. You'll also evaluate the model fit.\nIn order to make this experiment interesting, you'll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse matrix algebra to calculate the parameter values of a linear regression\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gce6931adc68361b10a894c60cafbf6f7"},{"id":455715,"title":"Computational Complexity: From OLS to Gradient Descent","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-computational-complexity\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll be introduced to computational complexity. You'll learn about this idea in relationship with OLS regression and see how this may not be the most efficient algorithm to calculate the regression parameters when performing regression with large datasets. You'll set the stage for an optimization algorithm called \"Gradient Descent\" which will be covered in detail later. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe computational complexity and how it is related to Big O notation \u003c/li\u003e\n\u003cli\u003eDescribe why OLS with matrix algebra would become problematic for large/complex data \u003c/li\u003e\n\u003cli\u003eExplain how optimizing techniques such as gradient descent can solve complexity issues\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eComplexities in OLS\u003c/h2\u003e\n\n\u003cp\u003eRecall the OLS formula for calculating the beta vector:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\beta =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\" src=\"/equation_images/%20%255Cbeta%20=(%255Cboldsymbol{X}^T%255Cboldsymbol{X})^{-1}%255Cboldsymbol{X}^T%20y\" alt=\"{\" data-equation-content=\" \\beta =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis formula looks very simple, elegant, and intuitive. It works perfectly fine for the case of simple linear regression due to a limited number of computed dimensions, but with datasets that are very large or \u003cstrong\u003ebig data\u003c/strong\u003e sets, it becomes computationally very expensive as it can potentially involve a huge number of complex mathematical operations.\u003c/p\u003e\n\n\u003cp\u003eFor this formula, we need to find \u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})\"\u003e, and invert it as well, which makes it very expensive. Imagine the matrix \u003cimg class=\"equation_image\" title=\"X_{(N \\times M+1)}\" src=\"/equation_images/X_{(N%20%255Ctimes%20M+1)}\" alt=\"{\" data-equation-content=\"X_{(N \\times M+1)}\"\u003e has \u003cimg class=\"equation_image\" title=\"(M+1)\" src=\"https://learning.flatironschool.com/equation_images/(M+1)\" alt=\"{\" data-equation-content=\"(M+1)\"\u003e columns where \u003cimg class=\"equation_image\" title=\"M\" src=\"https://learning.flatironschool.com/equation_images/M\" alt=\"{\" data-equation-content=\"M\"\u003e is the number of predictors and \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e is the number of rows of observations. In machine learning, you will often find datasets with \u003cimg class=\"equation_image\" title=\"M \u003e1000\" src=\"/equation_images/M%20\u003e1000\" alt=\"{\" data-equation-content=\"M \u003e1000\"\u003e and \u003cimg class=\"equation_image\" title=\"N \u003e 1,000,000\" src=\"/equation_images/N%20\u003e%201,000,000\" alt=\"{\" data-equation-content=\"N \u003e 1,000,000\"\u003e. The \u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})\"\u003e matrix itself takes a while to calculate, then you have to invert an \u003cimg class=\"equation_image\" title=\"M \\times M\" src=\"https://learning.flatironschool.com/equation_images/M%20%255Ctimes%20M\" alt=\"{\" data-equation-content=\"M \\times M\"\u003e matrix which adds more to the complexity - making it very expensive. You'll also come across situations where the input matrix grows so large that it cannot fit into your computer's memory. \u003c/p\u003e\n\n\u003ch2\u003eThe Big O notation\u003c/h2\u003e\n\n\u003cp\u003eIn computer science, Big O notation is used to describe how \"fast\" an algorithm grows, by comparing the number of operations within the algorithm. Big O notation helps you see the worst-case scenario for an algorithm. Typically, we are most concerned with the Big O time because we are interested in how slowly a given algorithm will possibly run at worst.\u003c/p\u003e\n\n\u003ch4\u003eExample\u003c/h4\u003e\n\n\u003cp\u003eImagine you need to find a person you only know the name of. What's the most straightforward way of finding this person? Well, you could go through every single name in the phone book until you find your target. This is known as a simple search. If the phone book is not very long, with say, only 10 names, this is a fairly fast process. But what if there are 10,000 names in the phone book?\u003c/p\u003e\n\n\u003cp\u003eAt best, your target's name is at the front of the list and you only need to need to check the first item. At worst, your target's name is at the very end of the phone book and you will need to have searched all 10,000 names. As the \"dataset\" (or the phone book) increases in size, the maximum time it takes to run a simple search also linearly increases.\u003c/p\u003e\n\n\u003cp\u003eBig O notation allows you to describe what the worst case is. The worst case is that you will have to search through all elements (\u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e) in the phone book. You can describe the run-time as:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e where \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e is the number of operations\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eBecause the maximum number of operations is equal to the maximum number of elements in our phone book, we say the Big \u003cimg class=\"equation_image\" title=\"O\" src=\"https://learning.flatironschool.com/equation_images/O\" alt=\"{\" data-equation-content=\"O\"\u003e of a simple search is \u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e. \u003cstrong\u003eA simple search will never be slower than \u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e time.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eDifferent algorithms have different run-times. That is, algorithms grow at different rates. The most common Big O run-times, from fastest to slowest, are:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(\\log n)\" src=\"https://learning.flatironschool.com/equation_images/O(%255Clog%20n)\" alt=\"{\" data-equation-content=\"O(\\log n)\"\u003e: aka \u003cimg class=\"equation_image\" title=\"\\log\" src=\"https://learning.flatironschool.com/equation_images/%255Clog\" alt=\"{\" data-equation-content=\"\\log\"\u003e time\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e: aka linear time\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(n^2)\" src=\"/equation_images/O(n^2)\" alt=\"{\" data-equation-content=\"O(n^2)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(n^3)\" src=\"/equation_images/O(n^3)\" alt=\"{\" data-equation-content=\"O(n^3)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese rates, as well as some other rates, can be visualized in the following figure:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/big_o.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eOLS and Big O notation\u003c/h3\u003e\n\n\u003cp\u003eInverting a matrix costs \u003cimg class=\"equation_image\" title=\"O(n^3)\" src=\"/equation_images/O(n^3)\" alt=\"{\" data-equation-content=\"O(n^3)\"\u003e for computation where n is the number of rows in \u003cimg class=\"equation_image\" title=\"X\" src=\"https://learning.flatironschool.com/equation_images/X\" alt=\"{\" data-equation-content=\"X\"\u003e matrix, i.e., the observations. Here is an explanation of how to calculate Big O for OLS.\u003c/p\u003e\n\n\u003cp\u003eOLS linear regression is computed as \u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})^{-1}%255Cboldsymbol{X}^T%20y\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eIf \u003cimg class=\"equation_image\" title=\"\\boldsymbol{X}\" src=\"/equation_images/%255Cboldsymbol{X}\" alt=\"{\" data-equation-content=\"\\boldsymbol{X}\"\u003e is an \u003cimg class=\"equation_image\" title=\"(n \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(n%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(n \\times k)\"\u003e matrix:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})\"\u003e takes \u003cimg class=\"equation_image\" title=\"O(n*k^2)\" src=\"/equation_images/O(n*k^2)\" alt=\"{\" data-equation-content=\"O(n*k^2)\"\u003e time and produces a \u003cimg class=\"equation_image\" title=\"(k \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(k%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(k \\times k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe matrix inversion of a (k x k) matrix takes \u003cimg class=\"equation_image\" title=\"O(k^3)\" src=\"/equation_images/O(k^3)\" alt=\"{\" data-equation-content=\"O(k^3)\"\u003e time\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{Y})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{Y})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{Y})\"\u003e takes \u003cimg class=\"equation_image\" title=\"O(n*k^2)\" src=\"/equation_images/O(n*k^2)\" alt=\"{\" data-equation-content=\"O(n*k^2)\"\u003e time and produces a \u003cimg class=\"equation_image\" title=\"(k \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(k%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(k \\times k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe final matrix multiplication of two \u003cimg class=\"equation_image\" title=\"(k \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(k%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(k \\times k)\"\u003e matrices takes \u003cimg class=\"equation_image\" title=\"O(k^3)\" src=\"/equation_images/O(k^3)\" alt=\"{\" data-equation-content=\"O(k^3)\"\u003e time\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e** So the Big O running time for OLS is \u003cimg class=\"equation_image\" title=\"O(k^{2*(n + k)})\" src=\"/equation_images/O(k^{2*(n%20+%20k)})\" alt=\"{\" data-equation-content=\"O(k^{2*(n + k)})\"\u003e - which is pretty expensive **\u003c/p\u003e\n\n\u003cp\u003eMoreover, if  \u003cimg class=\"equation_image\" title=\"X\" src=\"https://learning.flatironschool.com/equation_images/X\" alt=\"{\" data-equation-content=\"X\"\u003e is ill-conditioned  (i.e. it isn't a square matrix), there will be computational errors in the estimation. \nAnother common problem is overfitting and underfitting in estimation of regression coefficients.\u003c/p\u003e\n\n\u003cp\u003eSo, this leads us to the gradient descent kind of optimization algorithm which can save us from this type of problem. The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in most cases. \u003c/p\u003e\n\n\u003ch2\u003eGradient Descent\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/gradient_descent.png\" width=\"850\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGradient Descent is an iterative approach to minimize the model loss (error), used while training a machine learning model like linear regression. It is an optimization algorithm based on a convex function as shown in the figure above, that tweaks its parameters iteratively to minimize a given function to its local minimum.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn regression, it is used to find the values of model parameters (coefficients, or the \u003cimg class=\"equation_image\" title=\"\\beta\" src=\"https://learning.flatironschool.com/equation_images/%255Cbeta\" alt=\"{\" data-equation-content=\"\\beta\"\u003e matrix) that minimize a cost function (like RMSE) as far as possible.\u003c/p\u003e\n\n\u003cp\u003eIn order to fully understand how this works, you need to know what a gradient is and how is it calculated. And for this, you would need some Calculus. It may sound a bit intimidating at this stage, but don't worry. The next few sections will introduce you to the basics of calculus with gradients and derivatives. \u003c/p\u003e\n\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\"\u003eWiki: Computational complexity of mathematical operations\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835\"\u003eSimplified Big O notation\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the shortcomings and limitations of OLS and matrix inverses. You looked at the Big O notation to explain how calculating inverses and transposes for large matrix might make our analysis unstable and computationally very expensive. This lesson sets a stage for your next section on calculus and gradient descent. You will have a much better understanding of the gradient descent diagram shown above and how it all works by the end of next section. \u003c/p\u003e","exportId":"computational-complexity-from-ols-to-gradient-descent"},{"id":455719,"title":"Quiz: Linear Algebra","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g4a3f882d37f3810875ee6486149a8c35","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"gc405785850874cf6a53b7923e4458e4d"},{"id":455730,"title":"Linear Algebra Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g12d88cc7c0173532794428b3f141a2df","questionCount":7,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g62b3e5e4cbc8970111617a88cdab7801"},{"id":455732,"title":"Introduction to Derivatives","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-intro\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the linear regression section, you learned about the basic notion of mathematical functions. Now, imagine that you used the number of bedrooms as a predictor and house rental price as the target variable, you can formulate this as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\text{price} = f(\\text{number of bedrooms})\" src=\"https://learning.flatironschool.com/equation_images/%255Ctext%7Bprice%7D%20=%20f(%255Ctext%7Bnumber%20of%20bedrooms%7D)\" alt=\"{\" data-equation-content=\"\\text{price} = f(\\text{number of bedrooms})\"\u003e\u003c/p\u003e or, alternatively\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" y = f(\\text{x})\" src=\"https://learning.flatironschool.com/equation_images/%20y%20=%20f(%255Ctext%7Bx%7D)\" alt=\"{\" data-equation-content=\" y = f(\\text{x})\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eNow let's say the price of the apartment is set in a very simplified way, and there is a perfectly linear relationship between the apartment size and the rental price. Say that the price goes up by 500 USD/month for every bedroom an apartment has. In that case, we can express the price as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\text{price} = 500 * \\text{number of bedrooms}\" src=\"https://learning.flatironschool.com/equation_images/%255Ctext%7Bprice%7D%20=%20500%20*%20%255Ctext%7Bnumber%20of%20bedrooms%7D\" alt=\"{\" data-equation-content=\"\\text{price} = 500 * \\text{number of bedrooms}\"\u003e\u003c/p\u003e or \u003cp\u003e\u003cimg class=\"equation_image\" title=\"y = f(x) = 500 * x = 500x\" src=\"https://learning.flatironschool.com/equation_images/y%20=%20f(x)%20=%20500%20*%20x%20=%20500x\" alt=\"{\" data-equation-content=\"y = f(x) = 500 * x = 500x\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eNote that there is no intercept here! Now, we want to dive deeper into how the rental price changes as the number of bedrooms changes. This is what derivatives are all about!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what a derivative means in the context of a real-world example\u003c/li\u003e\n\u003cli\u003eCalculate the derivative of a linear function\u003c/li\u003e\n\u003cli\u003eDefine derivatives as the instantaneous rate of change of a function\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g9777bfbacb7532d96452a0bc04a904b5"},{"id":455734,"title":"Introduction to Derivatives - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-intro-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we will practice our knowledge of derivatives. Remember that our key formula for derivatives, is \n\u003cimg class=\"equation_image\" title=\"f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}\" src=\"https://learning.flatironschool.com/equation_images/f'(x)%20=%20%255Cdfrac%7B%255CDelta%20y%7D%7B%255CDelta%20x%7D%20=%20%20%255Cdfrac%7Bf(x%20+%20%255CDelta%20x)%20-%20f(x)%7D%7B%255CDelta%20x%7D\" alt=\"{\" data-equation-content=\"f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}\"\u003e.  So in driving towards this formula, we will do the following: \u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eLearn how to represent linear and nonlinear functions in code \u003c/li\u003e\n\u003cli\u003eThen, because our calculation of a derivative relies on seeing the output at an initial value and the output at that value plus \u003cimg class=\"equation_image\" title=\"\\Delta x\" src=\"https://learning.flatironschool.com/equation_images/%255CDelta%20x\" alt=\"{\" data-equation-content=\"\\Delta x\"\u003e, we need an \u003ccode\u003eoutput_at\u003c/code\u003e function\u003c/li\u003e\n\u003cli\u003eThen we will be able to code the \u003cimg class=\"equation_image\" title=\"\\Delta f\" src=\"https://learning.flatironschool.com/equation_images/%255CDelta%20f\" alt=\"{\" data-equation-content=\"\\Delta f\"\u003e function that sees the change in output between the initial \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e and that initial \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e plus the \u003cimg class=\"equation_image\" title=\"\\Delta x\" src=\"https://learning.flatironschool.com/equation_images/%255CDelta%20x\" alt=\"{\" data-equation-content=\"\\Delta x\"\u003e \u003c/li\u003e\n\u003cli\u003eFinally, we will calculate the derivative at a given \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e value, \u003ccode\u003ederivative_at\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse python functions to demonstrate derivatives of functions\u003c/li\u003e\n\u003cli\u003eDescribe what a derivative means in the context of a real-world example\u003c/li\u003e\n\u003c/ul\u003e","exportId":"ga3c10c23d3d9e1151dccb8cb9437a1f4"},{"id":455739,"title":"Derivatives of Non-Linear Functions","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-of-non-linear-functions\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson, we saw that the derivative was the rate of change and that the derivative of a straight line is a constant. Let's explore non-linear functions and their derivatives in this lesson!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate the derivative of a non-linear function\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g3096b7ec24bf7a02135c7a720a776e2d"},{"id":455744,"title":"Rules for Derivatives","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-rules-for-derivatives\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous lesson, we calculated the derivative by changing our delta to see the convergence around a number as reflected in the table above.  However, mathematicians have derived shortcuts to calculate the derivative. \nYou'll learn about these shortcuts in this lesson!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate derivatives of more complex functions by using power rules, constant factor and the addition rule\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g240536dee629c9b857d210808dac71ec"},{"id":455748,"title":"Derivatives: Conclusion","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-conclusion\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eData science is all about finding good models to understand patterns in your data. You'll find yourself performing optimizations all the time. Examples are: maximizing model likelihoods and minimizing errors. Essentially, you'll perform a lot of minimizations and maximizations along the way when creating machine learning models. This is where derivatives come in very handy!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how minima and maxima are related to machine learning and optimization\u003c/li\u003e\n\u003cli\u003eCalculate minima and maxima mathematically\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gc0e76e1a6e29d946d6f18e9aacad1c3d"},{"id":455751,"title":"üé¨  Lecture: Calculus and Cost Functions","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv style=\"padding:56.25% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/681831452?h=1127a46572\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"Calculus and Cost Functions\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003cp\u003eIn this lesson, Victor Geislinger reviews some calculus concepts and cost functions along with their use in data science. Topics covered include: derivatives, partial derivatives, integration, differentiation, the difference between integration and differentiation.\u003c/p\u003e\n\u003cp\u003eThe repository for this lecture can be found here: \u003ca class=\"inline_disabled\" style=\"color: #3598db;\" href=\"https://github.com/flatiron-school/ds-calculus-kvo32\"\u003eCalculus and Cost Function Lecture Repository\u003c/a\u003e\u003c/p\u003e","exportId":"lecture-calculus-and-cost-functions"},{"id":455755,"title":"Calculus and Cost Functions Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g07c50903bfd70ff6e7919d625878acc2","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"gb3db5cbf0bcdb11aabfa93891bfac5b0"},{"id":455759,"title":"Introduction to Gradient Descent","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-intro\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIt's possible to solve for the optimal values of a regression using closed-form Ordinary Least Squares programming when there are a limited number of features, but this process might become computationally expensive when there are many features. Therefore, iterative algorithms like the \u003cem\u003egradient descent\u003c/em\u003e algorithm are the basis of many models in statistics and machine learning!\u003c/p\u003e\n\n\u003cp\u003eYou previously saw how after choosing the slope and y-intercept values of a regression line, we can calculate the residual sum of squares (RSS) and related root mean squared error. We can use either the RSS or RMSE to calculate the accuracy of a line. In this lesson, we'll use the RSS to iteratively find the best fit line for our problem at hand!\u003c/p\u003e\n\n\u003cp\u003eOnce calculating the accuracy of a line, we are pretty close to improving upon a line by minimizing the RSS.  This is the task of the gradient descent technique.  But before learning about gradient descent, let's review and ensure that we understand how to evaluate how our line fits our data.  \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what gradient descent is, and its relationship to minima\u003c/li\u003e\n\u003cli\u003eDescribe a cost curve and what it means to move along it\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g0eaaa285661d545beda76b6a1d97b302"},{"id":455765,"title":"Gradient Descent: Step Sizes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-step-sizes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last section, you took a first look at the process for improving regression lines.  You began with some data then used a simple regression line in the form \u003cimg class=\"equation_image\" title=\"\\hat{y}= mx + b \" src=\"https://learning.flatironschool.com/equation_images/%255Chat%7By%7D=%20mx%20+%20b\" alt=\"{\" data-equation-content=\"\\hat{y}= mx + b \"\u003e to predict an output, given an input.  Finally, you measured the accuracy of your regression line by calculating the differences between the outputs predicted by the regression line and the actual values. In this lesson, you'll look at how we can make your approach more efficient.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eDefine step sizes in the context of gradient descent\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eDefine a learning rate, and its relationship to step size when performing gradient descent\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ePlot visualizations of the process of gradient descent\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g508c94e5f4410b774066661cb6893554"},{"id":455768,"title":"Gradient Descent: Step Sizes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-step-sizes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice applying gradient descent.  As you know, gradient descent begins with an initial regression line and moves to a \"best fit\" regression line by changing values of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e and evaluating the RSS.  So far, we have illustrated this technique by changing the values of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and evaluating the RSS.  In this lab, you will work through applying this technique by changing the value of \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e instead.  Let's get started.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse gradient descent to find the optimal parameters for a linear regression model\u003c/li\u003e\n\u003cli\u003eDescribe how to use an RSS curve to find the optimal parameters for a linear regression model\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gafebbe6171e387ed7e205bba80badcc0"},{"id":455772,"title":"Gradient Descent in 3D","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-in-3d\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003ePreviously, we talked about how to think about gradient descent when moving along a 3D cost curve.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-in-3d/master/images/new_gradientdescent.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWe know that moving along the 3D cost curve above means changing the \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e variables of a regression line like the one below.  And we do so with the purpose of having our line better match our data. In this section, you'll learn about \u003cem\u003epartial derivatives\u003c/em\u003e which will make you achieve this. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine a partial derivative\u003c/li\u003e\n\u003cli\u003eInterpret visual representations of gradient descent in more than two dimensions\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g3929ccf412b073ec731206e2dce84453"},{"id":455777,"title":"The Gradient in Gradient Descent","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-gradient-in-gradient-descent\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs you know, we entered our discussion of derivatives to determine the size and direction of a step with which to move along a cost curve.  We first used a derivative in a single variable function to see how the output of our cost curve changed with respect to change a change in one of our regression line's variables.  Then we learned about partial derivatives to see how a \u003cem\u003ethree-dimensional cost curve\u003c/em\u003e responded to a change in the regression line.  \u003c/p\u003e\n\n\u003cp\u003eHowever, we have not yet explicitly showed how partial derivatives apply to gradient descent.\u003c/p\u003e\n\n\u003cp\u003eWell, that's what we hope to show in this lesson: explain how we can use partial derivatives to find the path to minimize our cost function, and thus find our \"best fit\" regression line.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine a gradient in relation to gradient descent\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is the gradient?\u003c/h2\u003e\n\n\u003cp\u003eNow gradient descent literally means that we are taking the shortest path to \u003cem\u003edescend\u003c/em\u003e towards our minimum.  However, it is somewhat easier to understand gradient \u003cem\u003eascent\u003c/em\u003e than descent, and the two are quite related, so that's where we'll begin.  Gradient ascent, as you could guess, simply means that we want to move in the direction of steepest ascent.\u003c/p\u003e\n\n\u003cp\u003eNow moving in the direction of greatest ascent for a function \u003cimg class=\"equation_image\" title=\"f(x,y)\" src=\"https://learning.flatironschool.com/equation_images/f(x,y)\" alt=\"{\" data-equation-content=\"f(x,y)\"\u003e, means that our next step is a step some distance in the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction and some distance in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction which is the steepest upward at that point.\u003c/p\u003e\n\n\u003cp\u003eNote how this is a different task from what we have previously worked on for multivariable functions.   So far, we have used partial derivatives to calculate the \u003cstrong\u003egain\u003c/strong\u003e from moving directly in either the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction or the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.  \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHere, in finding gradient ascent, our task is not to calculate the gain from a move in either the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e or \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.  Instead, our task is to \u003cstrong\u003efind some combination of a change in \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e,\u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e that brings the largest change in output\u003c/strong\u003e.  \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eSo if you look at the path our climbers are taking in the picture above, \u003cem\u003ethat\u003c/em\u003e is the direction of gradient ascent.  If they tilt their path to the right or left, they will no longer be moving along the steepest upward path.\u003c/p\u003e\n\n\u003cp\u003eThe direction of the greatest rate of increase of a function is called the gradient.  We denote the gradient with the nabla, which comes from the Greek word for harp, which is kind of what it looks like: \u003cimg class=\"equation_image\" title=\"\\nabla \" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla\" alt=\"{\" data-equation-content=\"\\nabla \"\u003e.  So we can denote the gradient of a function, \u003cimg class=\"equation_image\" title=\"f(x, y)\" src=\"https://learning.flatironschool.com/equation_images/f(x,%20y)\" alt=\"{\" data-equation-content=\"f(x, y)\"\u003e, with \u003cimg class=\"equation_image\" title=\"\\nabla f(x, y) \" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla%20f(x,%20y)\" alt=\"{\" data-equation-content=\"\\nabla f(x, y) \"\u003e.\u003c/p\u003e\n\n\u003ch2\u003eCalculating the gradient\u003c/h2\u003e\n\n\u003cp\u003eNow how do we find the direction for the greatest rate of increase?  We use partial derivatives.  Here's why.\u003c/p\u003e\n\n\u003cp\u003eAs we know, the partial derivative \u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}\"\u003e calculates the change in output from moving a little bit in the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction, and the partial derivative \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}\" src=\"/equation_images/%255Cfrac{df}{dy}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}\"\u003e calculates the change in output from moving in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.  Because with gradient ascent our goal is to make a nudge in \u003cimg class=\"equation_image\" title=\"x, y\" src=\"https://learning.flatironschool.com/equation_images/x,%20y\" alt=\"{\" data-equation-content=\"x, y\"\u003e that produces the greatest change in output, if \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy} \u003e \\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dy}%20\u003e%20%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy} \u003e \\frac{df}{dx}\"\u003e, we should make that move more in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction than the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction, and vice versa.  That is, we want to get the biggest bang for our buck.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/Denali.jpg\" alt=\"hikers on Denali\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's relate this again to mountain climbers. Imagine the vertical edge on the left is our y-axis and the horizontal edge is on the bottom is our x-axis.  For the climber in the yellow jacket, imagine his step size is three feet. A step straight along the y-axis will move him further upwards than a step along the x-axis.  So in taking that step, he should direct himself more towards the y-axis than the x-axis.  That will produce a bigger increase per step size.\u003c/p\u003e\n\n\u003cp\u003eIn fact, the direction of greatest ascent for a function,  \u003cimg class=\"equation_image\" title=\"\\nabla f(x, y)\" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla%20f(x,%20y)\" alt=\"{\" data-equation-content=\"\\nabla f(x, y)\"\u003e, is the direction which is a proportion of \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}\" src=\"/equation_images/%255Cfrac{df}{dy}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}\"\u003e steps in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction and \u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}\"\u003e in the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  So, for example, if \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}\" src=\"/equation_images/%255Cfrac{df}{dy}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}\"\u003e = 5 and \u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}\"\u003e = 1, the direction of gradient ascent is five times more in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction than the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  And this seems to be the path, more or less that our climbers are taking - some combination of \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e and \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e, but tilted more towards the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.\u003c/p\u003e\n\n\u003ch2\u003eApplying Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eNow that we have a better understanding of a gradient, let's apply our understanding to a multivariable function.  Here is a plot of a function:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"f(x,y) = 2x + 3y \" src=\"https://learning.flatironschool.com/equation_images/f(x,y)%20=%202x%20+%203y\" alt=\"{\" data-equation-content=\"f(x,y) = 2x + 3y \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" alt=\"2d plane projected within a 3d plot\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eImagine being at the bottom left of the graph at the point \u003cimg class=\"equation_image\" title=\"x = 1\" src=\"https://learning.flatironschool.com/equation_images/x%20=%201\" alt=\"{\" data-equation-content=\"x = 1\"\u003e, \u003cimg class=\"equation_image\" title=\"y = 1\" src=\"https://learning.flatironschool.com/equation_images/y%20=%201\" alt=\"{\" data-equation-content=\"y = 1\"\u003e.  What would be the direction of steepest ascent?  It seems, just sizing it up visually, that we should move both in the positive \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction and the positive \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  Looking more carefully, it seems we should move \u003cstrong\u003emore\u003c/strong\u003e in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction than the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  Let's see what our technique of taking the partial derivative indicates.   \u003c/p\u003e\n\n\u003cp\u003eThe gradient of the function \u003cimg class=\"equation_image\" title=\"f(x,y)\" src=\"https://learning.flatironschool.com/equation_images/f(x,y)\" alt=\"{\" data-equation-content=\"f(x,y)\"\u003e, that is \u003cimg class=\"equation_image\" title=\" \\nabla f(x,y) = 2x + 3y \" src=\"https://learning.flatironschool.com/equation_images/%20%255Cnabla%20f(x,y)%20=%202x%20+%203y\" alt=\"{\" data-equation-content=\" \\nabla f(x,y) = 2x + 3y \"\u003e is the following: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}(2x + 3y) = 2 \" src=\"/equation_images/%255Cfrac{df}{dx}(2x%20+%203y)%20=%202\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}(2x + 3y) = 2 \"\u003e and \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}(2x + 3y) = 3 \" src=\"/equation_images/%255Cfrac{df}{dy}(2x%20+%203y)%20=%203\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}(2x + 3y) = 3 \"\u003e.\u003c/p\u003e\n\n\u003cp\u003eSo what this tells us is to move in the direction of greatest ascent for the function \u003cimg class=\"equation_image\" title=\"f(x,y) = 2x + 3y \" src=\"https://learning.flatironschool.com/equation_images/f(x,y)%20=%202x%20+%203y\" alt=\"{\" data-equation-content=\"f(x,y) = 2x + 3y \"\u003e, is to move up three and to the right two.  So we would expect our path of greatest ascent to look like the following.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/gradient-plot.png\" alt=\"line graph plotting y = 1.5x\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" alt=\"2d plane projected within a 3d plot\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSo this path maps up well to what we see visually.  That is the idea behind gradient descent.  The gradient is the partial derivative with respect to each type of variable of a multivariable function, in this case \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e and \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e.  And the importance of the gradient is that its direction is the direction of steepest ascent.  The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent.  So our direction of gradient descent for the graph above is \u003cimg class=\"equation_image\" title=\"x = -2\" src=\"https://learning.flatironschool.com/equation_images/x%20=%20-2\" alt=\"{\" data-equation-content=\"x = -2\"\u003e, \u003cimg class=\"equation_image\" title=\"y = -3\" src=\"https://learning.flatironschool.com/equation_images/y%20=%20-3\" alt=\"{\" data-equation-content=\"y = -3\"\u003e.  And looking at the two graphs above, it seems that the steepest downward direction is just the opposite of the steepest upward direction.  We get that by mathematically by simply taking the multiplying our partial derivatives by negative one.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you saw how to use gradient descent to find the direction of steepest descent.  You saw that the direction of steepest descent is generally some combination of a change in your variables to produce the greatest negative rate of change.  \u003c/p\u003e\n\n\u003cp\u003eYou first how saw how to calculate the gradient \u003cstrong\u003eascent\u003c/strong\u003e, or the gradient \u003cimg class=\"equation_image\" title=\"\\nabla \" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla\" alt=\"{\" data-equation-content=\"\\nabla \"\u003e, by calculating the partial derivative of a function with respect to the variables of the function.  So \u003cimg class=\"equation_image\" title=\"\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} \" src=\"/equation_images/%255Cnabla%20f(x,%20y)%20=%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y},%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}\" alt=\"{\" data-equation-content=\"\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} \"\u003e.  This means that to take the path of greatest ascent, you should move \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta y} \" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y}\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta y} \"\u003e divided by \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta x} \" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta x} \"\u003e.  So for example, when \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta y}f(x, y)  = 3 \" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y}f(x,%20y)%20%20=%203\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta y}f(x, y)  = 3 \"\u003e , and \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta x}f(x, y)  = 2\" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}f(x,%20y)%20%20=%202\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta x}f(x, y)  = 2\"\u003e, you traveled in line with a slope of 3/2.\u003c/p\u003e\n\n\u003cp\u003eFor gradient descent, that is to find the direction of greatest decrease, you simply reverse the direction of your partial derivatives and move in \u003cimg class=\"equation_image\" title=\" - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}\" src=\"/equation_images/%20-%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y},%20-%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}\" alt=\"{\" data-equation-content=\" - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}\"\u003e. \u003c/p\u003e","exportId":"the-gradient-in-gradient-descent"},{"id":455780,"title":"Short Video: The Gradient","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802605?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-the-gradient"},{"id":455784,"title":"Gradient to Cost Function","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-to-cost-function-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous lesson, we learned the mathematical definition of a gradient.  We saw that the gradient of a function was a combination of our partial derivatives with respect to each variable of that function.  We saw the direction of gradient descent was simply to move in the negative direction of the gradient.  For example, if the direction of ascent of a function is a move up and to the right, the descent is down and to the left. In this lesson, we will apply gradient descent to our cost function to see how we can move towards a best fit regression line by changing variables of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e.  \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRepresent RSS as a multivariable function and take partial derivatives to perform gradient descent\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gc5264c4c0c07fc228d3a316350dc8504"},{"id":455788,"title":"Applying Gradient Descent - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-applying-gradient-descent-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson, we derived the functions that we help us descend along our cost functions efficiently.  Remember that this technique is not so different from what we saw with using the derivative to tell us our next step size and direction in two dimensions.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/master/images/slopes.png\" alt=\"RSS with changes to slope\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen descending along our cost curve in two dimensions, we used the slope of the tangent line at each point, to tell us how large of a step to take next.  And with the cost curve being a function of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e, we had to use the gradient to determine each step.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/master/images/new_gradientdescent.png\" alt=\"gradient descent in 3d with absolute minimum highlighted\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eBut really it's an analogous approach.  Just like we can calculate the use derivative of a function \u003cimg class=\"equation_image\" title=\"f(x)\" src=\"https://learning.flatironschool.com/equation_images/f(x)\" alt=\"{\" data-equation-content=\"f(x)\"\u003e to calculate the slope at a given value of \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e on the graph and thus our next step.  Here, we calculated the partial derivative with respect to both variables, our slope and y-intercept, to calculate the amount to move next in either direction and thus to steer us towards our minimum.   \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate functions to perform a simulation of gradient descent for an actual dataset\u003c/li\u003e\n\u003cli\u003eRepresent RSS as a multivariable function and take partial derivatives to perform gradient descent\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gf1d5c6a7ab52e0d588b4b7db4b395574"},{"id":455801,"title":"Gradient Descent Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"gc49c8298a982b1ac6bd1c70fa4c89133","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"gc53fdbad8c7fa83c61a7b59363bb80bf"},{"id":455806,"title":"Linear Algebra and Calculus - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-algebra-and-calculus-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eCongratulations! You have learned the fundamentals of the math at the core of machine learning: linear algebra and calculus.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003ch3\u003eLinear Algebra\u003c/h3\u003e\n\n\u003cp\u003eThe goal of this part was to provide both a conceptual and computational introduction to linear algebra. Some of the key takeaways include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eScalars, vectors, matrices, and tensors\n\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003escalar\u003c/em\u003e\u003c/strong\u003e is a single, real number\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003evector\u003c/em\u003e\u003c/strong\u003e is a one-dimensional array of numbers\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003ematrix\u003c/em\u003e\u003c/strong\u003e is a 2-dimensional array of numbers\u003c/li\u003e\n\u003cli\u003eTwo matrices can be added together if they have the same shape\u003c/li\u003e\n\u003cli\u003eScalars can be added to matrices by adding the scalar (number) to each element\u003c/li\u003e\n\u003cli\u003eTo calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003etensor\u003c/em\u003e\u003c/strong\u003e is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\u003c/li\u003e\n\u003cli\u003eOne use case for vectors and matrices is for representing and solving systems of linear equations \u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003eLinear algebra in Python\n\n\u003cul\u003e\n\u003cli\u003eOperating on \u003cstrong\u003e\u003cem\u003eNumPy\u003c/em\u003e\u003c/strong\u003e data types is substantially more computationally efficient than performing the same operations on native Python data types\u003c/li\u003e\n\u003cli\u003eIt is possible to use linear algebra in NumPy to solve for a linear regression using the \u003cstrong\u003e\u003cem\u003eOLS\u003c/em\u003e\u003c/strong\u003e method\u003c/li\u003e\n\u003cli\u003eOLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eCalculus and Gradient Descent\u003c/h3\u003e\n\n\u003cp\u003eThe goal of this part was to learn some of the foundational calculus that underpins the gradient descent algorithm. Some of the key takeaways include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculus\n\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003ederivative\u003c/em\u003e\u003c/strong\u003e is the \"instantaneous rate of change\" of a function - or it can be thought of as the \"slope of the curve\" at a point in time\u003c/li\u003e\n\u003cli\u003eA derivative can also be thought of as a special case of the rate of change over a period of time - as that period of time is zero. \u003c/li\u003e\n\u003cli\u003eIf you calculate the rate of change over a period of time and keep reducing the period of time, it usually tends to a limit - which is the value of that derivative\u003c/li\u003e\n\u003cli\u003eRules can be used for finding derivatives\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003e\u003cem\u003epower rule\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003econstant factor rule\u003c/em\u003e\u003c/strong\u003e, and \u003cstrong\u003e\u003cem\u003eaddition rule\u003c/em\u003e\u003c/strong\u003e are key tools for calculating derivatives for various kinds of functions\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003e\u003cem\u003echain rule\u003c/em\u003e\u003c/strong\u003e can be a useful tool for calculating the derivate of composite functions\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003eGradient descent\n\n\u003cul\u003e\n\u003cli\u003eA derivative can be useful for identifying local \u003cstrong\u003e\u003cem\u003emaxima\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eminima\u003c/em\u003e\u003c/strong\u003e as in both cases, the derivative tends to zero\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003ecost curve\u003c/em\u003e\u003c/strong\u003e can be used to plot the values of a cost function (in the case of linear regression) for various values of offset and slope for the best fit line\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eGradient descent\u003c/em\u003e\u003c/strong\u003e can be used to move towards the local minimum on the cost curve and thus the ideal values for the y-intercept and slope to minimize the selected cost function when performing a linear regression\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003c/ul\u003e","exportId":"linear-algebra-and-calculus-recap"}]},{"id":46957,"name":"Topic 23: Machine Learning Fundamentals","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g1853751b3adb03f437e97cf14f6a7a8b","items":[{"id":455817,"title":"Topic 23 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.6634%; height: 127px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eInference vs. Prediction\u003c/em\u003e\u0026nbsp;Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Machine Learning Fundamentals - Introduction\" href=\"pages/machine-learning-fundamentals-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/machine-learning-fundamentals-introduction\" data-api-returntype=\"Page\"\u003eMachine Learning Fundamentals - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 1st\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Data Science Processes\" href=\"pages/data-science-processes\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/data-science-processes\" data-api-returntype=\"Page\"\u003eData Science Processes\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Statistical Learning Theory\" href=\"pages/statistical-learning-theory\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/statistical-learning-theory\" data-api-returntype=\"Page\"\u003e\u003cstrong\u003eStatistical Learning Theory\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Regression Model Validation\" href=\"assignments/g7481fbdcbe748b61b8266678416afd43\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197106\" data-api-returntype=\"Assignment\"\u003eRegression Model Validation\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e\u0026nbsp;1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Regression Model Validation - Lab\" href=\"assignments/g8878f371f80e2be164db9ca56270ac6f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197107\" data-api-returntype=\"Assignment\"\u003eRegression Model Validation - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Cross-Validation\" href=\"assignments/gf75f69ba85f77765f91d254d7d93222e\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197099\" data-api-returntype=\"Assignment\"\u003eIntroduction to Cross-Validation\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Introduction to Cross-Validation - Lab\" href=\"assignments/g5c9102ed5e9c336f43723273d0ebd59b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197100\" data-api-returntype=\"Assignment\"\u003eIntroduction to Cross-Validation - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Short Video: Cross-Validation\" href=\"pages/short-video-cross-validation\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/short-video-cross-validation\" data-api-returntype=\"Page\"\u003eShort Video: Cross-Validation\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Bias-Variance Tradeoff\" href=\"assignments/g24fef19b98005baa41a82dd41eea0b54\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197089\" data-api-returntype=\"Assignment\"\u003eBias-Variance Tradeoff\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e\u0026nbsp;1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Bias-Variance Tradeoff - Lab\" href=\"assignments/g3b15a2f839ce8b5455258fa436cec3b0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197090\" data-api-returntype=\"Assignment\"\u003eBias-Variance Tradeoff - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Short Video: Bias-Variance Tradeoff\" href=\"pages/short-video-bias-variance-tradeoff\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/short-video-bias-variance-tradeoff\" data-api-returntype=\"Page\"\u003eShort Video: Bias-Variance Tradeoff\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Quiz: Machine Learning Fundamentals\" href=\"quizzes/gde2a7ea9ada6166586c9cf787822bfba\"\u003e\u003cstrong\u003eQuiz: Machine Learning Fundamentals\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e\u0026nbsp;1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.6649%; height: 53px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eInference vs. Prediction\u003c/em\u003e Lecture, Before \u003cem\u003eModel Validation and Data Leakage \u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\" Inference vs. Prediction Exit Ticket\" href=\"quizzes/g63baf40c53d2206ad7b65a1501c08cbf\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33587\" data-api-returntype=\"Quiz\"\u003e Inference vs. Prediction Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.3789%; height: 42px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eModel Validation and Data Leakage\u003c/em\u003e\u0026nbsp;Lecture, Before \u003cem\u003eRegularization \u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Model Validation and Data Leakage Exit Ticket\" href=\"quizzes/g343df895fe21388a003ee4f3f256d3e0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33591\" data-api-returntype=\"Quiz\"\u003eModel Validation and Data Leakage Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Ridge and Lasso Regression\" href=\"assignments/g80f8d50fa5bf6e890812cf951e88626b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197108\" data-api-returntype=\"Assignment\"\u003eRidge and Lasso Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 1st\u0026quot;}\"\u003e\u003cstrong\u003e 1st \u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Ridge and Lasso Regression - Lab\" href=\"assignments/g7ff4049e3a4478aad77ba3a1b80487cc\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197109\" data-api-returntype=\"Assignment\"\u003eRidge and Lasso Regression - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Feature Selection Methods\" href=\"assignments/gf5dc93ee053ea7f248ee15a7b680c8fe\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197094\" data-api-returntype=\"Assignment\"\u003eFeature Selection Methods\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Extensions to Linear Models - Lab\" href=\"assignments/gf751cd1332473f2e094bab2de0075b78\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197093\" data-api-returntype=\"Assignment\"\u003eExtensions to Linear Models - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Quiz: Regularization\" href=\"quizzes/gb7d6a7cdf1300cc8cb899aa98c8bc385\"\u003e\u003cstrong\u003eQuiz: Regularization\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.6634%; height: 53px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eRegularization \u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Regularization Exit Ticket\" href=\"quizzes/g15774678425387b9fbeab8a27d51575f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33585\" data-api-returntype=\"Quiz\"\u003eRegularization Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\" href=\"quizzes/g9d5b89d09608ee0da642cc68cf19da0e\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33594\" data-api-returntype=\"Quiz\"\u003e‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Machine Learning Fundamentals - Recap\" href=\"pages/machine-learning-fundamentals-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/machine-learning-fundamentals-recap\" data-api-returntype=\"Page\"\u003eMachine Learning Fundamentals - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","exportId":"topic-23-lesson-priorities-live"},{"id":455821,"title":"Machine Learning Fundamentals - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ml-fundamentals-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eLinear regression serves as the bridge between traditional statistics and machine learning in this curriculum. In this section you'll learn how to apply machine learning techniques using the familiar linear regression algorithm.\u003c/p\u003e\n\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\n\u003cp\u003eStatistical learning goes beyond the statistical modeling we have covered already. We will continue to build models that model the relationships between independent and dependent variables, but the emphasis will shift from \u003cstrong\u003e\u003cem\u003einference\u003c/em\u003e\u003c/strong\u003e to \u003cstrong\u003e\u003cem\u003eprediction\u003c/em\u003e\u003c/strong\u003e. In the predictive context we also need to consider model \u003cstrong\u003e\u003cem\u003egeneralization\u003c/em\u003e\u003c/strong\u003e and not just how well the model is fit to the training data.\u003c/p\u003e\n\n\u003ch3\u003eModel Validation\u003c/h3\u003e\n\n\u003cp\u003eIn order to assess how well a predictive model will generalize to unseen data, we need model \u003cstrong\u003e\u003cem\u003evalidation\u003c/em\u003e\u003c/strong\u003e techniques. These include a \u003cstrong\u003e\u003cem\u003etrain-test split\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003ecross-validation\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eBias-Variance Trade-Off\u003c/h3\u003e\n\n\u003cp\u003ePredictive modeling often involves striking the right balance between \u003cstrong\u003e\u003cem\u003ebias\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003evariance\u003c/em\u003e\u003c/strong\u003e, also referred to as the balance between \u003cstrong\u003e\u003cem\u003eunderfitting\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e. Especially as you learn models beyond linear regression, you'll be able to tune the model performance to strike the right balance.\u003c/p\u003e\n\n\u003ch2\u003eRegularization\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eRegularization\u003c/em\u003e\u003c/strong\u003e is a technique to help prevent overfitting in predictive modeling. We'll specifically discuss \u003cstrong\u003e\u003cem\u003eridge\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003elasso\u003c/em\u003e\u003c/strong\u003e regression, which are extensions to linear regression that include penalty terms to help prevent overfitting.\u003c/p\u003e\n\n\u003cp\u003eLasso regression in particular does not have a closed form solution and therefore must be solved using an alternative approach such as gradient descent. It also can be helpful for feature selection purposes.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eLinear regression can be used for prediction as well as inference. This has implications for the modeling techniques required, because the emphasis is not solely on the fit to the training data. In this section we'll go over the theoretical concerns as well as the practical approaches to tackling them.\u003c/p\u003e","exportId":"machine-learning-fundamentals-introduction"},{"id":455826,"title":"Data Science Processes","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-data-science-processes\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the different data science process frameworks\u003c/li\u003e\n\u003cli\u003eCompare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is a Data Science Process?\u003c/h2\u003e\n\n\u003cp\u003eData Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects.  In this lesson, you'll explore three of the most popular methodologies -- \u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e, and \u003cstrong\u003e\u003cem\u003eOSEMN\u003c/em\u003e\u003c/strong\u003e, and explore how you can make use of them to keep your projects well-structured and organized. \u003c/p\u003e\n\n\u003ch2\u003eCRoss-Industry Standard Process for Data Mining (CRISP-DM)\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\u003c/p\u003e\n\n\u003cp\u003eLet's take a look at the individual steps involved in CRISP-DM.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eBusiness Understanding:\u003c/em\u003e\u003c/strong\u003e  This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization.  Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you to clarify the requirements and make sure that everyone involved understands what the project is and isn't. \u003c/p\u003e\n\n\u003cp\u003eDuring this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project. \u003c/p\u003e\n\n\u003cp\u003eGood questions for this stage include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eWho are the stakeholders in this project? Who will be directly affected by the creation of this project?\u003c/li\u003e\n\u003cli\u003eWhat business problem(s) will this Data Science project solve for the organization?\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eWhat problems are inside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat problems are outside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat data sources are available to us?\u003c/li\u003e\n\u003cli\u003eWhat is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\u003c/li\u003e\n\u003cli\u003eDo stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Understanding:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project.  This step includes both understanding where our data is coming from, as well as the information contained within the data. \u003c/p\u003e\n\n\u003cp\u003eConsider the following questions when working through this stage:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eWhat data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\u003c/li\u003e\n\u003cli\u003eWho controls the data sources, and what steps are needed to get access to the data?\u003c/li\u003e\n\u003cli\u003eWhat is our target?\u003c/li\u003e\n\u003cli\u003eWhat predictors are available to us?\u003c/li\u003e\n\u003cli\u003eWhat data types are the predictors we'll be working with?\u003c/li\u003e\n\u003cli\u003eWhat is the distribution of our data?\u003c/li\u003e\n\u003cli\u003eHow many observations does our dataset contain? Do we have a lot of data? Only a little? \u003c/li\u003e\n\u003cli\u003eDo we have enough data to build a model? Will we need to use resampling methods?\u003c/li\u003e\n\u003cli\u003eHow do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Preparation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce we have a strong understanding of our data, we can move onto preparing the data for our modeling steps. \u003c/p\u003e\n\n\u003cp\u003eDuring this stage, we'll want to handle the following issues:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDetecting and dealing with missing values\u003c/li\u003e\n\u003cli\u003eData type conversions (e.g. numeric data mistakenly encoded as strings)\u003c/li\u003e\n\u003cli\u003eChecking for and removing multicollinearity (correlated predictors)\u003c/li\u003e\n\u003cli\u003eNormalizing our numeric data\u003c/li\u003e\n\u003cli\u003eConverting categorical data to numeric format through one-hot encoding\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModeling:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task. \u003c/p\u003e\n\n\u003cp\u003eConsider the following questions during the modeling step:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIs this a classification task? A regression task? Something else?\u003c/li\u003e\n\u003cli\u003eWhat models will we try?\u003c/li\u003e\n\u003cli\u003eHow do we deal with overfitting?\u003c/li\u003e\n\u003cli\u003eDo we need to use regularization or not?\u003c/li\u003e\n\u003cli\u003eWhat sort of validation strategy will we be using to check that our model works well on unseen data?\u003c/li\u003e\n\u003cli\u003eWhat loss functions will we use?\u003c/li\u003e\n\u003cli\u003eWhat threshold of performance do we consider as successful?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eEvaluation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eDuring this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem.  Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both \u003cem\u003eBusiness Understanding\u003c/em\u003e and \u003cem\u003eDeployment\u003c/em\u003e.  As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope.  Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.  \u003c/p\u003e\n\n\u003cp\u003eOf course, if the results are satisfactory, then we instead move onto deployment!\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eDeployment:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation.  If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights.  For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production. \u003c/p\u003e\n\n\u003cp\u003eThis is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\u003c/p\u003e\n\n\u003ch2\u003eKnowledge Discovery in Databases\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKnowledge Discovery in Databases\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, \u003ca href=\"https://www.kdnuggets.com/\"\u003ekdnuggets\u003c/a\u003e. If you're interested, read the original white paper on KDD, which can be found \u003ca href=\"https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf\"\u003ehere\u003c/a\u003e!\u003c/p\u003e\n\n\u003cp\u003eThe KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSelection\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.  \u003c/p\u003e\n\n\u003cp\u003eOnce you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you.  Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve.  During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.  \u003c/p\u003e\n\n\u003cp\u003eThe output of this stage is the dataset you'll be using for the Data Science project. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePreprocessing\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThe preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it.  For text data, this may include things like tokenization.  You'll also identify and deal with issues like outliers and/or missing data in this stage.  \u003c/p\u003e\n\n\u003cp\u003eIn practice, this stage often blurs with the \u003cem\u003eTransformation\u003c/em\u003e stage. \u003c/p\u003e\n\n\u003cp\u003eThe output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTransformation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling.  This may include steps like feature engineering and dimensionality reduction.  At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\u003c/p\u003e\n\n\u003cp\u003eThe output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Mining\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThe Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.  \u003c/p\u003e\n\n\u003cp\u003eYou may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\"  Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays. \u003c/p\u003e\n\n\u003cp\u003eThe output of this stage results from a fit to the data for the problem we're trying to solve.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretation/Evaluation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!).  Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".  \u003c/p\u003e\n\n\u003ch2\u003eOSEMN\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\"\u003e\n\u003ca href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\"\u003eAdapted from: KDNuggets\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThis brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc.  It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eObtain\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eAs with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eScrub\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, we'll focus on preprocessing our data.  Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage.  The line with this stage really blurs with the \u003cem\u003eExplore\u003c/em\u003e stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.  \u003c/p\u003e\n\n\u003cp\u003eNote that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration.  This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExplore\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThis step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the \u003cem\u003eScrub\u003c/em\u003e step mentioned above.  During this step, you'll create visualizations to really get a feel for your dataset.  You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that.  If your project is a classification task, you may check the balance of the different classes in your dataset.  If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.  \u003c/p\u003e\n\n\u003cp\u003eAt the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThis step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox.  In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results.  As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like \u003cem\u003eScrub\u003c/em\u003e or \u003cem\u003eExplore\u003c/em\u003e, and make some changes to see how it affects the model.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpret\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this step, you'll interpret the results of your model(s), and communicate results to stakeholders.  As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data.  That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.  \u003c/p\u003e\n\n\u003ch2\u003eA Note On Communicating Results\u003c/h2\u003e\n\n\u003cp\u003eRegardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eA short, high-level presentation covering your question, process, and results meant for non-technical audiences\u003c/li\u003e\n\u003cli\u003eA detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way.  Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\u003c/p\u003e\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way.\u003c/p\u003e","exportId":"data-science-processes"},{"id":455831,"title":"Statistical Learning Theory","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-stat-learning-theory-v2-5\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eBefore you get into building machine learning models, a basic understanding of statistical learning theory is essential.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the difference between modeling for inference and prediction\u003c/li\u003e\n\u003cli\u003eExplain generalization in the context of statistical modeling\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\n\u003cp\u003eStatistical learning theory is based on the idea of using data along with statistics to provide a framework for learning.\u003c/p\u003e\n\n\u003cp\u003eIn statistical learning theory, the main idea is to construct a \u003cstrong\u003emodel\u003c/strong\u003e to draw certain conclusions from data, and next, to use this model to make \u003cstrong\u003epredictions\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eThis builds on statistical modeling, which represents the relationship between independent and dependent variables as a mathematical equation. For parametric statistical models such as linear regression, this means that the model learns \u003cstrong\u003eparameters\u003c/strong\u003e that will be used for future predictions.\u003c/p\u003e\n\n\u003ch2\u003eInference vs. Prediction\u003c/h2\u003e\n\n\u003cp\u003eThere are two different modeling approaches to statistical modeling: modeling for \u003cstrong\u003e\u003cem\u003einference\u003c/em\u003e\u003c/strong\u003e and modeling for \u003cstrong\u003e\u003cem\u003eprediction\u003c/em\u003e\u003c/strong\u003e. A \"perfect\" model might be useful for both, but often your modeling strategy will need to be calibrated based on the goal of the model.\u003c/p\u003e\n\n\u003ch3\u003eInference\u003c/h3\u003e\n\n\u003cp\u003eWhen you are modeling for inference, you are asking the question:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhat is the relationship between \u003ccode\u003eX\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eand sometimes, if you have good reason to infer a causal relationship:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHow does \u003ccode\u003eX\u003c/code\u003e affect \u003ccode\u003ey\u003c/code\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003ewhere \u003ccode\u003eX\u003c/code\u003e is your collection of independent variables (i.e. features) and \u003ccode\u003ey\u003c/code\u003e is your dependent variable (i.e. target). The focus is on \u003cem\u003eunderstanding\u003c/em\u003e. Most of the history of statistics and all of the linear regression content so far has been focused on this approach.\u003c/p\u003e\n\n\u003cp\u003eWhen modeling for inference, it is important for the model to be \u003cstrong\u003estatistically significant\u003c/strong\u003e and \u003cstrong\u003einterpretable\u003c/strong\u003e, sometimes at the expense of overall model fit. Every feature used in the model should be carefully chosen based on underlying domain understanding.\u003c/p\u003e\n\n\u003ch3\u003ePrediction\u003c/h3\u003e\n\n\u003cp\u003eWhen you are modeling for prediction, you are asking the question:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHow well can I use \u003ccode\u003eX\u003c/code\u003e to predict \u003ccode\u003ey\u003c/code\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ccode\u003eX\u003c/code\u003e is still your collection of independent variables, and \u003ccode\u003ey\u003c/code\u003e is still your dependent variable. But you are less concerned about how and which features impact \u003ccode\u003ey\u003c/code\u003e as opposed to how you can efficiently use them to predict \u003ccode\u003ey\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhen modeling for prediction, it is important for the model to \u003cstrong\u003e\u003cem\u003egeneralize\u003c/em\u003e\u003c/strong\u003e to unseen data. This means that the overall model fit is more important than the coefficients of features or statistical significance, and that you will often use all available features rather than carefully choosing them. Both in terms of the number of features and in terms of the type of model used, predictive models tend to be more \u003cstrong\u003e\u003cem\u003ecomplex\u003c/em\u003e\u003c/strong\u003e than inferential models.\u003c/p\u003e\n\n\u003ch2\u003eModel Generalization\u003c/h2\u003e\n\n\u003cp\u003eThe model learns about the data during the \u003cstrong\u003e\u003cem\u003etraining\u003c/em\u003e\u003c/strong\u003e stage. Examples are presented to the model and the model tweaks its parameters to better understand the data.\u003c/p\u003e\n\n\u003cp\u003eOnce the training is over, the model is unleashed upon new data and then uses what it has learned to make predictions with that data. This is where problems can emerge. If we over-train the model on the training data -- i.e. make the model memorize every detail of the data it is shown -- it will be able to identify all the relevant information in the training data, but will fail miserably when presented with the new data. \u003c/p\u003e\n\n\u003cp\u003eWe then say that the \u003cstrong\u003emodel is not capable of generalizing\u003c/strong\u003e, or that the \u003cstrong\u003emodel is over-fitting the training data\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eLet's take a look at an example of the phenomenon: modeling happiness as a function of wealth. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5/master/images/new_happy.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the top three diagrams, we have data and models (dashed curves). From left to right the models have been trained longer and longer on the training data. The training error curve in the bottom box shows that the training error gets better and better as we train longer (increasing model complexity).\u003c/p\u003e\n\n\u003cp\u003eYou may think that if we train longer we'll get better! Well, yes, but \u003cstrong\u003eonly better at describing the training data\u003c/strong\u003e. The top right box shows a very complex model that hits all the data points. This model does great on the training data, but when presented with new data (examine the prediction error curve in the bottom box) then it does worse! The gap between the training error and prediction error for new data (labeled \"optimism\") is growing as model complexity increases, which means that we are getting \u003cem\u003eworse\u003c/em\u003e at generalizing.\u003c/p\u003e\n\n\u003cp\u003eIn order to create good predictive models in machine learning that are capable of generalizing, one needs to know when to stop training the model so that it doesn't over-fit.\u003c/p\u003e\n\n\u003ch3\u003eModel Validation\u003c/h3\u003e\n\n\u003cp\u003eAs the data which is available to us for modeling is finite, the available data needs to be used very effectively to build and \u003cstrong\u003evalidate\u003c/strong\u003e a model. Model validation is a process of measuring overfitting and indicates the degree of generalizability.\u003c/p\u003e\n\n\u003cp\u003eHere is how we perform validation, in its simplest form:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSplit the data into two parts with a 70/30, 80/20, or a similar split\u003c/li\u003e\n\u003cli\u003eUse the larger part for \u003cstrong\u003etraining\u003c/strong\u003e so the model learns from it\u003c/li\u003e\n\u003cli\u003eUse the smaller part for \u003cstrong\u003e\u003cem\u003etesting\u003c/em\u003e\u003c/strong\u003e the model\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis setup looks like as shown below:\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5/master/images/new_train_test_sets.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is called a \u003cstrong\u003e\u003cem\u003etrain-test split\u003c/em\u003e\u003c/strong\u003e and means that you can compare the model performance on training data vs. testing data using a given \u003cstrong\u003emetric\u003c/strong\u003e. The metric can be R-Squared or it can be an error-based metric like RMSE.\u003c/p\u003e\n\n\u003cp\u003eIf the metric is much better on the training data than the test data, this indicates overfitting. A model that generalizes well will have similar metrics on the two datasets.\u003c/p\u003e\n\n\u003cp\u003eAnother approach to validation is \u003cstrong\u003e\u003cem\u003ecross-validation\u003c/em\u003e\u003c/strong\u003e. This involves splitting the data multiple times and training multiple models, to get more of a distribution of possible metrics rather than relying on metrics from a single train-test split.\u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=rqJ8SrnmWu0\"\u003eYoutube: Introduction to Statistical Learning Theory\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.princeton.edu/%7Ekulkarni/Papers/Journals/j077_2011_KulHar_WileyTutorial.pdf\"\u003eAn Overview of Statistical Learning Theory with examples\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly looked at statistical learning theory and its main components. When modeling for inference rather than prediction, some additional conceptual considerations become important. In particular, predictive models should generalize to unseen data. Model validation is used to measure how well a model will generalize.\u003c/p\u003e","exportId":"statistical-learning-theory"},{"id":455835,"title":"Regression Model Validation","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-model-validation\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003ePreviously you've evaluated a multiple linear regression model by calculating metrics based on the fit of the training data. In this lesson you'll learn why it's important to split your data in a train and a test set if you want to evaluate a model used for prediction.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform a train-test split\u003c/li\u003e\n\u003cli\u003ePrepare training and testing data for modeling\u003c/li\u003e\n\u003cli\u003eCompare training and testing errors to determine if model is over or underfitting\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g7481fbdcbe748b61b8266678416afd43"},{"id":455839,"title":"Regression Model Validation - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-model-validation-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll be able to validate your Ames Housing data model using a train-test split.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform a train-test split\u003c/li\u003e\n\u003cli\u003ePrepare training and testing data for modeling\u003c/li\u003e\n\u003cli\u003eCompare training and testing errors to determine if model is over or underfitting\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g8878f371f80e2be164db9ca56270ac6f"},{"id":455844,"title":"Introduction to Cross-Validation","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-cross-validation\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eCross-validation is another model validation strategy, which addresses one of the limitations of the train-test split strategy.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the process of cross-validation\u003c/li\u003e\n\u003cli\u003ePerform cross-validation on a model\u003c/li\u003e\n\u003cli\u003eCompare and contrast model validation strategies\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gf75f69ba85f77765f91d254d7d93222e"},{"id":455848,"title":"Introduction to Cross-Validation - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-cross-validation-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll be able to practice your cross-validation skills!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform cross validation on a model\u003c/li\u003e\n\u003cli\u003eCompare and contrast model validation strategies\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g5c9102ed5e9c336f43723273d0ebd59b"},{"id":455852,"title":"Short Video: Cross-Validation","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding: 62.5% 0 0 0; position: relative;\"\u003e\u003ciframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" title=\"cross-val_phase2_gd\" src=\"https://player.vimeo.com/video/713478191?h=8a86a87a5e\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" allowfullscreen=\"allowfullscreen\" allow=\"autoplay; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-cross-validation"},{"id":455856,"title":"Bias-Variance Tradeoff","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bias-variance-trade-off\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've seen how you can extend your linear models by including interaction effects as well as polynomial terms. Including these in models comes at a price though: not only do the models become more complex (with more parameter estimates), adding more terms can potentially harm model performance when making predictions. This tradeoff between performance on the training data and performance making predictions is called the bias-variance tradeoff. You'll learn about that in this lesson.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the bias-variance tradeoff in machine learning \u003c/li\u003e\n\u003cli\u003eDiscuss how bias and variance are related to over and underfitting \u003c/li\u003e\n\u003cli\u003eList the three components of error \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g24fef19b98005baa41a82dd41eea0b54"},{"id":455860,"title":"Bias-Variance Tradeoff - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bias-variance-trade-off-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice the concepts you learned in the last lesson, bias-variance tradeoff. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDemonstrate the tradeoff between bias and variance by way of fitting a machine learning model \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g3b15a2f839ce8b5455258fa436cec3b0"},{"id":455864,"title":"Short Video: Bias-Variance Tradeoff","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding: 62.5% 0 0 0; position: relative;\"\u003e\u003ciframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" title=\"one-hot_encoding_phase2_gd\" src=\"https://player.vimeo.com/video/713478315?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" allowfullscreen=\"allowfullscreen\" allow=\"autoplay; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-bias-variance-tradeoff"},{"id":455867,"title":"Quiz: Machine Learning Fundamentals","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g9fcf8c2214c578d100cc0bec5e35577c","questionCount":5,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"gde2a7ea9ada6166586c9cf787822bfba"},{"id":455882,"title":" Inference vs. Prediction Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g4287b0c35fcf1baaaa7444da6d55b434","questionCount":7,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g63baf40c53d2206ad7b65a1501c08cbf"},{"id":455898,"title":"Model Validation and Data Leakage Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g403c51f0a915f7bf1d4798f721ac611c","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g343df895fe21388a003ee4f3f256d3e0"},{"id":455904,"title":"Ridge and Lasso Regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ridge-and-lasso-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAt this point, you've seen a number of criteria and algorithms for fitting regression models to data. You've seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You've also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\u003c/p\u003e\n\n\u003cp\u003eNow you're going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine lasso regression \u003c/li\u003e\n\u003cli\u003eDefine ridge regression \u003c/li\u003e\n\u003cli\u003eDescribe why standardization is necessary before ridge and lasso regression \u003c/li\u003e\n\u003cli\u003eCompare and contrast lasso, ridge, and non-regularized regression \u003c/li\u003e\n\u003cli\u003eUse lasso and ridge regression with scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g80f8d50fa5bf6e890812cf951e88626b"},{"id":455909,"title":"Ridge and Lasso Regression - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ridge-and-lasso-regression-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice your knowledge of ridge and lasso regression!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse lasso and ridge regression with scikit-learn \u003c/li\u003e\n\u003cli\u003eCompare and contrast lasso, ridge and non-regularized regression \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g7ff4049e3a4478aad77ba3a1b80487cc"},{"id":455913,"title":"Feature Selection Methods","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-feature-selection-methods\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the different techniques you can use to only use features that are most relevant to your model.\u003c/p\u003e\n\n\u003ch3\u003eObjectives\u003c/h3\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse feature selection to obtain the optimal subset of features in a dataset \u003c/li\u003e\n\u003cli\u003eIdentify when it is appropriate to use certain methods of feature selection \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gf5dc93ee053ea7f248ee15a7b680c8fe"},{"id":455917,"title":"Extensions to Linear Models - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-extensions-to-linear-models-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice many concepts you have learned so far, from adding interactions and polynomials to your model to regularization!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBuild a linear regression model with interactions and polynomial features \u003c/li\u003e\n\u003cli\u003eUse feature selection to obtain the optimal subset of features in a dataset\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gf751cd1332473f2e094bab2de0075b78"},{"id":455923,"title":"Quiz: Regularization","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"gb2c59f56f7a0911bf496fbe6b43a0eaf","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"gb7d6a7cdf1300cc8cb899aa98c8bc385"},{"id":455934,"title":"Regularization Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"gc88411b07bef5b5aa7ea7b53912ae392","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g15774678425387b9fbeab8a27d51575f"},{"id":455938,"title":"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g5ebdcf9e39a987128ef740ce30aa0d03","questionCount":1,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure what to do, refer to the instructions in \u003ca title=\"‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\" href=\"quizzes/g0c9a5bdde5f8af00ba4e30f35458c335\"\u003e‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\u003c/a\u003e\u003c/p\u003e","exportId":"g9d5b89d09608ee0da642cc68cf19da0e"},{"id":455944,"title":"Machine Learning Fundamentals - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ml-fundamentals-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you used a familiar model, linear regression, to learn about machine learning fundamentals.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eInference\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eprediction\u003c/em\u003e\u003c/strong\u003e are two different use cases for statistical modeling\n\n\u003cul\u003e\n\u003cli\u003eLinear regression can be used for both kinds of modeling\u003c/li\u003e\n\u003cli\u003eWhile inference is most interested in understanding relationships in the data, prediction is most interested in \u003cstrong\u003e\u003cem\u003egeneralization\u003c/em\u003e\u003c/strong\u003e, i.e. making good predictions on unseen data\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eModel validation\u003c/em\u003e\u003c/strong\u003e techniques allow you to measure how well your model generalizes\n\n\u003cul\u003e\n\u003cli\u003eThe two most popular validation techniques are \u003cstrong\u003e\u003cem\u003etrain-test split\u003c/em\u003e\u003c/strong\u003e (which randomly shuffles the data into a training set and a test set) and \u003cstrong\u003e\u003cem\u003ecross-validation\u003c/em\u003e\u003c/strong\u003e (which splits the data into folds and uses one fold at a time as the test set)\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003ePredictive modeling involves balancing between \u003cstrong\u003e\u003cem\u003ebias\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003evariance\u003c/em\u003e\u003c/strong\u003e\n\n\u003cul\u003e\n\u003cli\u003eBias is associated with \u003cstrong\u003e\u003cem\u003eunderfitting\u003c/em\u003e\u003c/strong\u003e and less-complex models\u003c/li\u003e\n\u003cli\u003eAn example of a less-complex model would be linear regression\u003c/li\u003e\n\u003cli\u003eA high-bias model will perform poorly on both the training and the test data\u003c/li\u003e\n\u003cli\u003eVariance is associated with \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e and more-complex models\u003c/li\u003e\n\u003cli\u003eAn example of a more-complex model would be polynomial regression\u003c/li\u003e\n\u003cli\u003eA high-variance model will perform well on the training data and poorly on the test data\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eRegularization\u003c/em\u003e\u003c/strong\u003e is a technique to reduce overfitting\n\n\u003cul\u003e\n\u003cli\u003eTwo popular penalized coefficient regularization techniques are \u003cstrong\u003e\u003cem\u003eridge\u003c/em\u003e\u003c/strong\u003e regression and \u003cstrong\u003e\u003cem\u003elasso\u003c/em\u003e\u003c/strong\u003e regression\u003c/li\u003e\n\u003cli\u003eLasso regression penalizes coefficients to 0, which means that it can be used for \u003cstrong\u003e\u003cem\u003efeature selection\u003c/em\u003e\u003c/strong\u003e in addition to other feature selection techniques such as recursive feature elimination\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003c/ul\u003e","exportId":"machine-learning-fundamentals-recap"}]},{"id":46961,"name":"Topic 24: Logistic Regression","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g3ca6a0af38c0b8fea8e33b3bcd24d8e1","items":[{"id":455954,"title":"Topic 24 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.8127%; height: 150px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eLogistic Regression 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.9107%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 9.57491%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression - Introduction\" href=\"pages/logistic-regression-introduction\"\u003eLogistic Regression - Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.9107%;\"\u003e\u003ca title=\"Introduction to Supervised Learning\" href=\"pages/introduction-to-supervised-learning\"\u003eIntroduction to Supervised Learning\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Linear to Logistic Regression\" href=\"assignments/ge143c152064afbb74f901a0771d431e0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12071\" data-api-returntype=\"Assignment\"\u003eLinear to Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Fitting a Logistic Regression Model - Lab\" href=\"assignments/g70829cf72bde217bb84b7b85a704288d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12072\" data-api-returntype=\"Assignment\"\u003eFitting a Logistic Regression Model - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression in scikit-learn\" href=\"assignments/gb8692713ef61b8e0a83cffd5b60950ae\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12073\" data-api-returntype=\"Assignment\"\u003eLogistic Regression in scikit-learn\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Logistic Regression in scikit-learn - Lab\" href=\"assignments/gb8cdf52053921f13266a12eab476e9eb\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12074\" data-api-returntype=\"Assignment\"\u003eLogistic Regression in scikit-learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.9107%;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Introduction to Logistic Regression\" href=\"quizzes/g8f0af8fb6b9743cf18b550abf04e6f7f\"\u003eQuiz: Introduction to Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 100%; height: 198px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eLogistic Regression 1\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eLogistic Regression 2\u0026nbsp;\u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.9107%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 9.57491%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression 1 Exit Ticket\" href=\"quizzes/g211cf52a1b09af29717a39b460ae425d\"\u003eLogistic Regression 1 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; text-align: center; height: 29px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/mle-review\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/mle-review\" data-api-returntype=\"Page\"\u003eMLE Review\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/mle-and-logistic-regression\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/mle-and-logistic-regression\" data-api-returntype=\"Page\"\u003eMLE and Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/gradient-descent-review\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/gradient-descent-review\" data-api-returntype=\"Page\"\u003eGradient Descent Review\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;Low priority\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Gradient Descent - Lab\" href=\"assignments/gd4dcde7ebdef12668ae7dbb3f568182b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12083\" data-api-returntype=\"Assignment\"\u003eGradient Descent - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;Low priority\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Coding Logistic Regression from Scratch - Lab\" href=\"assignments/g4819ae6efc6a7095e874f1dbb8a1ecb2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12084\" data-api-returntype=\"Assignment\"\u003eCoding Logistic Regression from Scratch - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.9064%; height: 76px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eLogistic Regression 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.9107%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 9.57491%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression 2 Exit Ticket\" href=\"quizzes/g0ec8da043796b87f5899023655c90b12\"\u003eLogistic Regression 2 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Logistic Regression - Recap\" href=\"pages/logistic-regression-recap\"\u003eLogistic Regression - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","exportId":"topic-24-lesson-priorities-live"},{"id":455958,"title":"Logistic Regression - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-intro-v2-4\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro-v2-4\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro-v2-4/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression and how it can be seen through a statistical point of view with maximum likelihood estimation (MLE). This should provide you some additional time to wrangle with statistical concepts and improve your overall coding abilities.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\u003c/p\u003e\n\n\u003cp\u003eWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\u003c/p\u003e\n\n\u003ch2\u003eMaximum Likelihood Estimation (MLE)\u003c/h2\u003e\n\n\u003cp\u003eMaximum likelihood estimation is a statistical procedure for determining underlying parameter distributions. As the name implies, the underlying motivation is to find parameters that maximize the theoretical chances of observing the actual observations.\u003c/p\u003e\n\n\u003ch2\u003eMLE and Logistic Regression\u003c/h2\u003e\n\n\u003cp\u003eLogistic regression, despite its name, is a classification algorithm. An interesting nuance is that it provides confidence values with its predictions since the raw output is a probability of a class between 0 and 1. The general process for this is similar to linear regression, where coefficients for various feature weights are altered in order to optimize the accuracy of subsequent predictions from the model.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIt's important to be aware of logistic regression as one of the most basic classifiers that you can use. In this section you'll learn how it works and how to use it.\u003c/p\u003e","exportId":"logistic-regression-introduction"},{"id":455962,"title":"Introduction to Supervised Learning","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll examine what exactly the term \"Supervised Learning\" means, and where it fits in Data Science. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the components of what makes something a supervised learning task \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Supervised Learning?\u003c/h2\u003e\n\n\u003cp\u003eThe term \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e refers to a class of machine learning algorithms that can \"learn\" a task through \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. We'll explore this definition more fully in a bit -- but first, it's worth taking some time to understand where supervised learning fits in the overall picture in regards to Data Science. By now, you've probably noticed that many of the things we've learned in Data Science and Computer Science are very hierarchical. This is especially true when it comes to AI and Machine Learning. Let's break down the hierarchy a bit, and see where \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e fits.  \u003c/p\u003e\n\n\u003ch2\u003eArtificial Intelligence\u003c/h2\u003e\n\n\u003cp\u003eAt the top of the hierarchy is \u003cstrong\u003e\u003cem\u003eArtificial Intelligence\u003c/em\u003e\u003c/strong\u003e.  AI is a catch-all term for various kinds of algorithms that can complete tasks that normally require human intelligence to complete. AI is made up of several subcategories, and is also a subcategory itself in the greater hierarchy of Computer Science. When data scientists talk about AI, we're almost focused on a single branch of AI, \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e. Machine Learning is responsible for the boom in AI technologies and abilities in the last few decades, but it's worth noting that there are other areas of AI that do not fall under the umbrella of 'Machine Learning'. Other branches of AI include things like \u003cem\u003eGenetic Algorithms\u003c/em\u003e for optimization, or rules-based AI for things like building a bot for players to play against in a video game. While these are still active areas of research, they have little to no application in Data Science, so they're beyond the scope of this lesson. In general, when you see the phrase 'Artificial Intelligence', it's generally safe to assume that that the speaker is probably referring to the subfield of AI known as \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e (which is also sometimes referred to by it's older, more traditional name -- \u003cstrong\u003e\u003cem\u003eStatistical Learning\u003c/em\u003e\u003c/strong\u003e).\u003c/p\u003e\n\n\u003cp\u003eThe following graphic shows the breakdown of the 'Machine Learning' branch of AI:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/master/images/new_ml-hierarchy.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eMachine Learning\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eMachine Learning\u003c/em\u003e can be further divided into two overall categories:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe main difference between these two areas of machine learning is the need for \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. In \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e, any data used must have a \u003cstrong\u003e\u003cem\u003elabel\u003c/em\u003e\u003c/strong\u003e. These labels are the \u003cem\u003eground truth\u003c/em\u003e , which allows our supervised learning algorithms to 'check their work'. By comparing its predictions against the actual labels, our algorithm can learn to make less incorrect predictions and improve the overall performance of the task its learning to do. It helps to think of Supervised Learning as close to the type of learning we do as students in grade school. Imagine using practice exams to study for the SAT or ACT test. We can go through all the practice questions we want, but in order to learn from our performance on those practice questions, we need to know what the correct answers are! Without them, we would have no way of knowing which questions we got right and which ones we got wrong, so we wouldn't be able to learn what changes we would need to make to improve our overall performance! \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"A computer program is said to \u003cstrong\u003elearn\u003c/strong\u003e from experience \u003cem\u003eE\u003c/em\u003e with respect to some class of tasks \u003cem\u003eT\u003c/em\u003e and performance measure \u003cem\u003eP\u003c/em\u003e, if its performance at tasks in \u003cem\u003eT\u003c/em\u003e, as measured by \u003cem\u003eP\u003c/em\u003e, improves with experience \u003cem\u003eE\u003c/em\u003e.\"  -- \u003ca href=\"http://www.cs.cmu.edu/%7Etom/\"\u003eTom Mitchell\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLet's pretend we've built and trained a model to detect if a picture contains a cat or not. Using the language from the definition above:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eTask (T)\u003c/strong\u003e: predict if a picture contains a cat or not\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePerformance Measure (P)\u003c/strong\u003e: The objective function used to score the predictions made by our model for each image\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eExperience (E)\u003c/strong\u003e: All of our labeled training data. The more training data we provide, the more 'experience' our model gets!\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe'll spend some time learning about \u003cstrong\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/strong\u003e in the next module, so don't worry about it for now!\u003c/p\u003e\n\n\u003ch2\u003eClassification and Regression\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eSupervised Learning\u003c/em\u003e can be further broken down into two categories -- \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e. At this point in your studies, you already have significant experience with regression -- specifically \u003cstrong\u003eLinear Regression\u003c/strong\u003e , probably the most foundational (and important) machine learning model. Recall that regression allows us to answer questions like \"how much?\" or \"how many?\". If our label is a real-valued number, then the supervised learning problem you're trying to solve is a \u003cem\u003eregression\u003c/em\u003e problem. \u003c/p\u003e\n\n\u003cp\u003eThe other main kind of supervised learning problem is \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e. Classification allows us to tell if something belongs to one class or the other. In the case of the \u003ca href=\"https://www.kaggle.com/c/titanic\"\u003etitanic\u003c/a\u003e dataset, this may be something like survival. For example, given various characteristics of a passenger, predict whether they will survive or not. Questions that can be answered in a True/False format (in the titanic example, \"Survived\" or \"Not survived\") are a type of \u003cstrong\u003e\u003cem\u003eBinary Classification\u003c/em\u003e\u003c/strong\u003e. To perform binary classification, you will be introduced to \u003cstrong\u003eLogistic Regression\u003c/strong\u003e. Don't let the name confuse you, although the name contains the word \"regression,\" this important foundational technique is very important in understanding classification problems. There are several other classification techniques you will be learning in this module, but in order to gain a sound understanding of \u003cstrong\u003eClassification\u003c/strong\u003e tasks, this section will be focused exclusively on building and evaluating logistic regression models. \u003c/p\u003e\n\n\u003cp\u003eHowever, we are not limited to only two classes when working with classification algorithms -- we can have as many classes as we see fit. When a supervised learning problem has more than two classes, we refer to it as a \u003cstrong\u003e\u003cem\u003eMulticlass Classification\u003c/em\u003e\u003c/strong\u003e problem. \u003c/p\u003e\n\n\u003ch2\u003eObjective Functions\u003c/h2\u003e\n\n\u003cp\u003eWhenever we're dealing with supervised learning, we have an \u003cstrong\u003e\u003cem\u003eObjective Function\u003c/em\u003e\u003c/strong\u003e (also commonly called a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e) that we're trying to optimize against. Regardless of the supervised learning model we're working with, we can be sure that we have some sort of function under the hood that we're using to grade the predictions made by our model against the actual ground-truth labels for each prediction. In the quote from Tom Mitchell listed above, objective functions are \u003cem\u003eP\u003c/em\u003e. While classification and regression models use different kinds of objective functions to evaluate their performance, the concept is the same -- these functions allow the model to evaluate exactly how right or wrong a prediction is, which the algorithm can then \"learn\" from. These objective functions serve an important purpose, because they act as the ground-truth for determining if our model is getting better or not. \u003c/p\u003e\n\n\u003ch3\u003eThe Limitations of Labeled Data\u003c/h3\u003e\n\n\u003cp\u003eBecause supervised learning requires \u003cstrong\u003e\u003cem\u003eLabels\u003c/em\u003e\u003c/strong\u003e for any data used, this severely limits the amount of available data we have for use with supervised learning algorithms.  Of all the data in the world, only a very, very small percentage is labeled. Why? Because labeling data is a purposeful activity that can only be done by humans, and is therefore time-consuming and expensive. In supervised learning, labels are not universal -- they are unique to the problem we're trying to solve. If we're trying to train a model to predict if someone survived the titanic disaster, we need to know the survival results of every passenger in our dataset -- there's no way around it. However, if we're trying to predict how much a person paid for a ticket on the titanic, survival data now no longer works as a label -- instead, we need to know how much each passenger paid for a ticket. In a more generalized sense, this means that for whatever problem we're trying to train a supervised learning model to solve, we need to have a large enough dataset containing examples where humans have already done the things we're trying to get our model to learn how to do.  \u003c/p\u003e\n\n\u003cp\u003eAlthough labeled data is still expensive and time-consuming to get, the internet has made the overall process of getting labeled data a bit easier than it used to be. Nowadays, when companies need to construct a dataset of labeled training data to solve a problem, they typically make use of services like Amazon's \u003ca href=\"https://docs.aws.amazon.com/mturk/index.html\"\u003eAWS Mechanical Turk\u003c/a\u003e, or 'MTurk' for short. Services like this obtain labels by paying people for each label they generate. In this way, a company can crowdsource the work to label the training data needed. The company simply uploads unlabeled training data like an image, and a \"turker\" will then provide a label for that image according to the instructions from the company. Depending on the problem the company is trying to solve, the label for the image might be something as simple as the word \"cat\", or as complex as as boxes drawn around all the cats in the image. \u003c/p\u003e\n\n\u003ch3\u003eNegative Examples\u003c/h3\u003e\n\n\u003cp\u003eWhen creating a labeled dataset for a classification problem, it is worth noting that negative examples are just as important to be included in the dataset as positive examples. If our training data in the titanic dataset only contained data on passengers that all survived, no supervised learning algorithm would be able to learn how to predict if a passenger survived or died with any sort of accuracy. \u003cstrong\u003e\u003cem\u003ePositive Examples\u003c/em\u003e\u003c/strong\u003e are data points that belong to the class we're training our model to recognize. For instance, let's pretend we're building a model to tell if a picture is of a cat or not. All the pictures of cats in our dataset would be positive examples. However, in order to build a good cat classifier, our dataset would also need to contain many different kinds of pictures that don't include cats. Intuitively, this makes sense -- if every picture that our model ever saw had a cat in it, then the only thing that model will learn is that everything is a cat. To truly learn what we need it to learn, this model will also need to learn what a cat \u003cem\u003eisn't\u003c/em\u003e, by looking at pictures that don't include cats -- our \u003cstrong\u003e\u003cem\u003eNegative Examples\u003c/em\u003e\u003c/strong\u003e. In this way, with a complex enough model and enough labeled training data, our classifier will eventually learn that the differentiating factor between images with positive labels and images with negative labels are the shapes and patterns common to cats, but not dogs (or other animals). In this way, supervised learning can be a bit tricky. For instance, if all of the negative examples in our cat classifier dataset are of cars and houses, then the model will almost certainly get a picture of a dog incorrect by predicting that the picture is of a cat. Why does this happen? Because the model hasn't seen a dog before, and therefore has no idea whether this fits. In this particular example, we can guess that any picture of a dog will look more like a cat than it would a house or car, which from the model's perspective means that this is probably a picture of a cat. \u003c/p\u003e\n\n\u003cp\u003eIn summary, this part of supervised learning can often be more art than science -- when creating a dataset, make sure that your dataset contains enough negative examples, and that you are very thoughtful about what those negative examples actually contain! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about \u003cem\u003eSupervised Learning\u003c/em\u003e, and where it fits in relation to Machine Learning and Artificial Intelligence. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"introduction-to-supervised-learning"},{"id":455967,"title":"Linear to Logistic regression","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-to-logistic-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll be introduced to the logistic regression model. You'll start with an introductory example using linear regression, which you've seen before, to act as a segue into logistic regression. After that, you'll learn about the formal notation of logistic regression models. Then, you'll conclude this lesson by looking at a real-world example.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the need for logistic regression\u003c/li\u003e\n\u003cli\u003eInterpret the parameters of a logistic regression model\u003c/li\u003e\n\u003c/ul\u003e","exportId":"ge143c152064afbb74f901a0771d431e0"},{"id":455973,"title":"Fitting a Logistic Regression Model - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-fitting-a-logistic-regression-model-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you'll be investigating fitting logistic regressions with \u003ccode\u003estatsmodels\u003c/code\u003e. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the \u003ca href=\"https://www.kaggle.com/c/titanic/data\"\u003eTitanic\u003c/a\u003e shipwreck or not (yes, it's a bit morbid).\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement logistic regression with \u003ccode\u003estatsmodels\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003eInterpret the statistical results associated with model parameters\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g70829cf72bde217bb84b7b85a704288d"},{"id":455979,"title":"Logistic Regression in scikit-learn","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-in-scikit-learn\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for \u003ccode\u003estatsmodels\u003c/code\u003e. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\u003c/p\u003e\n\n\u003cp\u003eThe other main process of model building and evaluation which we didn't discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (\u003ccode\u003eX\u003c/code\u003e).\u003c/p\u003e\n\n\u003cp\u003eWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFit a logistic regression model using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gb8692713ef61b8e0a83cffd5b60950ae"},{"id":455985,"title":"Logistic Regression in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-in-scikit-learn-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled \u003ccode\u003e'target'\u003c/code\u003e. 1 is for positive for heart disease while 0 indicates no heart disease.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFit a logistic regression model using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gb8cdf52053921f13266a12eab476e9eb"},{"id":455990,"title":"Quiz: Introduction to Logistic Regression","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g66f71747bc93011e9c6c3c1f31f08f37","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"g8f0af8fb6b9743cf18b550abf04e6f7f"},{"id":456005,"title":"Logistic Regression 1 Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"gd6d3c3e505fd36534136395582d2a713","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g211cf52a1b09af29717a39b460ae425d"},{"id":456010,"title":"MLE Review","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-review\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've seen MLE (Maximum Likelihood Estimation) when discussing Bayesian statistics, but did you know logistic regression can also be seen from this statistical perspective? In this section, you'll gain a deeper understanding of logistic regression by coding it from scratch and analyzing the statistical motivations backing it. But first take some time to review maximum likelihood estimation.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how to take MLE of a binomial variable \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eMLE\u003c/h2\u003e\n\n\u003cp\u003eMaximum likelihood estimation can often sound academic, confusing, and cryptic when first introduced. It is often presented and introduced with complex integrals of statistical distributions that scare away many readers. Hopefully, this hasn't been your experience. While the mathematics can quickly become complex, the underlying concepts are actually quite intuitive.\u003c/p\u003e\n\n\u003cp\u003eTo demonstrate this, imagine a simple coin flipping example. Let's say that you flip a coin 100 times and get 55 heads. Maximum likelihood estimation attempts to uncover the underlying theoretical probability of this coin landing on heads given your observations. In other words, given the observations, what is the chance that the coin was fair and had a 0.5 chance of landing on heads each time? Or what is the chance that the coin actually had a 0.75 probability of lands of heads, given what we observed? It turns out that the answer to these questions is rather intuitive. If you observe 55 out of 100 coin flips, the underlying probability which maximizes the chance of us observing 55 out of 100 coin flips is 0.55. In this simple example, MLE simply returns the current sample mean as the underlying parameter that makes the observations most probable. Slight deviations to this would be almost as probable but slightly less so, and large deviations from our sample mean should be rare. This intuitively makes some sense; as your sample size increases, you expect the sample mean to converge to the true underlying parameter. MLE takes a flipped perspective, asking what underlying parameter is most probable given the observations.\u003c/p\u003e\n\n\u003ch2\u003eLog-likelihood\u003c/h2\u003e\n\n\u003cp\u003eWhen calculating maximum likelihood, it is common to use the log-likelihood, as taking the logarithm can simplify calculations. For example, taking the logarithm of a set of products allows you to decompose the problem from products into sums. (You may recall from high school mathematics that \u003cimg class=\"equation_image\" title=\"x^{(a+b)} = x^a \\bullet x^b\" src=\"/equation_images/x^{(a+b)}%20=%20x^a%20%255Cbullet%20x^b\" alt=\"{\" data-equation-content=\"x^{(a+b)} = x^a \\bullet x^b\"\u003e. Similarly, taking the logarithm of both sides of a function allows you to transform products into sums. \u003c/p\u003e\n\n\u003ch2\u003eMLE for a binomial variable\u003c/h2\u003e\n\n\u003cp\u003eLet's take a deeper mathematical investigation into the coin flipping example above. \u003c/p\u003e\n\n\u003cp\u003eIn general, if you were to observe \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e flips, you would have observations \u003cimg class=\"equation_image\" title=\"y_1, y_2, ..., y_n\" src=\"https://learning.flatironschool.com/equation_images/y_1,%20y_2,%20...,%20y_n\" alt=\"{\" data-equation-content=\"y_1, y_2, ..., y_n\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn maximum likelihood estimation, you are looking to maximize the likelihood:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"L(p) = L(y_1, y_2, ..., y_n | p) = p^y (1-p)^{n-y}\" src=\"/equation_images/L(p)%20=%20L(y_1,%20y_2,%20...,%20y_n%20|%20p)%20=%20p^y%20(1-p)^{n-y}\" alt=\"{\" data-equation-content=\"L(p) = L(y_1, y_2, ..., y_n | p) = p^y (1-p)^{n-y}\"\u003e  where \u003cimg class=\"equation_image\" title=\" y = \\sum_{i=1}^{n}y_i\" src=\"/equation_images/%20y%20=%20%255Csum_{i=1}^{n}y_i\" alt=\"{\" data-equation-content=\" y = \\sum_{i=1}^{n}y_i\"\u003e\u003c/p\u003e\n\n\u003cp\u003eTaking the log of both sides:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"ln[L(p)] = ln[p^y (1-p)^{n-y}] = y ln(p)+(n-y)ln(1-p)\" src=\"/equation_images/ln[L(p)]%20=%20ln[p^y%20(1-p)^{n-y}]%20=%20y%20ln(p)+(n-y)ln(1-p)\" alt=\"{\" data-equation-content=\"ln[L(p)] = ln[p^y (1-p)^{n-y}] = y ln(p)+(n-y)ln(1-p)\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIf \u003cimg class=\"equation_image\" title=\"y = 1, 2, ..., n-1\" src=\"https://learning.flatironschool.com/equation_images/y%20=%201,%202,%20...,%20n-1\" alt=\"{\" data-equation-content=\"y = 1, 2, ..., n-1\"\u003e the derivative of \u003cimg class=\"equation_image\" title=\"ln[L(p)]\" src=\"/equation_images/ln[L(p)]\" alt=\"{\" data-equation-content=\"ln[L(p)]\"\u003e with respect to \u003cimg class=\"equation_image\" title=\"p\" src=\"https://learning.flatironschool.com/equation_images/p\" alt=\"{\" data-equation-content=\"p\"\u003e is:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\frac{d\\,ln[L(p)]}{dp} = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\" src=\"/equation_images/%255Cfrac{d%255C,ln[L(p)]}{dp}%20=%20y%20(%255Cfrac{1}{p})+(n-y)(%255Cfrac{-1}{1-p})\" alt=\"{\" data-equation-content=\"\\frac{d\\,ln[L(p)]}{dp} = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\"\u003e  \u003c/p\u003e\n\n\u003cp\u003eAs you've seen previously, the maximum will then occur when the derivative equals zero:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"0 = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\" src=\"/equation_images/0%20=%20y%20(%255Cfrac{1}{p})+(n-y)(%255Cfrac{-1}{1-p})\" alt=\"{\" data-equation-content=\"0 = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\"\u003e\u003c/p\u003e\n\n\u003cp\u003eDistributing, you have\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"0 = \\frac{y}{p} - \\frac{n-y}{1-p}\" src=\"/equation_images/0%20=%20%255Cfrac{y}{p}%20-%20%255Cfrac{n-y}{1-p}\" alt=\"{\" data-equation-content=\"0 = \\frac{y}{p} - \\frac{n-y}{1-p}\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAnd solving for p: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\frac{n-y}{1-p} = \\frac{y}{p} \" src=\"/equation_images/%20%255Cfrac{n-y}{1-p}%20=%20%255Cfrac{y}{p}\" alt=\"{\" data-equation-content=\" \\frac{n-y}{1-p} = \\frac{y}{p} \"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"p(n-y) = \\frac{y(1-p)}{p}\" src=\"/equation_images/p(n-y)%20=%20%255Cfrac{y(1-p)}{p}\" alt=\"{\" data-equation-content=\"p(n-y) = \\frac{y(1-p)}{p}\"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"\\frac{n-y}{y} = \\frac{1-p}{p}\" src=\"/equation_images/%255Cfrac{n-y}{y}%20=%20%255Cfrac{1-p}{p}\" alt=\"{\" data-equation-content=\"\\frac{n-y}{y} = \\frac{1-p}{p}\"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"\\frac{n}{y}-1 = \\frac{1}{p}-1\" src=\"/equation_images/%255Cfrac{n}{y}-1%20=%20%255Cfrac{1}{p}-1\" alt=\"{\" data-equation-content=\"\\frac{n}{y}-1 = \\frac{1}{p}-1\"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"\\frac{n}{y} = \\frac{1}{p} \" src=\"/equation_images/%255Cfrac{n}{y}%20=%20%255Cfrac{1}{p}\" alt=\"{\" data-equation-content=\"\\frac{n}{y} = \\frac{1}{p} \"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"p = \\frac{y}{n}\" src=\"/equation_images/p%20=%20%255Cfrac{y}{n}\" alt=\"{\" data-equation-content=\"p = \\frac{y}{n}\"\u003e  \u003c/p\u003e\n\n\u003cp\u003eAnd voil√†, you've verified the intuitive solution discussed above; the maximum likelihood for a binomial sample is the observed frequency!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly reviewed maximum likelihood estimation. In the upcoming lesson, you'll see how logistic regression can also be interpreted from this framework, which will help set the stage for you to code a logistic regression function from scratch using NumPy. Continue on to the next lesson to take a look at how this works for logistic regression.\u003c/p\u003e","exportId":"mle-review"},{"id":456013,"title":"MLE and Logistic Regression","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-logistic-regression\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDetermine how MLE is tied into logistic regression \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eMLE formulation\u003c/h2\u003e\n\n\u003cp\u003eAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.  \u003c/p\u003e\n\n\u003cp\u003eFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor \u003cimg class=\"equation_image\" title=\"X\" src=\"https://learning.flatironschool.com/equation_images/X\" alt=\"{\" data-equation-content=\"X\"\u003e as:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\pi_i=Pr(Y_i=1|X_i=x_i)=\\dfrac{\\text{exp}(\\beta_0+\\beta_1 x_i)}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\" src=\"/equation_images/%255Cpi_i=Pr(Y_i=1|X_i=x_i)=%255Cdfrac{%255Ctext{exp}(%255Cbeta_0+%255Cbeta_1%20x_i)}{1+%255Ctext{exp}(%255Cbeta_0+%255Cbeta_1%20x_i)}\" alt=\"{\" data-equation-content=\"\\pi_i=Pr(Y_i=1|X_i=x_i)=\\dfrac{\\text{exp}(\\beta_0+\\beta_1 x_i)}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is the standard linear regression model (\u003cimg class=\"equation_image\" title=\"\\beta_0+\\beta_1 x_i\" src=\"https://learning.flatironschool.com/equation_images/%255Cbeta_0+%255Cbeta_1%20x_i\" alt=\"{\" data-equation-content=\"\\beta_0+\\beta_1 x_i\"\u003e) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\u003c/p\u003e\n\n\u003cp\u003eThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\" src=\"/equation_images/%20L(%255Cbeta_0,%255Cbeta_1)=%255Cprod%255Climits_{i=1}^N%20%255Cpi_i^{y_i}(1-%255Cpi_i)^{n_i-y_i}=%255Cprod%255Climits_{i=1}^N%20%255Cdfrac{%255Ctext{exp}{y_i(%255Cbeta_0+%255Cbeta_1%20x_i)}}{1+%255Ctext{exp}(%255Cbeta_0+%255Cbeta_1%20x_i)}\" alt=\"{\" data-equation-content=\" L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\"\u003e   \u003c/p\u003e\n\n\u003ch2\u003eNotes on mathematical symbols\u003c/h2\u003e\n\n\u003cp\u003eRecall that the \u003cimg class=\"equation_image\" title=\"\\prod\" src=\"https://learning.flatironschool.com/equation_images/%255Cprod\" alt=\"{\" data-equation-content=\"\\prod\"\u003e sign stands for a product of each of these individual probabilities. (Similar to how \u003cimg class=\"equation_image\" title=\"\\sum\" src=\"https://learning.flatironschool.com/equation_images/%255Csum\" alt=\"{\" data-equation-content=\"\\sum\"\u003e stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.  \u003c/p\u003e\n\n\u003ch2\u003eAlgorithm bias and ethical concerns\u003c/h2\u003e\n\n\u003cp\u003eIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\u003c/p\u003e\n\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\n\u003cp\u003eBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases: \u003c/p\u003e\n\n\u003ch3\u003eAlgorithm bias and ethical concerns\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\"\u003eMachine Bias\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.bloomberg.com/opinion/articles/2018-10-16/amazon-s-gender-biased-algorithm-is-not-alone\"\u003eAmazon‚Äôs Gender-Biased Algorithm Is Not Alone\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/business/2017/12/21/the-software-that-runs-our-lives-can-bigoted-and-unfair-but-can-fix/RK4xG4gYxcVNVTIubeC1JI/story.html\"\u003eThe software that runs our lives can be bigoted and unfair. But we can fix it\u003c/a\u003e  \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html\"\u003eWhy artificial intelligence is far too human\u003c/a\u003e   \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms\"\u003eCan Computers Be Racist? The Human-Like Bias Of Algorithms\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eAdditional mathematical resources\u003c/h3\u003e\n\n\u003cp\u003eIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: \u003ca href=\"https://web.stanford.edu/%7Ehastie/ElemStatLearn//\"\u003ehttps://web.stanford.edu/~hastie/ElemStatLearn//\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\u003c/p\u003e","exportId":"mle-and-logistic-regression"},{"id":456016,"title":"Gradient Descent Review","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-review\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eRecall that gradient descent is a numerical approximation method for finding optimized solutions to problems with no closed form. That is, some mathematical problems are very easy to solve analytically. A trivial example is basic algebra problems which you undoubtedly saw in grade school:\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"x + 2 = 10\" src=\"https://learning.flatironschool.com/equation_images/x%20+%202%20=%2010\" alt=\"{\" data-equation-content=\"x + 2 = 10\"\u003e subtracting 2 from both sides you get \u003cimg class=\"equation_image\" title=\"x = 8\" src=\"https://learning.flatironschool.com/equation_images/x%20=%208\" alt=\"{\" data-equation-content=\"x = 8\"\u003e. Similarly, some more complex mathematical problems such as ordinary least squares, our preliminary regression approach, also have closed-form solutions where we can follow a rote procedure and be guaranteed a solution. In other cases, this is not possible and numerical approximation methods are used to find a solution. The first instance that you witnessed of this was adding the L1 and L2 (lasso and ridge, respectively) penalties to OLS regression. In these cases, numerical approximation methods, such as gradient descent, are used in order to find optimal or near-optimal solutions.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the elements of gradient descent in the context of a logistic regression \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGradient descent\u003c/h2\u003e\n\n\u003cp\u003eGradient descent is grounded in basic calculus theory. Whenever you have a minimum or maximum, the derivative at that point is equal to zero. This is displayed visually in the picture below; the slope of the red tangent lines is equal to the derivative of the curve at that point. As you can see, the slope of all of these horizontal tangent lines will be zero. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_dxdy0.png\" alt=\"higher-order polynomial graph with horizontal bars at all minima and maxima\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eThe gradient is simply another term for the derivative. Typically, this is the term used when we are dealing with multivariate data. The gradient is the rate of change, which is also the slope of the line tangent.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eBuilding upon this, gradient descent attempts to find the minimum of a function by taking successive steps in the steepest direction downhill.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_gradient.png\" alt=\"on the left, a 3d plot of a higher-order polynomial. on the right, an image representing finding a specific minimum\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhile this process guarantees a local minimum, the starting point and step size can affect the outcome. For example, for two different runs of gradient descent, one may lead to the global minimum while the other may lead to a local minimum.\u003c/p\u003e\n\n\u003cp\u003eRecall that the general outline for gradient descent is:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eDefine initial parameters:\n\n\u003col\u003e\n\u003cli\u003ePick a starting point\u003c/li\u003e\n\u003cli\u003ePick a step size \u003cimg class=\"equation_image\" title=\"\\alpha\" src=\"https://learning.flatironschool.com/equation_images/%255Calpha\" alt=\"{\" data-equation-content=\"\\alpha\"\u003e (alpha)\u003c/li\u003e\n\u003cli\u003eChoose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\u003c/li\u003e\n\u003cli\u003e(optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligible difference \u003c/li\u003e\n\u003c/ol\u003e\u003c/li\u003e\n\u003cli\u003eCalculate the gradient at the current point (initially, the starting point)\u003c/li\u003e\n\u003cli\u003eTake a step (of size alpha) in the direction of the gradient\u003c/li\u003e\n\u003cli\u003eRepeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly reviewed that a gradient is the derivative of a function, which is the rate of change at a specific point. You then reviewed the intuition behind gradient descent, as well as some of its pitfalls. Finally, you saw a brief outline of the algorithm itself. In the next lab, you'll practice coding gradient descent and applying that to some simple mathematical functions.\u003c/p\u003e","exportId":"gradient-descent-review"},{"id":456020,"title":"Gradient Descent - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll continue to formalize your knowledge of gradient descent by coding the algorithm yourself. In the upcoming labs, you'll apply similar procedures to implement logistic regression on your own.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement gradient descent from scratch to minimize OLS\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gd4dcde7ebdef12668ae7dbb3f568182b"},{"id":456025,"title":"Coding Logistic Regression From Scratch - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-coding-logistic-regression-from-scratch\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBuild a logistic regression model from scratch using gradient descent \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g4819ae6efc6a7095e874f1dbb8a1ecb2"},{"id":456039,"title":"Logistic Regression 2 Exit Ticket","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"ga65380dbf7600a45461766bdf5eabbb9","questionCount":9,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g0ec8da043796b87f5899023655c90b12"},{"id":456042,"title":"Logistic Regression - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eWell done! In this section you learned about a different supervised learning technique: classification.\u003c/p\u003e\n\n\u003cp\u003eYou also reviewed maximum likelihood estimation and saw how it applies to logistic regression. This included writing some challenging code, including gradient descent, which pushed you to think critically regarding algorithm implementation.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eSpecifically, you practiced building a very basic classification model from scratch - a logistic regression model. Logistic regression uses a sigmoid function which helps to plot an \"s\"-like curve that enables a linear function to act as a binary classifier.\u003c/p\u003e\n\n\u003ch2\u003eLog-likelihoods in Maximum Likelihood Estimation\u003c/h2\u003e\n\n\u003cp\u003eOne of the nuances you saw in maximum likelihood estimation was that of log-likelihoods. Recall that the purpose of taking log-likelihoods as opposed to likelihoods themselves is that it allows us to decompose the product of probabilities as sums of log probabilities. Analytically, this is essential to calculating subsequent gradients in order to find the next steps for our optimization algorithm.\u003c/p\u003e\n\n\u003ch2\u003eLocal Minima in Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eOne of the most important notes from this section is that \u003cstrong\u003egradient descent does not guarantee an optimal solution\u003c/strong\u003e. Gradient descent is meant to find optimal solutions, but it only guarantees a local minimum. For this reason, gradient descent is frequently run multiple times, and the parameters with the lowest loss function then being selected for the final model.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section was designed to give you additional practice coding algorithms in Python, and a deeper understanding of how iterative algorithms such as logistic regression converge to produce underlying model parameters.\u003c/p\u003e","exportId":"logistic-regression-recap"}]},{"id":46965,"name":"Topic 25: Classification Metrics","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gec1b618791ced2ade978d083978119b0","items":[{"id":456055,"title":"Topic 25 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.859%; height: 123px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eClassification Metrics 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.4896%;\"\u003e\u003ca title=\"Classification Metrics - Introduction\" href=\"pages/classification-metrics-introduction\"\u003eClassification Metrics - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca class=\"inline_disabled\" href=\"assignments/gee82ca01b6cbad81fe60f2bfd74f5c6f\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12075\" data-api-returntype=\"Assignment\"\u003eConfusion Matrices\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Visualizing Confusion Matrices - Lab\" href=\"assignments/gfc4cabc49a2b5825bcd2280f966121f2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12076\" data-api-returntype=\"Assignment\"\u003eVisualizing Confusion Matrices - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/evaluation-metrics\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/evaluation-metrics\" data-api-returntype=\"Page\"\u003eEvaluation Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Evaluating Logistic Regression Models - Lab\" href=\"assignments/g163b0dfb05702dcac7b6cd220709bd40\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12078\" data-api-returntype=\"Assignment\"\u003eEvaluating Logistic Regression Models - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.859%; height: 113px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Metrics 1\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eClassification Metrics 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Classification Metrics 1 Exit Ticket\" href=\"quizzes/g496587f746349f84af2ab1d3597060af\"\u003eClassification Metrics 1 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"ROC Curves and AUC\" href=\"assignments/g4bde6387fe8446d8a78599461096f6fa\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12079\" data-api-returntype=\"Assignment\"\u003eROC Curves and AUC\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"ROC Curves and AUC - Lab\" href=\"assignments/g7dd679196a289cb69567a310143e0a69\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12080\" data-api-returntype=\"Assignment\"\u003eROC Curves and AUC - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.4896%;\"\u003e\u003ca title=\"Logistic Regression Model Comparisons - Lab\" href=\"assignments/ge33ea8e6409baf98644fba31c69392ac\"\u003eLogistic Regression Model Comparisons - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Class Imbalance Problems\" href=\"assignments/g3a698bf676d43a6258d675063aefc220\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12081\" data-api-returntype=\"Assignment\"\u003eClass Imbalance Problems\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Class Imbalance Problems - Lab\" href=\"assignments/gc7eb5852a1d8a6a65e79c5a0b1b487fa\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12418\" data-api-returntype=\"Assignment\"\u003eClass Imbalance Problems - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.4896%;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Classification Metrics\" href=\"quizzes/gfbc831faf7b76124ddf50175a5d466d8\"\u003eQuiz: Classification Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.7653%; height: 48px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Metrics 2\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eClassification Workflow 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Classification Metrics 2 Exit Ticket\" href=\"quizzes/g7e7a19244813ae045fb81ce6e5ee782d\"\u003eClassification Metrics 2 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.9527%; height: 99px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Workflow 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Classification Workflow 1 Exit Ticket\" href=\"quizzes/g88bf554f6b9c73e7af697f7b2c831308\"\u003eClassification Workflow 1 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Logistic Regression - Cumulative Lab\" href=\"quizzes/g307f021a855d355fe1b11925047cb804\"\u003e‚≠êÔ∏è Logistic Regression - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Classification Metrics - Recap\" href=\"pages/classification-metrics-recap\"\u003eClassification Metrics - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","exportId":"topic-25-lesson-priorities-live"},{"id":456060,"title":"Classification Metrics - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classification-metrics-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eClassification models can be more complex to evaluate than regression models. There are more trade-offs involved as well as different metrics that can be used.\u003c/p\u003e\n\n\u003ch2\u003eEvaluating Classifiers\u003c/h2\u003e\n\n\u003cp\u003eWe'll look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\u003c/p\u003e\n\n\u003cp\u003eWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\u003c/p\u003e\n\n\u003ch2\u003eClass Imbalance Problems\u003c/h2\u003e\n\n\u003cp\u003eWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eMany of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\u003c/p\u003e","exportId":"classification-metrics-introduction"},{"id":456065,"title":"Confusion Matrices","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-confusion-matrices\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn how to construct and interpret a \u003cstrong\u003e\u003cem\u003eConfusion Matrix\u003c/em\u003e\u003c/strong\u003e to evaluate the performance of a classifier!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the four quadrants of a confusion matrix \u003c/li\u003e\n\u003cli\u003eInterpret a confusion matrix\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCreate a confusion matrix using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gee82ca01b6cbad81fe60f2bfd74f5c6f"},{"id":456069,"title":"Visualizing Confusion Matrices - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-visualizing-confusion-matrices-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll build upon the previous lesson on confusion matrices and visualize a confusion matrix using \u003ccode\u003ematplotlib\u003c/code\u003e. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a confusion matrix from scratch \u003c/li\u003e\n\u003cli\u003eCreate a confusion matrix using scikit-learn \u003c/li\u003e\n\u003cli\u003eCraft functions that visualize confusion matrices \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gfc4cabc49a2b5825bcd2280f966121f2"},{"id":456074,"title":"Evaluation Metrics","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-evaluation-metrics\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about common \u003cstrong\u003e\u003cem\u003eevaluation metrics\u003c/em\u003e\u003c/strong\u003e used to quantify the performance of classifiers!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eEvaluate classification models using the evaluation metrics appropriate for a specific problem \u003c/li\u003e\n\u003cli\u003eDefine precision and recall \u003c/li\u003e\n\u003cli\u003eDefine accuracy and F1 score \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eEvaluation metrics for classification\u003c/h2\u003e\n\n\u003cp\u003eNow that we've started discussing classification, it's time to examine comparing models to one other and choosing the models that have the best fit. Previously in regression, you were predicting values so it made sense to discuss error as a distance of how far off the estimates were from the actual values. However, in classifying a binary variable you are either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives there are in a model. In particular, there are a few different specific measurements when evaluating the performance of a classification algorithm.  \u003c/p\u003e\n\n\u003cp\u003eLet's work through these evaluation metrics to understand what each metric tells us.\u003c/p\u003e\n\n\u003ch2\u003ePrecision and recall\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e are two of the most basic evaluation metrics available to us. \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e measures how precise the predictions are, while \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e indicates what percentage of the classes we're interested in were actually captured by the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/images/new_EvalMatrices.png\" alt=\"diagram showing precision and recall using blue and orange circles and dots\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ePrecision\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how to use information found in a confusion matrix to calculate the precision of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} \" src=\"/equation_images/%20%255Ctext{Precision}%20=%20%255Cfrac{%255Ctext{Number%20of%20True%20Positives}}{%255Ctext{Number%20of%20Predicted%20Positives}}\" alt=\"{\" data-equation-content=\" \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eTo reuse a previous analogy of a model that predicts whether or not a person has a certain disease, precision allows us to answer the following question:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the times the model said someone had a disease, how many times did the patient in question actually have the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that a high precision score can be a bit misleading.  For instance, let's say we take a model and train it to make predictions on a sample of 10,000 patients. This model predicts that 6000 patients have the disease when in reality, only 5500 have the disease.  This model would have a precision of 91.6%. Now, let's assume we create a second model that only predicts that a person is sick when it's incredibly obvious.  Out of 10,000 patients, this model only predicts that 5 people in the entire population are sick.  However, each of those 5 times, it is correct.  model 2 would have a precision score of 100%, even though it missed 5,495 cases where the patient actually had the disease! In this way, more conservative models can have a high precision score, but this doesn't necessarily mean that they are the \u003cem\u003ebest performing\u003c/em\u003e model!\u003c/p\u003e\n\n\u003ch3\u003eRecall\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how we can use the information found in a confusion matrix to calculate the recall of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} \" src=\"/equation_images/%20%255Ctext{Recall}%20=%20%255Cfrac{%255Ctext{Number%20of%20True%20Positives}}{%255Ctext{Number%20of%20Actual%20Total%20Positives}}\" alt=\"{\" data-equation-content=\" \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} \"\u003e\u003c/p\u003e \u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eFollowing the same disease analogy, recall allows us to ask:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the patients we saw that actually had the disease, what percentage of them did our model correctly identify as having the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that recall can be a bit of a tricky statistic because improving our recall score doesn't necessarily always mean a better model overall. For example, our model could easily score 100% for recall by just classifying every single patient that walks through the door as having the disease in question. Sure, it would have many False Positives, but it would also correctly identify every single sick person as having the disease!\u003c/p\u003e\n\n\u003ch3\u003eThe relationship between precision and recall\u003c/h3\u003e\n\n\u003cp\u003eAs you may have guessed, precision and recall have an inverse relationship. As our recall goes up, our precision will go down, and vice versa. If this doesn't seem intuitive, let's examine this through the lens of our disease analogy. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with recall will have a very low threshold for declaring someone as sick because they are most worried about sick patients. Their precision will be quite low, because they classify almost everyone as sick, and don't care when they're wrong -- they only care about making sure that sick people are identified as sick. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with precision will have a very high threshold for declaring someone as sick, because they only declare someone as sick when they are completely sure that they will be correct if they declare a person as sick. Although their precision will be very high, their recall will be incredibly low, because a lot of people that are sick but don't meet the doctor's threshold will be incorrectly classified as healthy. \u003c/p\u003e\n\n\u003ch3\u003eWhich metric is better?\u003c/h3\u003e\n\n\u003cp\u003eA classic Data Science interview question is to ask \"What is better -- more false positives, or false negatives?\" This is a trick question designed to test your critical thinking on the topics of precision and recall. As you're probably thinking, the answer is \"It depends on the problem!\".  Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't.  This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score.\u003c/p\u003e\n\n\u003cp\u003eTake a few minutes and see if you can think of at least two examples each of situations where a high precision might be preferable to high recall, and two examples where high recall might be preferable to high precision. This is a common interview topic, so it's always handy to have a few examples ready!\u003c/p\u003e\n\n\u003ch2\u003eAccuracy and F1 score\u003c/h2\u003e\n\n\u003cp\u003eThe two most informative metrics that are often cited to describe the performance of a model are \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e. Let's take a look at each and see what's so special about them.\u003c/p\u003e\n\n\u003ch3\u003eAccuracy\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e is probably the most intuitive metric. The formula for accuracy is:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} \" src=\"/equation_images/%20%255Ctext{Accuracy}%20=%20%255Cfrac{%255Ctext{Number%20of%20True%20Positives%20+%20True%20Negatives}}{%255Ctext{Total%20Observations}}\" alt=\"{\" data-equation-content=\" \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eAccuracy is useful because it allows us to measure the total number of predictions a model gets right, including both \u003cstrong\u003e\u003cem\u003eTrue Positives\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eTrue Negatives\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eSticking with our analogy, accuracy allows us to answer:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the predictions our model made, what percentage were correct?\"\u003c/p\u003e\n\n\u003cp\u003eAccuracy is the most common metric for classification. It provides a solid holistic view of the overall performance of our model. \u003c/p\u003e\n\n\u003ch3\u003eF1 score\u003c/h3\u003e\n\n\u003cp\u003eThe F1 score is a bit more tricky, but also more informative. F1 score represents the \u003cstrong\u003e\u003cem\u003eHarmonic Mean of Precision and Recall\u003c/em\u003e\u003c/strong\u003e.  In short, this means that the F1 score cannot be high without both precision and recall also being high. When a model's F1 score is high, you know that your model is doing well all around. \u003c/p\u003e\n\n\u003cp\u003eThe formula for F1 score is:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{F1 score} = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall} \" src=\"/equation_images/%20%255Ctext{F1%20score}%20=%202%255C%20%255Cfrac{Precision%255C%20x%255C%20Recall}{Precision%20+%20Recall}\" alt=\"{\" data-equation-content=\" \\text{F1 score} = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall} \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eTo demonstrate the effectiveness of F1 score, let's plug in some numbers and compare F1 score with a regular arithmetic average of precision and recall. \u003c/p\u003e\n\n\u003cp\u003eLet's assume that the model has 98% recall and 6% precision.  \u003c/p\u003e\n\n\u003cp\u003eTaking the arithmetic mean of the two, we get: \u003cimg class=\"equation_image\" title=\" \\frac{0.98 + 0.06}{2} = \\frac{1.04}{2} = 0.52 \" src=\"/equation_images/%20%255Cfrac{0.98%20+%200.06}{2}%20=%20%255Cfrac{1.04}{2}%20=%200.52\" alt=\"{\" data-equation-content=\" \\frac{0.98 + 0.06}{2} = \\frac{1.04}{2} = 0.52 \"\u003e\u003c/p\u003e\n\n\u003cp\u003eHowever, using these numbers in the F1 score formula results in:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{F1 score} = 2 \\frac{0.98 * 0.06}{0.98 + 0.06} = 2 \\frac{0.0588}{1.04} = 2(0.061152) = 0.122304\" src=\"/equation_images/%20%255Ctext{F1%20score}%20=%202%20%255Cfrac{0.98%20*%200.06}{0.98%20+%200.06}%20=%202%20%255Cfrac{0.0588}{1.04}%20=%202(0.061152)%20=%200.122304\" alt=\"{\" data-equation-content=\" \\text{F1 score} = 2 \\frac{0.98 * 0.06}{0.98 + 0.06} = 2 \\frac{0.0588}{1.04} = 2(0.061152) = 0.122304\"\u003e\u003c/p\u003e or 12.2%!\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see, F1 score penalizes models heavily if it skews too hard towards either precision or recall. For this reason, F1 score is generally the most used metric for describing the performance of a model. \u003c/p\u003e\n\n\u003ch2\u003eWhich metric to use?\u003c/h2\u003e\n\n\u003cp\u003eThe metrics that are most important to a project will often be dependent on the business use case or goals for that model. This is why it's \u003cstrong\u003e\u003cem\u003every important\u003c/em\u003e\u003c/strong\u003e to understand why you're doing what you're doing, and how your model will be used in the real world! Otherwise, you may optimize your model for the wrong metric! \u003c/p\u003e\n\n\u003cp\u003eIn general, it is worth noting that it's a good idea to calculate all relevant metrics, when in doubt.  In most classification tasks, you don't know which model will perform best when you start. The common workflow is to train each different type of classifier, and select the best by comparing the performance of each. It's common to make tables like the one below, and highlight the best performer for each metric:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/images/performance-comparisons.png\" alt=\"pandas dataframe showing different classification metrics as columns and different models as rows\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eCalculate evaluation metrics with confusion matrices\u003c/h2\u003e\n\n\u003cp\u003eNote that we can only calculate any of the metrics discussed here if we know the \u003cstrong\u003e\u003cem\u003eTrue Positives, True Negatives, False Positives, and False Negatives\u003c/em\u003e\u003c/strong\u003e resulting from the predictions of a model. If we have a confusion matrix, we can easily calculate \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e -- and if we know precision and recall, we can easily calculate \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\n\u003ch2\u003eClassification reports\u003c/h2\u003e\n\n\u003cp\u003eScikit-learn has a built-in function that will create a \u003cstrong\u003e\u003cem\u003eClassification Report\u003c/em\u003e\u003c/strong\u003e. This classification report even breaks down performance by individual class predictions for your model. You can find the \u003ccode\u003eclassification_report()\u003c/code\u003e function in the \u003ccode\u003esklearn.metrics\u003c/code\u003e module, which takes labels and predictions and returns the precision, recall, F1 score and support (number of occurrences of each label in \u003ccode\u003ey_true\u003c/code\u003e) for the results of a model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you were introduced to several metrics which can be used to evaluate classification models. In the following lab, you'll write functions to calculate each of these manually, as well as explore how you can use existing functions in scikit-learn to quickly calculate and interpret each of these metrics. \u003c/p\u003e","exportId":"evaluation-metrics"},{"id":456079,"title":"Evaluating Logistic Regression Models - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-evaluating-logistic-regression-models-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you'll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement evaluation metrics from scratch using Python \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g163b0dfb05702dcac7b6cd220709bd40"},{"id":456093,"title":"ROC Curves and AUC","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-roc-curves-and-auc\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis lesson will introduce ROC: Receiver Operating Characteristic curves and AUC: Area Under [the] Curve.\u003c/p\u003e\n\n\u003cp\u003eSome of the accuracy scores you've encountered thus far probably seem pretty impressive; an 80% accuracy seems pretty darn good on the first try! What you have to keep in mind is that for binary classification, you are bound to be right sometimes, even just by random guessing. For example, a person should be roughly 50% accurate in guessing whether or not a coin lands on heads. This also can lead to issues when tuning models down the road. If you have a skewed dataset with rare events (such as a disease or winning the lottery) where there are only 2 positive cases in 1000, then even a trivial algorithm that classifies everything as 'not a member' will achieve an accuracy of 99.8% (998 out of 1000 times it was correct). So remember that an 80% accuracy must be taken into account in a larger context. AUC is an alternative comprehensive metric to confusion matrices, and ROC graphs allow us to determine optimal precision-recall tradeoff balances specific to the problem you are looking to solve.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine ROC curves and AUC\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eExplain how ROC and AUC are used to evaluate and choose models \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g4bde6387fe8446d8a78599461096f6fa"},{"id":456097,"title":"ROC Curves and AUC - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-roc-curves-and-auc-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice drawing ROC graphs, calculating AUC, and interpreting these results. In doing so, you will also further review logistic regression, by briefly fitting a model as in a standard data science pipeline.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a visualization of ROC curves and use it to assess a model \u003c/li\u003e\n\u003cli\u003eEvaluate classification models using the evaluation metrics appropriate for a specific problem \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g7dd679196a289cb69567a310143e0a69"},{"id":456101,"title":"Logistic Regression Model Comparisons - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-model-comparisons-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare the different inputs with logistic regression models and determine the optimal model \u003c/li\u003e\n\u003c/ul\u003e","exportId":"ge33ea8e6409baf98644fba31c69392ac"},{"id":456104,"title":"Class Imbalance Problems","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-class-imbalance-problems\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've learned about precision, recall, accuracy, f1 score, ROC curves, and AUC as metrics for evaluating the performance of classifiers. With this, you've seen how measuring the performance of classification algorithms is substantially different from that of regression. For example, we briefly discussed a scenario where only 2 in 1000 cases were labeled 'positive'. In such drastically cases, even a naive classifier that simply always predicts a 'negative' label would be 99.8% accurate. Moreover, such scenarios are relatively common in areas such as medical conditions or credit card fraud. This is known as the 'class imbalance' problem. As such, there has been a lot of work and research regarding class imbalance problems and methods for tuning classification algorithms to better fit these scenarios.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe why class imbalance can lead to problems in machine learning\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eList the different methods of fixing class imbalance issues \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g3a698bf676d43a6258d675063aefc220"},{"id":456109,"title":"Class Imbalance Problems - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-class-imbalance-problems-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you've gone over some techniques for tuning classification models on imbalanced datasets, it's time to practice those techniques. In this lab, you'll investigate credit card fraud and attempt to tune a model to flag suspicious activity.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse sampling techniques to address a class imbalance problem within a dataset \u003c/li\u003e\n\u003cli\u003eCreate a visualization of ROC curves and use it to assess a model\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gc7eb5852a1d8a6a65e79c5a0b1b487fa"},{"id":456114,"title":"Quiz: Classification Metrics","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g878a100f8835986ec288696d10002046","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"gfbc831faf7b76124ddf50175a5d466d8"},{"id":456148,"title":"‚≠êÔ∏è Logistic Regression - Cumulative Lab","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"gaee4522395e47c76dbf067b362bf6eff","questionCount":1,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_submit","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure what to do, refer to the instructions in \u003ca title=\"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\" href=\"quizzes/g9d5b89d09608ee0da642cc68cf19da0e\"\u003e‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\u003c/a\u003e\u003c/p\u003e","exportId":"g307f021a855d355fe1b11925047cb804"},{"id":456152,"title":"Classification Metrics - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classification-metrics-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you learned about the different ways to evaluate classification models such as logistic regression.\u003c/p\u003e\n\n\u003ch2\u003eClassification Metrics\u003c/h2\u003e\n\n\u003cp\u003eWhile precision, recall, and accuracy are useful metrics for evaluating classifiers, determining an appropriate balance between false positives and false negatives will depend on the particular problem application and the relative costs of each. For example, in the context of medical screening, a false negative could be devastating, eliminating the possibility for early intervention of the given disease. On the other hand, in another context, such as finding spam email, the cost of false positives might be much higher than false negatives -- after all, having a spam email sneak its way into your inbox is probably preferable then missing an important time-sensitive email because it was marked as spam. Due to these contextual considerations, you as the practitioner are responsible for selecting appropriate trade-offs.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\u003c/li\u003e\n\u003cli\u003eA confusion matrix is another common way to visualize the performance of a classification model\u003c/li\u003e\n\u003cli\u003eReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall trade-off for a given classifier\u003c/li\u003e\n\u003cli\u003eClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\u003c/li\u003e\n\u003c/ul\u003e","exportId":"classification-metrics-recap"}]},{"id":46968,"name":"Topic 26: Decision Trees","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g82789eaec1c2f4bc30fcb1651aaa6e3f","items":[{"id":456163,"title":"Topic 26 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.228%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eDecision Trees\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 42.2964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 7.69282%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Decision Trees - Introduction\" href=\"pages/decision-trees-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/decision-trees-introduction\" data-api-returntype=\"Page\"\u003eDecision Trees - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Decision Trees\" href=\"pages/introduction-to-decision-trees\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/introduction-to-decision-trees\" data-api-returntype=\"Page\"\u003eIntroduction to Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Entropy and Information Gain\" href=\"pages/entropy-and-information-gain\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/entropy-and-information-gain\" data-api-returntype=\"Page\"\u003eEntropy and Information Gain\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"ID3 Classification Trees: Perfect Split with Information Gain - Lab\" href=\"assignments/gbfbf401b01ebcd3a4fb3a809ecca4366\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186993\" data-api-returntype=\"Assignment\"\u003eID3 Classification Trees: Perfect Split with Information Gain - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Building Trees using scikit-learn\" href=\"assignments/g322a58b3e3dc9b4bab6c5c9f989be7b6\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186961\" data-api-returntype=\"Assignment\"\u003eBuilding Trees using scikit-learn\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Building Trees using scikit-learn - Lab\" href=\"assignments/gabdf55215fd9529a86ae72cbec0a0c8f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186962\" data-api-returntype=\"Assignment\"\u003eBuilding Trees using scikit-learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Hyperparameter Tuning and Pruning in Decision Trees\" href=\"pages/hyperparameter-tuning-and-pruning-in-decision-trees\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/hyperparameter-tuning-and-pruning-in-decision-trees\" data-api-returntype=\"Page\"\u003eHyperparameter Tuning and Pruning in Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Hyperparameter Tuning and Pruning in Decision Trees - Lab\" href=\"assignments/gceceeb05c5feb34554c4ac6d68cf5757\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186992\" data-api-returntype=\"Assignment\"\u003eHyperparameter Tuning and Pruning in Decision Trees - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Regression with CART Trees\" href=\"assignments/g3bef8e1ab0e4730537fdf1fb5e809498\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187037\" data-api-returntype=\"Assignment\"\u003eRegression with CART Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Regression with CART Trees - Lab\" href=\"assignments/g79619abf81fde3603d82f5fc24d233fe\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187038\" data-api-returntype=\"Assignment\"\u003eRegression with CART Trees - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Regression Trees and Model Optimization - Lab\" href=\"assignments/gfb58ddbd96ed5ad6bfe046755028f202\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187036\" data-api-returntype=\"Assignment\"\u003eRegression Trees and Model Optimization - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Decision Trees\" href=\"quizzes/gc3c333700dc3d9b001b40f75e7ede459\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30634\" data-api-returntype=\"Quiz\"\u003eQuiz: Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.228%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eDecision Trees\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 42.2964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 7.69282%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 42.2964%;\"\u003e\u003ca title=\"Short Video: Regression Trees\" href=\"pages/short-video-regression-trees\"\u003eShort Video: Regression Trees\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Decision Trees Exit Ticket\" href=\"quizzes/gc01819b9360073166897477919216e53\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30630\" data-api-returntype=\"Quiz\"\u003eDecision Trees Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Decision Trees - Recap\" href=\"pages/decision-trees-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/decision-trees-recap\" data-api-returntype=\"Page\"\u003eDecision Trees - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","exportId":"topic-26-lesson-priorities-live"},{"id":456168,"title":"Decision Trees - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to introduce another kind of model for predicting values that can be used for both continuous and categorical predictions - decision trees. Decision trees are used to classify (or estimate continuous values) by partitioning the sample space as efficiently as possible into sets with similar data points until you get to (or close to) a homogenous set and can reasonably predict the value for new data points. \u003c/p\u003e\n\n\u003cp\u003eDespite the fact that they've been around for decades, they are still (in conjunction with ensemble methods that we'll learn about in the next section) one of the most powerful modeling tools available in the field of machine learning. They are also highly interpretable when compared to more complex models (they're simple to explain and it's easy to understand how they make their decisions).\u003c/p\u003e\n\n\u003ch3\u003eEntropy and Information Gain\u003c/h3\u003e\n\n\u003cp\u003eDue to the nature of decision trees, you can get very different predictions depending on what questions you ask and in what order. The question then is how to come up with the right questions to ask in the right order. In this section, we also introduce the idea of entropy and information gain as mechanisms for selecting the most promising questions to ask in a decision tree.\u003c/p\u003e\n\n\u003ch3\u003eID3 Classification Trees\u003c/h3\u003e\n\n\u003cp\u003eWe also talk about Ross Quinlan's ID3 (Iterative Dichotomiser 3) algorithm for generating a decision tree from a dataset. \u003c/p\u003e\n\n\u003ch3\u003eBuilding Trees using Scikit-learn\u003c/h3\u003e\n\n\u003cp\u003eNext up, we look at how to build a decision tree using the built-in functions available in scikit-learn, and how to test the accuracy of the predictions using a simple accuracy measure, AUC, and a confusion matrix. We also show how to use the \u003ccode\u003egraph_viz\u003c/code\u003e library to generate a visualization of the resulting decision tree.\u003c/p\u003e\n\n\u003ch3\u003eHyperparameter Tuning and Pruning\u003c/h3\u003e\n\n\u003cp\u003eWe then look at some of the hyperparameters available when optimizing a decision tree. For example, if you're not careful, generated decision trees can lead to overfitting of data (wherein a model is a perfect match for training data, but horrible for test data). There are a number of hyperparameters you can use when generating a tree to minimize overfitting such as maximum depth or minimum leaf sample size. We look at these various \"pruning\" strategies to avoid overfitting of the data and to create a better model. \u003c/p\u003e\n\n\u003ch3\u003eRegression with CART Trees\u003c/h3\u003e\n\n\u003cp\u003eIn addition to building decision tree classifiers, you will also build decision trees for regression problems. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eDecision trees are highly effective and interpretable. This section will provide you with the skills to create both classifiers and to perform regression using decision trees and to use hyperparameter tuning to optimize your model.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"decision-trees-introduction"},{"id":456173,"title":"Introduction to Decision Trees","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-decision-trees\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll take a look at \u003cstrong\u003e\u003cem\u003edecision tree classifiers\u003c/em\u003e\u003c/strong\u003e. These are rule-based classifiers and belong to the first generation of modern AI. Despite the fact that this algorithm has been used in practice for decades, its simplicity and effectiveness for routine classification tasks is still on par with more sophisticated approaches. They are quite common in the business world because they have decent effectiveness without sacrificing explainability. Let's get started!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe a decision tree algorithm in terms of graph architecture\u003c/li\u003e\n\u003cli\u003eDescribe how decision trees are used to create partitions in a sample space\u003c/li\u003e\n\u003cli\u003eDescribe the training and prediction process of a decision tree\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFrom graphs to decision trees\u003c/h2\u003e\n\u003cp\u003eWe have seen basic classification algorithms (a.k.a classifiers), including Naive Bayes and logistic regression, in earlier sections. A decision tree is a different type of classifier that performs a \u003cstrong\u003erecursive partition of the sample space\u003c/strong\u003e. In this lesson, you will get a conceptual understanding of how this is achieved.\u003c/p\u003e\n\u003cp\u003eA decision tree comprises of decisions that originate from a chosen point in sample space. If you are familiar with Graph theory, a tree is a \u003cstrong\u003edirected acyclic graph with a root called \"root node\" that has no incoming edges\u003c/strong\u003e. All other nodes have one (and only one) incoming edge. Nodes having outgoing edges are known as \u003cstrong\u003einternal\u003c/strong\u003e nodes. All other nodes are called \u003cstrong\u003eleaves\u003c/strong\u003e. Nodes with an incoming edge, but no outgoing edges, are called \u003cstrong\u003eterminal nodes\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDirected Acyclic Graphs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn computer science and mathematics, a directed graph is a collection of nodes and edges such that edges can be traversed only in a specified direction (eg, from node A to node B, but not from node B to node A). An acyclic graph is a graph such that it is impossible for a node to be visited twice along any path from one node to another. So, a directed acyclic graph (or, a DAG) is a directed graph with no cycles. A DAG has a \u003cstrong\u003etopological ordering\u003c/strong\u003e, or, a sequence of the nodes such that every edge is directed from earlier to later in the sequence.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003ePartitioning the sample space\u003c/h2\u003e\n\u003cp\u003eSo, a decision tree is effectively a DAG, such as the one seen below where \u003cstrong\u003eeach internal node partitions the sample space into two (or more) sub-spaces\u003c/strong\u003e. These nodes are partitioned according to some discrete function that takes the attributes of the sample space as input.\u003c/p\u003e\n\u003cp\u003eIn the simplest and most frequent case, each internal node considers a single attribute so that space is partitioned according to the attribute‚Äôs value. In the case of numeric attributes, the condition refers to a range.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt1.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the basic idea behind decision trees: every internal node checks for a condition and performs a decision, and every terminal node (AKA leaf node) represents a discrete class. Decision tree induction is closely related to \u003cstrong\u003erule induction\u003c/strong\u003e. In essence, a decision tree is a just series of IF-ELSE statements (rules). Each path from the root of a decision tree to one of its leaves can be transformed into a rule simply by combining the decisions along the path to form the antecedent, and taking the leaf‚Äôs class prediction as the consequence (IF-ELSE statements follow the form: IF \u003cem\u003eantecedent\u003c/em\u003e THEN \u003cem\u003econsequence\u003c/em\u003e ).\u003c/p\u003e\n\u003ch2\u003eDefinition\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA decision tree is a DAG type of classifier where each internal node represents a choice between a number of alternatives and each leaf node represents a classification. An unknown (or test) instance is routed down the tree according to the values of the attributes in the successive nodes. When the instance reaches a leaf, it is classified according to the label assigned to the corresponded leaf.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt2.png\" width=\"850\"\u003e\u003c/p\u003e\n\u003cp\u003eA real dataset would usually have a lot more features than the example above and will create much bigger trees, but the idea will remain exactly the same. The idea of feature importance is crucial to decision trees, since selecting the correct feature to make a split on will affect the complexity and efficacy of the classification process. Regression trees are represented in the same manner, but instead they predict continuous values like the price of a house.\u003c/p\u003e\n\u003ch2\u003eTraining process\u003c/h2\u003e\n\u003cp\u003eThe process of training a decision tree and predicting the target features of a dataset is as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePresent a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"\u003cstrong\u003einformation gain\u003c/strong\u003e\" and \"\u003cstrong\u003eGini Index\u003c/strong\u003e\". We shall cover these shortly.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe tree is grown until some \u003cstrong\u003estopping criteria\u003c/strong\u003e is achieved. This could be a set depth of the tree or any other similar measure.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShow a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt3.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch2\u003eSplitting criteria\u003c/h2\u003e\n\u003cp\u003eThe training process of a decision tree can be generalized as \"\u003cstrong\u003erecursive binary splitting\u003c/strong\u003e\".\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this procedure, all the features are considered and different split points are tried and tested using some \u003cstrong\u003ecost function\u003c/strong\u003e. The split with the lowest cost is selected.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are a couple of algorithms used to build a decision tree:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eCART (Classification and Regression Trees)\u003c/strong\u003e uses the Gini Index as a metric\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eID3 (Iterative Dichotomiser 3)\u003c/strong\u003e uses the entropy function and information gain as metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGreedy search\u003c/h2\u003e\n\u003cp\u003eWe need to determine the attribute that \u003cstrong\u003ebest\u003c/strong\u003e classifies the training data, and use this attribute at the root of the tree. At each node, we repeat this process creating further splits, until a leaf node is achieved, i.e., all data gets classified.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis means we are performing a top-down, greedy search through the space of possible decision trees.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn order to identify the best attribute for ID3 classification trees, we use the \"information gain\" criteria. Information gain (IG) measures how much \"information\" a feature gives us about the class. Decision trees always try to maximize information gain. So, the attribute with the highest information gain will be split on first.\u003c/p\u003e\n\u003cp\u003eLet's move on to the next lesson where we shall look into these criteria with simple examples.\u003c/p\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\"\u003eR2D3\u003c/a\u003e: This is highly recommended for getting a visual introduction to decision trees. Excellent animations explaining the training and prediction stages shown above.\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"http://www.dataversity.net/introduction-machine-learning-decision-trees/\"\u003eDataversity: Decision Trees Intro\u003c/a\u003e: A quick and visual introduction to DTs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html\"\u003eDirected Acyclic Graphs\u003c/a\u003e: This would help relate early understanding of graph computation to decision tree architectures.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we saw an introduction to decision trees as simple yet effective classifiers. We looked at how decision trees partition the sample space based on learning rules from a given dataset. We also looked at how feature selection for splitting the tree is of such high importance. Next, we shall look at information gain criteria used for feature selection.\u003c/p\u003e","exportId":"introduction-to-decision-trees"},{"id":456177,"title":"Entropy and Information Gain","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-entropy-and-information-gain\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInformation gain\u003c/em\u003e\u003c/strong\u003e is calculated using a statistical measure called \u003cstrong\u003e\u003cem\u003eEntropy\u003c/em\u003e\u003c/strong\u003e. Entropy is a widely used concept in the fields of Physics, Mathematics, Computer Science (information theory), and more. You may have come across the idea of entropy in thermodynamics, societal dynamics, and a number of other domains. In electronics and computer science, the idea of entropy is usually derived from \u003cstrong\u003eShannon's\u003c/strong\u003e description of entropy to measure the information gain against some cost incurred in the process. In this lesson, we shall look at how this works with the simple example we introduced in the previous lesson. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the process for selecting the best attribute for a split \u003c/li\u003e\n\u003cli\u003eCalculate entropy and information gain by hand for a simple dataset \u003c/li\u003e\n\u003cli\u003eCompare and contrast entropy and information gain \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eShannon's Entropy\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eEntropy is a measure of disorder or uncertainty.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe measure is named after \u003cem\u003eClaude Shannon\u003c/em\u003e, who is known as the \"father of information theory\". Information theory provides measures of uncertainty associated with random variables. These measures help calculate the average information content one is missing when one does not know the value of the random variable. This uncertainty is measured in bits, i.e., the amount of information (in bits) contained per average instance in a stream of instances.\u003c/p\u003e\n\n\u003cp\u003eConceptually, information can be thought of as being stored or transmitted as variables that can take on different values. A variable can be thought of as a unit of storage that can take on, at different times, one of several different specified values, following some process for taking on those values. Informally, we get information from a variable by looking at its value, just as we get information from an email by reading its contents. In the case of the variable, the information is about the process behind the variable.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe entropy of a variable is the \"amount of information\" contained in the variable. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis amount is not only determined by the number of different values the variable can take, just as the information in an email is not quantified just by the number of words in the email or the different possible words in the language of the email. Informally, the amount of information in an email is proportional to the amount of ‚Äúsurprise‚Äù its reading causes. \u003c/p\u003e\n\n\u003cp\u003eFor example, if an email is simply a repeat of an earlier email, then it is not informative at all. On the other hand, if, for example, the email reveals the outcome of an election, then it is highly informative. Similarly, the information in a variable is tied to the amount of surprise the value of that variable causes when revealed.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eShannon‚Äôs entropy quantifies the amount of information in a variable, thus providing the foundation for a theory around the notion of information.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn terms of data, we can informally describe entropy as an indicator of how messy your data is.  A high degree of entropy always reflects \"messed-up\" data with low/no information content. The uncertainty about the content of the data, before viewing the data remains the same (or almost the same) as that before the data was available. \u003c/p\u003e\n\n\u003cp\u003eIn a nutshell, higher entropy means less predictive power when it comes to doing data science with that data. \u003c/p\u003e\n\n\u003ch2\u003eEntropy and decision trees\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eDecision trees aim to tidy the data by separating the samples and re-grouping them in the classes they belong to.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBecause decision trees use a supervised learning approach, we know the target variable of our data. So, we maximize the \u003cstrong\u003epurity\u003c/strong\u003e of the classes \u003cstrong\u003eas much as possible\u003c/strong\u003e while making splits, aiming to have \u003cstrong\u003eclarity\u003c/strong\u003e in the leaf nodes. Remember, it may not be possible to remove the uncertainty totally, i.e., to fully clean up the data. Have a look at the image below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/split_fs.png\" alt=\"initial dataset with a decision split that reduces entropy\" width=\"300\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWe can see that the split has not \u003cstrong\u003eFULLY\u003c/strong\u003e classified the data above, but the resulting data is \u003cstrong\u003etidier\u003c/strong\u003e than it was before the split. By using a series of such splits that focus on different features, we try to clean up the data as much as possible in the leaf nodes. At each step, we want to decrease the entropy, so \u003cstrong\u003eentropy is computed before and after the split\u003c/strong\u003e. If it decreases, the split is retained and we can proceed to the next step, otherwise, we must try to split with another feature or stop this branch (or quit, in which case we claim that the current tree is the best solution).\u003c/p\u003e\n\n\u003ch3\u003eCalculating entropy\u003c/h3\u003e\n\n\u003cp\u003eLet's pretend we have a sample, \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e. This sample contains \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e total items falling into two different categories, \u003ccode\u003eTrue\u003c/code\u003e and \u003ccode\u003eFalse\u003c/code\u003e. Of the \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e total items we have, \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e observations have a target value equal to \u003cimg class=\"equation_image\" title=\"True\" src=\"https://learning.flatironschool.com/equation_images/True\" alt=\"{\" data-equation-content=\"True\"\u003e, and \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e observations have a target value equal to \u003cimg class=\"equation_image\" title=\"False\" src=\"https://learning.flatironschool.com/equation_images/False\" alt=\"{\" data-equation-content=\"False\"\u003e. Note that if we know \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e and \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e, we can easily calculate \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e to be \u003cimg class=\"equation_image\" title=\"m = N - n\" src=\"https://learning.flatironschool.com/equation_images/m%20=%20N%20-%20n\" alt=\"{\" data-equation-content=\"m = N - n\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eLet's assume our boss brings us the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e, and asks us to group each observation in \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e according to whether their target value is True or False. They also want to know the ratio of Trues to Falses in our dataset. We can calculate this as follows: \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"p = n/N - (class 1)\" src=\"https://learning.flatironschool.com/equation_images/p%20=%20n/N%20-%20(class%201)\" alt=\"{\" data-equation-content=\"p = n/N - (class 1)\"\u003e\u003c/p\u003e \u003cp\u003e\u003cimg class=\"equation_image\" title=\"q = m/N = 1-p - (class 2)\" src=\"https://learning.flatironschool.com/equation_images/q%20=%20m/N%20=%201-p%20-%20(class%202)\" alt=\"{\" data-equation-content=\"q = m/N = 1-p - (class 2)\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eIf we know these ratios, we can calculate the \u003cem\u003eentropy\u003c/em\u003e of the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e. This will provide us with an easy way to see how organized or disorganized our dataset is. For instance, let's assume that our boss believes that the dataset should mostly be full of \"True\"'s, with some occasional \"False\"'s slipping through. The more Falses in with the Trues (or Trues in with the Falses!), the more disorganized our dataset is. We can calculate entropy using the following equation:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"E = -p . log_2(p) - q . log_2(q)\" src=\"https://learning.flatironschool.com/equation_images/E%20=%20-p%20.%20log_2(p)%20-%20q%20.%20log_2(q)\" alt=\"{\" data-equation-content=\"E = -p . log_2(p) - q . log_2(q)\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eDon't worry too much about this equation yet -- we'll dig deeper into what it means in a minute. \u003c/p\u003e\n\n\u003cp\u003eThe equation above tells us that a dataset is considered tidy if it only contains one class (i.e. no uncertainty or confusion). If the dataset contains a mix of classes for our target variable, the entropy increases. This is easier to understand when we visualize it. Consider the following graph of entropy in a dataset that has two classes for our target variable:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/new_entropy_fs.png\" alt=\"parabola that shows the proportion of class p on the x-axis and entropy on the y-axis. the parabola peaks at 0.5\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see, when the classes are split equally, \u003cimg class=\"equation_image\" title=\"p = 0.5\" src=\"https://learning.flatironschool.com/equation_images/p%20=%200.5\" alt=\"{\" data-equation-content=\"p = 0.5\"\u003e and \u003cimg class=\"equation_image\" title=\"q = 1 - p = 0.5\" src=\"https://learning.flatironschool.com/equation_images/q%20=%201%20-%20p%20=%200.5\" alt=\"{\" data-equation-content=\"q = 1 - p = 0.5\"\u003e, the entropy value is at its maximum, 1. Conversely, when the proportion of the split is at 0 (all of one target class) or at 1 (all of the other target class), the entropy value is 0! This means that we can easily think of entropy as follows: the more one-sided the proportion of target classes, the less entropy. Think of a sock drawer that may or may not have some underwear mixed in. If the sock drawer contains only socks (or only underwear), then entropy is 0. If you reach in and pull out an article of clothing, you know exactly what you're going to get. However, if 10% of the items in that sock drawer are actually underwear, you are less certain what that random draw will give you. That uncertainty increases as more and more underwear gets mixed into that sock drawer, right up until there is the exact same amount of socks and underwear in the drawer. When the proportion is exactly equal, you have no way of knowing item of clothing a random draw might give you -- maximum entropy, and perfect chaos!\u003c/p\u003e\n\n\u003cp\u003eThis is where the logic behind decision trees comes in -- what if we could split the contents of our sock drawer into different subsets, which might divide the drawer into more organized subsets? For instance, let's assume that we've built a laundry robot that can separate items of clothing by color. If a majority of our socks are white, and a majority of our underwear is some other color, then we can safely assume that the two subsets will have a better separation between socks and underwear, even if the original chaotic drawer had a 50/50 mix of the two!\u003c/p\u003e\n\n\u003ch3\u003eGeneralization of entropy\u003c/h3\u003e\n\n\u003cp\u003eNow that we have a good real-world example to cling to, let's get back to thinking about the mathematical definition of entropy. \u003c/p\u003e\n\n\u003cp\u003eEntropy \u003cimg class=\"equation_image\" title=\"H(S)\" src=\"https://learning.flatironschool.com/equation_images/H(S)\" alt=\"{\" data-equation-content=\"H(S)\"\u003e is a measure of the amount of uncertainty in the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e. We can see this is a measurement or characterization of the amount of information contained within the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eWe saw how to calculate entropy for a two-class variable before. However, in the real world we deal with multiclass problems very often, so it would be a good idea to see a general representation of the formula we saw before. The general representation is: \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large H(S) = -\\sum (P_i . log_2(P_i))\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20H(S)%20=%20-%255Csum%20(P_i%20.%20log_2(P_i))\" alt=\"{\" data-equation-content=\"\\large H(S) = -\\sum (P_i . log_2(P_i))\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen  \u003cimg class=\"equation_image\" title=\"H(S) = 0\" src=\"https://learning.flatironschool.com/equation_images/H(S)%20=%200\" alt=\"{\" data-equation-content=\"H(S) = 0\"\u003e, this means that the set \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e is perfectly classified, meaning that there is no disorganization in our data because all of our data in S is the exact same class. If we know how much entropy exists in a subset (and remember, we can subset our data by just splitting it into 2 or more groups according to whatever metric we choose), then we can easily calculate how much \u003cstrong\u003e\u003cem\u003einformation gain\u003c/em\u003e\u003c/strong\u003e each potential split would give us!\u003c/p\u003e\n\n\u003ch2\u003eInformation gain\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInformation gain is an impurity/uncertainty based criterion that uses the entropy as the measure of impurity.\u003c/strong\u003e \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThere are several different algorithms out there for creating decision trees. Of those, the ID3 algorithm is one of the most popular. Information gain is the key criterion that is used by the ID3 classification tree algorithm to construct a decision tree. The decision tree algorithm will always try to \u003cstrong\u003emaximize information gain\u003c/strong\u003e. The entropy of the dataset is calculated using each attribute, and the attribute showing highest information gain is used to create the split at each node. A simple understanding of information gain can be written as:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"Information~Gain  = Entropy_{parent} - Entropy_{child}.[child ~weighted ~average]\" src=\"/equation_images/Information~Gain%20%20=%20Entropy_{parent}%20-%20Entropy_{child}.[child%20~weighted%20~average]\" alt=\"{\" data-equation-content=\"Information~Gain  = Entropy_{parent} - Entropy_{child}.[child ~weighted ~average]\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eA weighted average based on the number of samples in each class is multiplied by the child's entropy, since most datasets have class imbalance. Thus the information gain calculation for each attribute is calculated and compared, and the attribute showing the highest information gain will be selected for the split. Below is a more generalized form of the equation: \u003c/p\u003e\n\n\u003cp\u003eWhen we measure information gain, we're really measuring the difference in entropy from before the split (an untidy sock drawer) to after the split (a group of white socks and underwear, and a group of non-white socks and underwear). Information gain allows us to put a number to exactly how much we've reduced our \u003cem\u003euncertainty\u003c/em\u003e after splitting a dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e on some attribute, \u003cimg class=\"equation_image\" title=\"A\" src=\"https://learning.flatironschool.com/equation_images/A\" alt=\"{\" data-equation-content=\"A\"\u003e.  The equation for information gain is:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large IG(A, S) = H(S) - \\sum{}{p(t)H(t)}  \" src=\"/equation_images/%255Clarge%20IG(A,%20S)%20=%20H(S)%20-%20%255Csum{}{p(t)H(t)}\" alt=\"{\" data-equation-content=\"\\large IG(A, S) = H(S) - \\sum{}{p(t)H(t)}  \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eWhere:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"H(S)\" src=\"https://learning.flatironschool.com/equation_images/H(S)\" alt=\"{\" data-equation-content=\"H(S)\"\u003e is the entropy of set \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e is a subset of the attributes contained in \u003cimg class=\"equation_image\" title=\"A\" src=\"https://learning.flatironschool.com/equation_images/A\" alt=\"{\" data-equation-content=\"A\"\u003e (we represent all subsets \u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e as \u003cimg class=\"equation_image\" title=\"T\" src=\"https://learning.flatironschool.com/equation_images/T\" alt=\"{\" data-equation-content=\"T\"\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"p(t)\" src=\"https://learning.flatironschool.com/equation_images/p(t)\" alt=\"{\" data-equation-content=\"p(t)\"\u003e is the proportion of the number of elements in \u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e to the number of elements in \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"H(t)\" src=\"https://learning.flatironschool.com/equation_images/H(t)\" alt=\"{\" data-equation-content=\"H(t)\"\u003e is the entropy of a given subset \u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the ID3 algorithm, we use entropy to calculate information gain, and then pick the attribute with the largest possible information gain to split our data on at each iteration. \u003c/p\u003e\n\n\u003ch2\u003eEntropy and information gain example\u003c/h2\u003e\n\n\u003cp\u003eSo far, we've focused heavily on the math behind entropy and information gain. This usually makes the calculations look scarier than they actually are. To show that calculating entropy/information gain is actually pretty simple, let's take a look at an example problem -- predicting if we want to play tennis or not, based on the weather, temperature, humidity, and windiness of a given day!\u003c/p\u003e\n\n\u003cp\u003eOur dataset is as follows:\u003c/p\u003e\n\n\u003ctable\u003e\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center\"\u003eoutlook\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003etemp\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003ehumidity\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003ewindy\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003eplay\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eLet's apply the formulas we saw earlier to this problem:  \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\Large  H(S) = \\sum{}{-p(c) log_2 p(c)}\" src=\"/equation_images/%255CLarge%20%20H(S)%20=%20%255Csum{}{-p(c)%20log_2%20p(c)}\" alt=\"{\" data-equation-content=\"\\Large  H(S) = \\sum{}{-p(c) log_2 p(c)}\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large C={{yes, no}}\" src=\"/equation_images/%255Clarge%20C={{yes,%20no}}\" alt=\"{\" data-equation-content=\"\\large C={{yes, no}}\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eOut of 14 instances, 9 are classified as yes, and 5 as no. So:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large  p(yes) = -(9/14)log_2(9/14) = 0.28\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20%20p(yes)%20=%20-(9/14)log_2(9/14)%20=%200.28\" alt=\"{\" data-equation-content=\"\\large  p(yes) = -(9/14)log_2(9/14) = 0.28\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large  p(no) = -(5/14)log_2(5/14) = 0.37\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20%20p(no)%20=%20-(5/14)log_2(5/14)%20=%200.37\" alt=\"{\" data-equation-content=\"\\large  p(no) = -(5/14)log_2(5/14) = 0.37\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large  H(S) = p(yes) + p(no) = 0.65\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20%20H(S)%20=%20p(yes)%20+%20p(no)%20=%200.65\" alt=\"{\" data-equation-content=\"\\large  H(S) = p(yes) + p(no) = 0.65\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eThe current entropy of our dataset is 0.65. In the next lesson, we'll see how we can improve this by subsetting our dataset into different groups by calculating the entropy/information gain of each possible split, and then picking the one that performs best until we have a fully fleshed-out decision tree!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at calculating entropy and information gain measures for building decision trees. We looked at a simple example and saw how to use these measures to select the best split at each node. Next, we calculate these measures in Python, before digging deeper into decision trees. \u003c/p\u003e","exportId":"entropy-and-information-gain"},{"id":456181,"title":"ID3 Classification Trees: Perfect Split with Information Gain - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ID3-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we will simulate the example from the previous lesson in Python. You will write functions to calculate entropy and IG which will be used for calculating these uncertainty measures and deciding upon creating a split using information gain while growing an ID3 classification tree. You will also write a general function that can be used for other (larger) problems as well. So let's get on with it.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eWrite functions for calculating entropy and information gain measures\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eUse entropy and information gain to identify the attribute that results in the best split at each node\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gbfbf401b01ebcd3a4fb3a809ecca4366"},{"id":456184,"title":"Building Trees using scikit-learn","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-decision-trees-with-sklearn-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we will cover decision trees (for classification) in Python, using scikit-learn and pandas. The emphasis will be on the basics and understanding the resulting decision tree. Scikit-learn provides a consistent interface for running different classifiers/regressors. For classification tasks, evaluation is performed using the same measures as we have seen before. Let's look at our example from earlier lessons and grow a tree to find our solution. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse scikit-learn to fit a decision tree classification model \u003c/li\u003e\n\u003cli\u003ePlot a decision tree using Python \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g322a58b3e3dc9b4bab6c5c9f989be7b6"},{"id":456186,"title":"Building Trees using scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-decision-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eFollowing the simple example you saw in the previous lesson, you'll now build a decision tree for a more complex dataset. This lab covers all major areas of standard machine learning practice, from data acquisition to evaluation of results. We'll continue to use the Scikit-learn and Pandas libraries to conduct this analysis, following the same structure we saw in the previous lesson.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse scikit-learn to fit a decision tree classification model \u003c/li\u003e\n\u003cli\u003eUse entropy and information gain to identify the best attribute to split on at each node \u003c/li\u003e\n\u003cli\u003ePlot a decision tree using Python \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gabdf55215fd9529a86ae72cbec0a0c8f"},{"id":456188,"title":"Hyperparameter Tuning and Pruning in Decision Trees","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eHyperparameter tuning relates to how we sample candidate model architectures from the space of all possible hyperparameter values. This is often referred to as \u003cstrong\u003esearching the hyperparameter space for the optimum values\u003c/strong\u003e. In this lesson, we'll look at some of the key hyperparameters for decision trees and how they affect the learning and prediction processes. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify the role of pruning while training decision trees\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eList the different hyperparameters for tuning decision trees \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eHyperparameter Optimization\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBy contrast, the values of model parameters are derived via training as we have seen previously.\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, Lasso is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll look at these sorts of optimizations in the context of decision trees and see how these can affect the predictive performance as well as the computational complexity of the tree. \u003c/p\u003e\n\n\u003ch2\u003eTree pruning\u003c/h2\u003e\n\n\u003cp\u003eNow that we know how to grow a decision tree using Python and scikit-learn, let's move on and practice \u003cstrong\u003eoptimizing\u003c/strong\u003e a classifier. We can tweak a few parameters in the decision tree algorithm before the actual learning takes place. \u003c/p\u003e\n\n\u003cp\u003eA decision tree, grown beyond a certain level of complexity leads to overfitting. If we grow our tree and carry on using poor predictors that don't have any impact on the accuracy, we will eventually a) slow down the learning, and b) cause overfitting.  Different tree pruning parameters can adjust the amount of overfitting or underfitting in order to optimize for increased accuracy, precision, and/or recall.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThis process of trimming decision trees to optimize the learning process is called \"tree pruning\".\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe can prune our trees using:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eMaximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum samples leaf with split: Restrict the size of sample leaf\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum leaf sample size: Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum leaf nodes: Reduce the number of leaf nodes\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum features: Maximum number of features to consider when splitting a node\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet's look at a few hyperparameters and learn about their impact on classifier performance:  \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emax_depth\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe parameter for decision trees that we normally tune first is \u003ccode\u003emax_depth\u003c/code\u003e. This parameter indicates how deep we want our tree to be. If the tree is too deep, it means we are creating a large number of splits in the parameter space and capturing more information about underlying data. This may result in \u003cstrong\u003eoverfitting\u003c/strong\u003e as it will lead to learning granular information from given data, which makes it difficult for our model to generalize on unseen data. \nGenerally speaking, a low training error but a large testing error is a strong indication of this. \u003c/p\u003e\n\n\u003cp\u003eIf, on the other hand, the tree is too shallow, we may run into \u003cstrong\u003eunderfitting\u003c/strong\u003e, i.e., we are not learning enough information about the data and the accuracy of the model stays low for both the test and training samples. The following example shows the training and test AUC scores for a decision tree with depths ranging from 1 to 32.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/depth.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above example, we see that as the tree depth increases, our validation/test accuracy starts to go down after a depth of around 4. But with even greater depths, the training accuracy keeps on rising, as the classifier learns more information from the data. However this information can not be mapped onto unseen examples, hence the validation accuracy falls down constantly. Finding the sweet spot (e.g. depth = 4) in this case would be the first hyperparameter that we need to tune. \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_split\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe hyperparameter \u003ccode\u003emin_samples_split\u003c/code\u003e is used to set the \u003cstrong\u003eminimum number of samples required to split an internal node\u003c/strong\u003e. This can vary between two extremes, i.e., considering only one sample at each node vs. considering all of the samples at each node - for a given attribute. \u003c/p\u003e\n\n\u003cp\u003eWhen we increase this parameter value, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/split.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above plot, we see that the training and test accuracy stabilize at a certain minimum sample split size, and stays the same even if we carry on increasing the size of the split. This means that we will have a complex model, with similar accuracy than a much simpler model could potentially exhibit. Therefore, it is imperative that we try to identify the optimal sample size during the training phase. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003ccode\u003emax_depth\u003c/code\u003e and \u003ccode\u003emin_samples_split\u003c/code\u003e are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.  \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_leaf\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThis hyperparameter is used to identify the minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a node, it does not get split any further.  This parameter is similar to \u003ccode\u003emin_samples_splits\u003c/code\u003e, however, this describes the minimum number of samples at the leaves, the base of the tree.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/leaf.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above plot shows the impact of this parameter on the accuracy of the classifier. We see that increasing this parameter value after an optimal point reduces accuracy. That is due to underfitting again, as keeping too many samples in our leaf nodes means that there is still a high level of uncertainty in the data. \u003c/p\u003e\n\n\u003cp\u003eThe main difference between the two is that \u003ccode\u003emin_samples_leaf\u003c/code\u003e guarantees a minimum number of samples in a leaf, while \u003ccode\u003emin_samples_split\u003c/code\u003e can create arbitrary small leaves, though \u003ccode\u003emin_samples_split\u003c/code\u003e is more common in practice. These two hyperparameters make the distinction between a leaf (terminal/external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\u003c/p\u003e\n\n\u003cp\u003eFor instance, if \u003ccode\u003emin_samples_split = 5\u003c/code\u003e, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If \u003ccode\u003emin_samples_leaf = 2\u003c/code\u003e, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node.\u003c/p\u003e\n\n\u003ch3\u003eAre there more hyperparameters?\u003c/h3\u003e\n\n\u003cp\u003eYes, there are! Scikit-learn offers a number of other hyperparameters for further fine-tuning the learning process. \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\"\u003eConsult the official doc\u003c/a\u003e to look at them in detail. The hyperparameters mentioned here are directly related to the complexity which may arise in decision trees and are normally tuned when growing trees. We'll shortly see this in action with a real dataset. \u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview\"\u003eOverview of hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f\"\u003eDemystifying hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.displayr.com/machine-learning-pruning-decision-trees/\"\u003ePruning decision trees\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at the idea of optimizing hyperparameters and how pruning plays an important role in restricting the growth of a decision tree. We looked at a few hyperparameters which directly impact the potential overfitting/underfitting in trees. Next, we'll see these in practice using scikit-learn.   \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"hyperparameter-tuning-and-pruning-in-decision-trees"},{"id":456192,"title":"Hyperparameter Tuning and Pruning in Decision Trees - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-decision-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you will use the titanic dataset to see the impact of tree pruning and hyperparameter tuning on the predictive performance of a decision tree classifier. Pruning reduces the size of decision trees by removing nodes of the tree that do not provide much predictive power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDetermine the optimal hyperparameters for a decision tree model and evaluate the model performance\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gceceeb05c5feb34554c4ac6d68cf5757"},{"id":456196,"title":"Regression with CART Trees","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-cart-trees-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs we've learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Kindly visit the \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\"\u003eofficial documentation\u003c/a\u003e for the regressor tree function used in this lesson. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain recursive partitioning \u003c/li\u003e\n\u003cli\u003eFit a decision tree regression model with scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g3bef8e1ab0e4730537fdf1fb5e809498"},{"id":456200,"title":"Regression with CART Trees - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-cart-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll make use of what we learned in the previous lesson to build a model for the \u003ca href=\"https://www.kaggle.com/harinir/petrol-consumption\"\u003ePetrol Consumption Dataset\u003c/a\u003e from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFit a decision tree regression model with scikit-learn\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g79619abf81fde3603d82f5fc24d233fe"},{"id":456205,"title":"Regression Trees and Model Optimization - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-regression-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models \u003c/li\u003e\n\u003cli\u003eDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gfb58ddbd96ed5ad6bfe046755028f202"},{"id":456212,"title":"Quiz: Decision Trees","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g2773bcc7ce99bf018cdde53dbe3c8ce2","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"gc3c333700dc3d9b001b40f75e7ede459"},{"id":456224,"title":"Short Video: Regression Trees","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802712?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-regression-trees"},{"id":456232,"title":"Decision Trees - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDecision trees can be used for both categorization and regression tasks\u003c/li\u003e\n\u003cli\u003eThey are a powerful and interpretable technique for many machine learning problems (especially when combined with ensemble methods)\u003c/li\u003e\n\u003cli\u003eDecision trees are a form of Directed Acyclic Graphs (DAGs) - you traverse them in a specified direction, and there are no \"loops\" in the graphs to go backward\u003c/li\u003e\n\u003cli\u003eAlgorithms for generating decision trees are designed to maximize the information gain from each split\u003c/li\u003e\n\u003cli\u003eA popular algorithm for generating decision trees is ID3 - the Iterative Dichotomiser 3 algorithm\u003c/li\u003e\n\u003cli\u003eThere are several hyperparameters for decision trees to reduce overfitting - including maximum depth, minimum samples to split a node that is currently a leaf, minimum leaf sample size, maximum leaf nodes, and maximum features \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"decision-trees-recap"}]},{"id":46973,"name":"Topic 27: K Nearest Neighbors","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g2a0d32eb8567428df5a74a26b5cbba7d","items":[{"id":456246,"title":"Topic 27 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.7191%; height: 182px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eK-Nearest Neighbors\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"K-Nearest Neighbors - Introduction\" href=\"pages/k-nearest-neighbors-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/k-nearest-neighbors-introduction\" data-api-returntype=\"Page\"\u003eK-Nearest Neighbors - Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Distance Metrics\" href=\"assignments/g61391248a475ba95d6c3011ff7a40b23\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12086\" data-api-returntype=\"Assignment\"\u003eDistance Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Distance Metrics - Lab\" href=\"assignments/ga1388e96c2ed6de2dfc2332638720aeb\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12087\" data-api-returntype=\"Assignment\"\u003eDistance Metrics - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"K-Nearest Neighbors\" href=\"pages/k-nearest-neighbors\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/k-nearest-neighbors\" data-api-returntype=\"Page\"\u003eK-Nearest Neighbors\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"K-Nearest Neighbors - Lab\" href=\"assignments/g96f15a5a4928ab60f394fc9eedfc023d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12088\" data-api-returntype=\"Assignment\"\u003eK-Nearest Neighbors - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Finding the Best Value for K\" href=\"pages/finding-the-best-value-for-k\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/finding-the-best-value-for-k\" data-api-returntype=\"Page\"\u003eFinding the Best Value for K\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"KNN with scikit-learn\" href=\"pages/knn-with-scikit-learn\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/knn-with-scikit-learn\" data-api-returntype=\"Page\"\u003eKNN with scikit-learn\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"KNN with scikit-learn - Lab\" href=\"assignments/g60e8d59944503f2fff50b17ed9050ce2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12089\" data-api-returntype=\"Assignment\"\u003eKNN with scikit-learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: K Nearest Neighbors\" href=\"quizzes/g041301c0d6f44f9e19f87488351cf805\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/quizzes/23170\" data-api-returntype=\"Quiz\"\u003eQuiz: K Nearest Neighbors\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6254%; height: 78px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eK-Nearest Neighbors\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"KNN Exit Ticket\" href=\"quizzes/gbf96ad98a717fabb3b4f3675241611ed\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/quizzes/11197\" data-api-returntype=\"Quiz\"\u003eKNN Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Nonparametric ML Models - Cumulative Lab\" href=\"quizzes/g1056c04a78eb2b8ee77b244c23859397\"\u003e‚≠êÔ∏è Nonparametric ML Models - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"K-Nearest Neighbors - Recap\" href=\"pages/k-nearest-neighbors-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/k-nearest-neighbors-recap\" data-api-returntype=\"Page\"\u003eK-Nearest Neighbors - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","exportId":"topic-27-lesson-priorities-live"},{"id":456252,"title":"K-Nearest Neighbors - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll look at an intuitive algorithm known as K-Nearest Neighbors (KNN). KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction. \u003c/p\u003e\n\n\u003ch2\u003eKNN\u003c/h2\u003e\n\n\u003cp\u003eThe K-Nearest Neighbors algorithm works as follows: \u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eChoose a point \u003c/li\u003e\n\u003cli\u003eFind the K-nearest points\n\n\u003col\u003e\n\u003cli\u003eK is a predefined user constant such as 1, 3, 5, or 11 \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePredict a label for the current point:\n\n\u003col\u003e\n\u003cli\u003eClassification - Take the most common class of the k neighbors\u003c/li\u003e\n\u003cli\u003eRegression - Take the average target metric of the k neighbors\u003c/li\u003e\n\u003cli\u003eBoth classification or regression can also be modified to use weighted averages based on the distance of the neighbors \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eAn incredibly important decision when using the KNN algorithm is determining an appropriate distance metric. This makes a monumental impact to the output of the algorithm. While there are additional distance metrics, such as cosine distance which we will not cover, you'll get a solid introduction to distance metrics by looking at the standard Euclidean distance and its more generic counterpart, Minkowski distance.\u003c/p\u003e\n\n\u003ch2\u003eK-means\u003c/h2\u003e\n\n\u003cp\u003eWhile outside the scope of this section, it is worth mentioning the related K-means algorithm which uses similar principles as KNN but serves as an unsupervised learning clustering algorithm. In the K-means algorithm, K represents the number of clusters rather then the number of neighbors. Unlike KNN, K-means is an iterative algorithm which repeats until convergence. Nonetheless, its underlying principle is the same, in that it groups data points together using a distance metric in order to create homogeneous groupings.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this brief lesson, you were introduced to the KNN algorithm. From here, you'll jump straight to the details of KNN, practice coding your own implementation and then get an introduction to use pre-built tools within scikit-learn for KNN.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"k-nearest-neighbors-introduction"},{"id":456257,"title":"Distance Metrics","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-distance-metrics\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about various kinds of distance metrics that you can use as a way to quantify similarity!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate Manhattan distance between two points\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate Euclidean distance between two points\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCompare and contrast Manhattan, Euclidean, and Minkowski distance \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g61391248a475ba95d6c3011ff7a40b23"},{"id":456261,"title":"Distance Metrics - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-distance-metrics-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll calculate various distances between multiple points using the distance metrics you learned about!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate Manhattan distance between two points \u003c/li\u003e\n\u003cli\u003eCalculate Euclidean distance between two points\u003c/li\u003e\n\u003cli\u003eCalculate Minkowski distance between two points\u003c/li\u003e\n\u003c/ul\u003e","exportId":"ga1388e96c2ed6de2dfc2332638720aeb"},{"id":456267,"title":"K-Nearest Neighbors","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about a supervised learning algorithm, \u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e; and how you can use it to make predictions for classification and regression tasks!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how KNN makes classifications\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is K-Nearest Neighbors?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e (or KNN, for short) is a supervised learning algorithm that can be used for both \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e tasks. However, in this section, we will cover KNN only in the context of classification. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between two points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a \u003cstrong\u003e\u003cem\u003eSupervised learning algorithm\u003c/em\u003e\u003c/strong\u003e, you must also have the labels for each point in the dataset, or else you wouldn't know what to predict!\u003c/p\u003e\n\n\u003ch2\u003eFitting the model\u003c/h2\u003e\n\n\u003cp\u003eKNN is unique compared to other classifiers in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the \"fit\" step, KNN just stores all the training data and corresponding labels. No distances are calculated at this point. \u003c/p\u003e\n\n\u003ch2\u003eMaking predictions with K\u003c/h2\u003e\n\n\u003cp\u003eAll the magic happens during the \"predict\" step. During this step, KNN takes a point that you want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the \u003ccode\u003eK\u003c/code\u003e closest points, or \u003cstrong\u003e\u003cem\u003eNeighbors\u003c/em\u003e\u003c/strong\u003e, and examines the labels of each. You can think of each of the K-closest points getting to 'vote' about the predicted class. Naturally, they all vote for the same class that they belong to. The majority wins, and the algorithm predicts the point in question as whichever class has the highest count among all of the k-nearest neighbors.\u003c/p\u003e\n\n\u003cp\u003eIn the following animation, K=3: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-nearest-neighbors/master/images/knn.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://gfycat.com/wildsorrowfulchevrotain\"\u003egif source\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eWhen using KNN, you can use \u003cstrong\u003e\u003cem\u003eManhattan\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eEuclidean\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eMinkowski distance\u003c/em\u003e\u003c/strong\u003e, or any other distance metric. Choosing an appropriate distance metric is essential and will depend on the context of the problem at hand.\u003c/p\u003e\n\n\u003ch2\u003eEvaluating model performance\u003c/h2\u003e\n\n\u003cp\u003eHow to evaluate the model performance depends on whether you're using the model for a classification or regression task. KNN can be used for regression (by averaging the target scores from each of the K-nearest neighbors), as well as for both binary and multicategorical classification tasks. \u003c/p\u003e\n\n\u003cp\u003eEvaluating classification performance for KNN works the same as evaluating performance for any other classification algorithm -- you need a set of predictions, and the corresponding ground-truth labels for each of the points you made a prediction on. You can then compute evaluation metrics such as \u003cstrong\u003e\u003cem\u003ePrecision, Recall, Accuracy, F1-Score\u003c/em\u003e\u003c/strong\u003e etc. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat! Now that you know how the KNN classifier works, you'll implement KNN using Python from scratch in the next lab.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"k-nearest-neighbors"},{"id":456272,"title":"K-Nearest Neighbors - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-k-nearest-neighbors-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll build a simple version of a \u003cstrong\u003e\u003cem\u003eK-Nearest Neigbors classifier\u003c/em\u003e\u003c/strong\u003e from scratch, and train it to make predictions on a dataset!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement a basic KNN algorithm from scratch\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g96f15a5a4928ab60f394fc9eedfc023d"},{"id":456277,"title":"Finding the Best Value for K","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll investigate how changing the value for K can affect the performance of the model, and how to use this to find the best value for K.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eConduct a parameter search to find the optimal value for K \u003c/li\u003e\n\u003cli\u003eExplain how KNN is related to the curse of dimensionality \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eFinding the optimal number of neighbors\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've got a strong understanding of how the K-Nearest Neighbors algorithm works, but you likely have at least one lingering question‚Äî\u003cstrong\u003e\u003cem\u003ewhat is the best value to use for K\u003c/em\u003e\u003c/strong\u003e? There's no set number that works best. If there was, it wouldn't be called \u003cstrong\u003e\u003cem\u003eK\u003c/em\u003e\u003c/strong\u003e-nearest neighbors. While the best value for K is not immediately obvious for any problem, there are some strategies that you can use to select a good or near optimal value. \u003c/p\u003e\n\n\u003ch2\u003eK, overfitting, and underfitting\u003c/h2\u003e\n\n\u003cp\u003eIn general, the smaller K is, the tighter the \"fit\" of the model. Remember that with supervised learning, you want to fit a model to the data as closely as possible without \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e to patterns in the training set that don't generalize.  This can happen if your model pays too much attention to every little detail and makes a very complex decision boundary. Conversely, if your model is overly simplistic, then you may have \u003cstrong\u003e\u003cem\u003eunderfit\u003c/em\u003e\u003c/strong\u003e the model, limiting its potential. A visual explanation helps demonstrate this concept in practice:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/fit_fs.png\" width=\"700\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen K is small, any given prediction only takes into account a very small number of points around it to make the prediction. If K is too small, this can end up with a decision boundary that looks like the overfit picture on the right. \u003c/p\u003e\n\n\u003cp\u003eConversely, as K grows larger, it takes into account more and more points, that are farther and farther away from the point in question, increasing the overall size of the region taken into account. If K grows too large, then the model begins to underfit the data. \u003c/p\u003e\n\n\u003cp\u003eIt's important to try to find the best value for K by iterating over a multiple values and comparing performance at each step. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/best_k_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see from the image above, \u003ccode\u003ek=1\u003c/code\u003e and \u003ccode\u003ek=3\u003c/code\u003e will provide different results! \u003c/p\u003e\n\n\u003ch2\u003eIterating over values of K\u003c/h2\u003e\n\n\u003cp\u003eSince the model arrives at a prediction by voting, it makes sense that you should only use odd values for k, to avoid ties and subsequent arbitrary guesswork. By adding this constraint (an odd value for k) the model will never be able to evenly split between two classes. From here, finding an optimal value of K requires some iterative investigation.\u003c/p\u003e\n\n\u003cp\u003eThe best way to find an optimal value for K is to choose a minimum and maximum boundary and try them all! In practice, this means:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eFit a KNN classifier for each value of K \u003c/li\u003e\n\u003cli\u003eGenerate predictions with that model\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate and evaluate a performance metric using the predictions the model made \u003c/li\u003e\n\u003cli\u003eCompare the results for every model and find the one with the lowest overall error, or highest overall score!\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/plot_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eA common way to find the best value for K at a glance is to plot the error for each value of K. Find the value for K where the error is lowest. If this graph continued into higher values of K, we would likely see the error numbers go back up as K increased. \u003c/p\u003e\n\n\u003ch2\u003eKNN and the curse of dimensionality\u003c/h2\u003e\n\n\u003cp\u003eNote that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential. As you add more data points to the dataset, the number of operations needed to complete all the steps of the algorithm grows exponentially! That said, for smaller datasets, KNN often works surprisingly well, given the simplicity of the overall algorithm. However, if your dataset contains millions of rows and thousands of columns, you may want to choose another algorithm, as the algorithm may not run in any reasonable amount of time;in some cases, it could quite literally take years to complete! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you learned how to determine the best value for K and that the KNN algorithm may not necessarily be the best choice for large datasets due to the large amount of time it can take for the algorithm to run. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"finding-the-best-value-for-k"},{"id":456282,"title":"KNN with scikit-learn","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll explore how to use scikit-learn's implementation of the K-Nearest Neighbors algorithm. In addition, you'll also learn about best practices for using the algorithm. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the considerations when fitting a KNN model using scikit-learn\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy use scikit-learn?\u003c/h2\u003e\n\n\u003cp\u003eWhile you've written your own implementation of the KNN algorithm, scikit-learn adds many backend optimizations which can make the algorithm perform faster and more efficiently. Building your own implementation of any machine learning algorithm is a valuable experience, providing great insight into how said algorithm works. However, in general, you should always use professional toolsets such as scikit-learn whenever possible; since their implementations will always be best-in-class, in a way a single developer or data scientist simply can't hope to rival on their own. In the case of KNN, you'll find scikit-learn's implementation to be much more robust and fast, because of optimizations such as caching distances in clever ways under the hood. \u003c/p\u003e\n\n\u003ch2\u003eRead the \u003ccode\u003esklearn\u003c/code\u003e docs\u003c/h2\u003e\n\n\u003cp\u003eAs a rule of thumb, you should familiarize yourself with any documentation available for any libraries or frameworks you use. scikit-learn provides high-quality documentation. For every algorithm, you'll find a general \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\"\u003edocumentation page\u003c/a\u003e which tells you inputs, parameters, outputs, and caveats of any algorithm. In addition, you'll also find very informative \u003ca href=\"https://scikit-learn.org/stable/modules/neighbors.html#classification\"\u003eUser Guides\u003c/a\u003e that explain both how the algorithm works, and how to best use it, complete with sample code! \u003c/p\u003e\n\n\u003cp\u003eFor example, the following image can be found in the scikit-learn user guide for K-Nearest Neighbors, along with an explanation of how different parameters can affect the overall performance of the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-knn-with-scikit-learn/master/images/knn_docs.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eBest practices\u003c/h2\u003e\n\n\u003cp\u003eYou'll also find that scikit-learn provides robust implementations for additional components of the algorithm implementation process such as evaluation metrics. With that, you can easily evaluate models using precision, accuracy, or recall scores on the fly using built-in functions!\u003c/p\u003e\n\n\u003cp\u003eWith that, it's important to focus on practical questions when completing the upcoming lab. In particular, try to focus on the following questions:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWhat decisions do I need to make regarding my data? How might these decisions affect overall performance?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhich predictors do I need? How can I confirm that I have the right predictors?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat parameter values (if any) should I choose for my model? How can I find the optimal value for a given parameter?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat metrics will I use to evaluate the performance of my model? Why?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow do I know if there's room left for improvement with my model? Are the potential performance gains worth the time needed to reach them?\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eA final note\u003c/h2\u003e\n\n\u003cp\u003eAfter cleaning, preprocessing, and modeling the data in the next lab, you'll be given the opportunity to iterate on your model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got a brief overview of some of the advantages of using scikit-learn's built-in KNN implementation. While you haven't seen specific code examples, you now have an indispensable resource: the official documentation to guide you. Since it's an incredibly important skill to know where to seek out information and how to digest that into actionable processes, it'll be up to you to piece through the necessary documentation to complete the upcoming lab. Good luck!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"knn-with-scikit-learn"},{"id":456285,"title":"KNN with scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-knn-with-scikit-learn-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll learn how to use scikit-learn's implementation of a KNN classifier on the classic Titanic dataset from Kaggle!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eConduct a parameter search to find the optimal value for K \u003c/li\u003e\n\u003cli\u003eUse a KNN classifier to generate predictions on a real-world dataset \u003c/li\u003e\n\u003cli\u003eEvaluate the performance of a KNN model\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g60e8d59944503f2fff50b17ed9050ce2"},{"id":456289,"title":"Quiz: K Nearest Neighbors","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g68b1b16cb45b0c2602ed03edb7f3ef56","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"g041301c0d6f44f9e19f87488351cf805"},{"id":456302,"title":"‚≠êÔ∏è Nonparametric ML Models - Cumulative Lab","type":"Quizzes::Quiz","indent":0,"locked":false,"assignmentExportId":"g319dbdc14fa91ed73faa8241ef6a5199","questionCount":1,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_submit","completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nonparametric-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nonparametric-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure what to do, refer to \u003ca title=\"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\" href=\"quizzes/g9d5b89d09608ee0da642cc68cf19da0e\"\u003e‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\u003c/a\u003e\u003c/p\u003e","exportId":"g1056c04a78eb2b8ee77b244c23859397"},{"id":456307,"title":"K-Nearest Neighbors - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you'll briefly review some of the key concepts covered in this section including KNN's computational complexity and how to properly tune a model using scikit-learn. \u003c/p\u003e\n\n\u003ch2\u003eK-Nearest Neighbors\u003c/h2\u003e\n\n\u003cp\u003eAs you saw, KNN is an intuitive algorithm: to generate a prediction for a given data point, it finds the k-nearest data points and then predicts the majority class of these k points.\u003c/p\u003e\n\n\u003ch3\u003eComputational complexity\u003c/h3\u003e\n\n\u003cp\u003eAlso of note is the computational complexity of the KNN algorithm. As the number of data points and features increase, the required calculations increases exponentially! As such, KNN is extremely resource intensive for large datasets.\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eYou learned about Minkowski distance and two cases of Minkowski distance: Euclidean and Manhattan distance. Other distance metrics such as Hamming distance can even be used to compare strings! (Hamming distance can be used to offer typo correction-suggestions for instance by comparing similar words generated by changing only one or two letters from the mistyped word). \u003c/p\u003e\n\n\u003ch2\u003eModel tuning in scikit-learn\u003c/h2\u003e\n\n\u003cp\u003eRemember that model tuning encapsulates the entire gamut of the data science process from problem formulation and preprocessing through hyperparameter tuning. Furthermore, you also need to choose a validation method to determine the model's ability to generalize to new cases such as train-test split or cross-validation. Good models require careful thought, ample preprocessing, and exploration followed by hyperparameter tuning.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWell done! You have added another algorithm in your toolset. Even though KNN doesn't scale well to larger datasets, it has many useful applications from recommendations to classification. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"k-nearest-neighbors-recap"}]},{"id":46978,"name":"Topic 28: Bayes Classification","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gccd76d19614efb1b6f9d4bc63d404deb","items":[{"id":456320,"title":"Topic 28 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 100%; height: 217px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eNaive Bayes\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 29.8418%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.00988%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003ca title=\"Bayesian Classification - Introduction\" href=\"pages/bayesian-classification-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/bayesian-classification-introduction\" data-api-returntype=\"Page\"\u003eBayesian Classification - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 28px;\"\u003e\u003cstrong\u003e \u003ca title=\"Classifiers with Bayes\" href=\"pages/classifiers-with-bayes\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/classifiers-with-bayes\" data-api-returntype=\"Page\"\u003eClassifiers with Bayes\u003c/a\u003e \u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e \u003ca title=\"Gaussian Naive Bayes\" href=\"assignments/gc08f06121b1cb6900720308cdce032e8\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186981\" data-api-returntype=\"Assignment\"\u003eGaussian Naive Bayes\u003c/a\u003e \u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003ca title=\"Gaussian Naive Bayes - Lab\" href=\"assignments/gb77f66d2830c911912d6ef073be94510\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186982\" data-api-returntype=\"Assignment\"\u003eGaussian Naive Bayes - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Document Classification with Naive Bayes\" href=\"assignments/g236db4ca49d5baf1e702753a44377c26\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186975\" data-api-returntype=\"Assignment\"\u003eDocument Classification with Naive Bayes\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e \u003ca title=\"Document Classification with Naive Bayes - Lab\" href=\"assignments/gad6d501b5a276265442395228b9f671b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186976\" data-api-returntype=\"Assignment\"\u003eDocument Classification with Naive Bayes - Lab\u003c/a\u003e \u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Bayes Classification\" href=\"quizzes/g072d490545d2ff5e9675b66373a4d996\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30649\" data-api-returntype=\"Quiz\"\u003eQuiz: Bayes Classification\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.8127%; height: 82px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eNaive Bayes\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 29.8418%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.00988%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 29.8418%;\"\u003e\u003ca title=\"Short Video: Naive Bayes by Hand\" href=\"pages/short-video-naive-bayes-by-hand\"\u003eShort Video: Naive Bayes by Hand\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Gaussian Naive Bayes Exit Ticket\" href=\"quizzes/gac9c11dae98412bec642d898197f115a\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30643\" data-api-returntype=\"Quiz\"\u003eGaussian Naive Bayes Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003ca title=\"Bayesian Classification - Recap\" href=\"pages/bayesian-classification-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/bayesian-classification-recap\" data-api-returntype=\"Page\"\u003eBayesian Classification - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","exportId":"topic-28-lesson-priorities-live"},{"id":456324,"title":"Bayesian Classification - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-intro-v2-1\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn an earlier section, you learned about Bayesian statistics with plenty of theory and application of Bayes theorem. You'll now take a look at using Bayes theorem to perform some classification tasks. Here, you'll see that the Bayes theorem can be applied to multiple variables simultaneously. \u003c/p\u003e\n\n\u003ch2\u003eBayes Classification\u003c/h2\u003e\n\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another, which may not be met, (hence its naivety) it can nonetheless provide strong results in scenarios with clean and well normalized datasets. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features.\u003c/p\u003e\n\n\u003cp\u003eBayes' formula extended to multiple features is:  \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\" src=\"/equation_images/%20%255CLarge%20P(y|x_1,%20x_2,%20...,%20x_n)%20=%20%255Cfrac{P(y)%255Cprod_{i}^{n}P(x_i|y)}{P(x_1,%20x_2,%20...,%20x_n)}\" alt=\"{\" data-equation-content=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\"\u003e\u003c/p\u003e \u003cp\u003e\u003c/p\u003e\n\n\u003ch2\u003eDocument Classification\u003c/h2\u003e\n\n\u003cp\u003eAn interesting application of Bayes' theorem is to use \u003cem\u003ebag of words\u003c/em\u003e for document classification. A bag of words representation takes a text document and converts it into a word frequency representation. In this section, you'll use bag of words and Naive Bayes to classify YouTube videos into appropriate topics. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eOver the next few lessons you will learn about another fundamental classification algorithm which has many practical applications. It's time to jump into the wonderful Bayesian world again! This section will help you solidify your understanding of Bayesian stats. \u003c/p\u003e","exportId":"bayesian-classification-introduction"},{"id":456330,"title":"Classifiers with Bayes","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classifiers-with-bayes\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you're familiar with Bayes' theorem and foundational concepts of Bayesian statistics, you'll take a look at how to implement some of these ideas for machine learning. Classification tasks can be a natural application of Bayes' theorem since you are looking to predict some label given other information, which can be conceptualized through conditional probability.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the assumption that leads to Naive Bayes being \"naive\"\u003c/li\u003e\n\u003cli\u003eExplain how to use the probabilities generated by Naive Bayes to make a classification \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eNaive Bayes\u003c/h2\u003e\n\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features. \u003c/p\u003e\n\n\u003cp\u003eFor example, extending the previous medical examples of Bayes' theorem, a researcher might examine multiple patient measurements to better predict whether or not an individual has a given disease. Provided that these measurements are independent (and uncorrelated from one another), one can then examine the conditional probability of each of these metrics and apply Bayes' theorem to determine a relative probability of having the disease or not. Combining these probabilities can then give an overall confidence of a patient having the disease given all the information. From this, one can then make a prediction for whether or not you believe an individual has the disease or not based on which probability is higher.\u003c/p\u003e\n\n\u003cp\u003eMathematically, if \u003cimg class=\"equation_image\" title=\"Y\" src=\"https://learning.flatironschool.com/equation_images/Y\" alt=\"{\" data-equation-content=\"Y\"\u003e is a class you wish to predict (such as having a disease) and \u003cimg class=\"equation_image\" title=\"X_1, X_2, ..., X_n\" src=\"https://learning.flatironschool.com/equation_images/X_1,%20X_2,%20...,%20X_n\" alt=\"{\" data-equation-content=\"X_1, X_2, ..., X_n\"\u003e are the various measurements for the given individual or case, then the probability of class \u003cimg class=\"equation_image\" title=\"Y\" src=\"https://learning.flatironschool.com/equation_images/Y\" alt=\"{\" data-equation-content=\"Y\"\u003e can be written as:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\large P(Y|X_1, X_2, ..., X_n) = \\dfrac{P(X_1|Y) \\cdot P(X_2|Y) \\cdot ... \\cdot P(X_n|Y)}{P(X_1, X_2, ..., X_n)}P(Y)\" src=\"/equation_images/%20%255Clarge%20P(Y|X_1,%20X_2,%20...,%20X_n)%20=%20%255Cdfrac{P(X_1|Y)%20%255Ccdot%20P(X_2|Y)%20%255Ccdot%20...%20%255Ccdot%20P(X_n|Y)}{P(X_1,%20X_2,%20...,%20X_n)}P(Y)\" alt=\"{\" data-equation-content=\" \\large P(Y|X_1, X_2, ..., X_n) = \\dfrac{P(X_1|Y) \\cdot P(X_2|Y) \\cdot ... \\cdot P(X_n|Y)}{P(X_1, X_2, ..., X_n)}P(Y)\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eAgain, note that multiplying the conditional probabilities is based on the assumption that these probabilities (and their underlying features) are independent -- and it is this assumption that the Naive Bayes algorithm is considered naive, or simple, because this is almost never true. However, Naives Bayes can prove to be quite efficient given the right circumstances, as you will see in the upcoming lessons. \u003c/p\u003e\n\n\u003cp\u003eIn practice, calculating the denominator, \u003cimg class=\"equation_image\" title=\"P(X_1, X_2, ..., X_n)\" src=\"https://learning.flatironschool.com/equation_images/P(X_1,%20X_2,%20...,%20X_n)\" alt=\"{\" data-equation-content=\"P(X_1, X_2, ..., X_n)\"\u003e is often impractical or impossible as this exact combination of features may not have been previously observed. However, doing so is often not required. This is because when implementing a classifier, the exact probabilities themselves are not required to generate a prediction. Instead, you must simply answer which option is the most probable. To do this, you would calculate \u003cimg class=\"equation_image\" title=\"P(Y_0)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_0)\" alt=\"{\" data-equation-content=\"P(Y_0)\"\u003e, the probability of not having the disease as well as \u003cimg class=\"equation_image\" title=\"P(Y_1)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_1)\" alt=\"{\" data-equation-content=\"P(Y_1)\"\u003e, the probability of having the disease. Furthermore, since the denominator, \u003cimg class=\"equation_image\" title=\"P(X_1, X_2, ..., X_n)\" src=\"https://learning.flatironschool.com/equation_images/P(X_1,%20X_2,%20...,%20X_n)\" alt=\"{\" data-equation-content=\"P(X_1, X_2, ..., X_n)\"\u003e, is equal for both \u003cimg class=\"equation_image\" title=\"P(Y_0)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_0)\" alt=\"{\" data-equation-content=\"P(Y_0)\"\u003e and \u003cimg class=\"equation_image\" title=\"P(Y_1)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_1)\" alt=\"{\" data-equation-content=\"P(Y_1)\"\u003e, you can compare the numerators, as these will be proportional to the overall probability. You'll investigate this further as you code some Naive Bayes classification algorithms yourself in the upcoming lessons.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly explored how Bayes' theorem can be used to build classification algorithms. In the upcoming lessons and labs you'll investigate particular implementations of Naive Bayes classifiers which differ in how the individual conditional probabilities themselves are constructed. As you will see, Naive Bayes can be extremely effective or trivially useful depending on the context and implementation.\u003c/p\u003e","exportId":"classifiers-with-bayes"},{"id":456334,"title":"Gaussian Naive Bayes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gaussian-naive-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eExpanding Bayes theorem to account for multiple observations and conditional probabilities drastically increases predictive power. In essence, it allows you to develop a belief network taking into account all of the available information regarding the scenario. In this lesson, you'll take a look at one particular implementation of a multinomial naive Bayes algorithm: Gaussian Naive Bayes.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the Gaussian Naive Bayes algorithm\u003c/li\u003e\n\u003cli\u003eImplement the Gaussian Naive Bayes (GNB) algorithm using SciPy and NumPy\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gc08f06121b1cb6900720308cdce032e8"},{"id":456338,"title":"Gaussian Naive Bayes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gaussian-naive-bayes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you've seen how to employ multinomial Bayes for classification, its time to practice implementing the process yourself. You'll also get a chance to investigate the impacts of using true probabilities under the probability density function as opposed to the point estimate on the curve itself.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIndependently code and implement the Gaussian Naive Bayes algorithm\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gb77f66d2830c911912d6ef073be94510"},{"id":456343,"title":"Document Classification with Naive Bayes","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-document-classification-with-naive-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll investigate another implementation of the Bayesian framework in order to classify YouTube videos into the appropriate topic. The dataset you'll be investigating again comes from Kaggle. For further information, you can check out the original dataset here: \u003ca href=\"https://www.kaggle.com/extralime/math-lectures\"\u003ehttps://www.kaggle.com/extralime/math-lectures\u003c/a\u003e .\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement document classification using Naive Bayes \u003c/li\u003e\n\u003cli\u003eExplain how to code a bag of words representation\u003c/li\u003e\n\u003cli\u003eExplain why it is necessary to use Laplacian smoothing correction\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g236db4ca49d5baf1e702753a44377c26"},{"id":456347,"title":"Document Classification with Naive Bayes - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-document-classification-with-naive-bayes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll practice implementing the Naive Bayes algorithm on your own.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement document classification using Naive Bayes\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gad6d501b5a276265442395228b9f671b"},{"id":456352,"title":"Quiz: Bayes Classification","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g42112b49608a7d1b46550bab97255d53","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"g072d490545d2ff5e9675b66373a4d996"},{"id":456367,"title":"Short Video: Naive Bayes by Hand","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802877?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-naive-bayes-by-hand"},{"id":456376,"title":"Bayesian Classification - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-recap-v2-1\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features \u003c/li\u003e\n\u003cli\u003eThis assumption (that the underlying features are independent) is why Naive Bayes algorithm is considered naive -- because this is almost never true. However, Naives Bayes can prove to be quite efficient \u003c/li\u003e\n\u003cli\u003eExpanding to multiple features, the multinomial Bayes' formula is:\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\" src=\"/equation_images/%20%255CLarge%20P(y|x_1,%20x_2,%20...,%20x_n)%20=%20%255Cfrac{P(y)%255Cprod_{i}^{n}P(x_i|y)}{P(x_1,%20x_2,%20...,%20x_n)}\" alt=\"{\" data-equation-content=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\"\u003e\u003c/p\u003e \u003cp\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFinally, you saw how Naive Bayes algorithm can be used for document classification by classifying YouTube videos into the appropriate topic, and classifying documents as \"spam\" or \"no spam\"\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eDue to insufficient text preprocessing (which you will learn how to do in a later module), the performance of this algorithm was trivial \u003c/li\u003e\n\u003c/ul\u003e","exportId":"bayesian-classification-recap"}]},{"id":46979,"name":"Topic 29: Model Tuning and Pipelines","status":"completed","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"gaab62e319239fc8a2aa6ed300350f65e","items":[{"id":456390,"title":"Topic 29 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.9064%; height: 106px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eModel Tuning\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Model Tuning and Pipelines - Introduction\" href=\"pages/model-tuning-and-pipelines-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/model-tuning-and-pipelines-introduction\" data-api-returntype=\"Page\"\u003eModel Tuning and Pipelines - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003cstrong\u003e\u003ca title=\"GridSearchCV\" href=\"pages/gridsearchcv\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/gridsearchcv\" data-api-returntype=\"Page\"\u003eGridSearchCV\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Introduction to Pipelines\" href=\"pages/introduction-to-pipelines\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/introduction-to-pipelines\" data-api-returntype=\"Page\"\u003e\u003cstrong\u003eIntroduction to Pipelines\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Pipelines in scikit-learn - Lab\" href=\"assignments/g9460c3b0812bc3c864b5800d801a5245\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187031\" data-api-returntype=\"Assignment\"\u003ePipelines in scikit-learn - Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003ca class=\"ig-title title item_link\" title=\"Refactoring Your Code to Use Pipelines\" href=\"modules/items/g6e46afb5a0281f261c0a9a477ea29717\"\u003eRefactoring Your Code to Use Pipelines\u003c/a\u003e\u003cstrong\u003e\u003ca class=\"ig-title title item_link\" title=\"Refactoring Your Code to Use Pipelines\" href=\"modules/items/g6e46afb5a0281f261c0a9a477ea29717\"\u003e\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003ca title=\"Short Video: Pipelines\" href=\"pages/short-video-pipelines\"\u003eShort Video: Pipelines\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.9064%; height: 106px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eModel Tuning\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003cstrong\u003e\u003ca title=\"Model Tuning Exit Ticket\" href=\"quizzes/g5436ad5ecb4614660bbf52f63218ae19\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30638\" data-api-returntype=\"Quiz\"\u003eModel Tuning Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Pickle\" href=\"assignments/g0576788fe1f7929f3f8b2444c8156ef3\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187029\" data-api-returntype=\"Assignment\"\u003ePickle\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003ca title=\"Pickling and Deploying Pipelines\" href=\"assignments/g8492efd41e732bf37959b70989a9dcb6\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187030\" data-api-returntype=\"Assignment\"\u003ePickling and Deploying Pipelines\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Model Tuning and Pipelines - Recap\" href=\"pages/model-tuning-and-pipelines-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/model-tuning-and-pipelines-recap\" data-api-returntype=\"Page\"\u003eModel Tuning and Pipelines - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","exportId":"topic-29-lesson-priorities-live"},{"id":456394,"title":"Model Tuning and Pipelines - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-pipelines-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have learned the basics of a supervised learning workflow, it's time to get into some more-advanced techniques! In this section you'll learn about tools for tuning model hyperparameters, building pipelines, and persisting your trained model on disk.\u003c/p\u003e\n\n\u003ch2\u003eTuning Model Hyperparameters with GridSearchCV\u003c/h2\u003e\n\n\u003cp\u003eWith non-parametric models such as decision trees and k-nearest neighbors, you have seen that there are various hyperparameters that you can specify when you instantiate the model. For example, the maximum depth of the tree, or the number of neighbors. Often these hyperparameters help to balance the bias-variance trade-off between underfitting and overfitting and are important for finding the optimal model.\u003c/p\u003e\n\n\u003cp\u003eWith so many different hyperparameter combinations to try out, it can be difficult to write clean, readable code. Fortunately there is a tool from scikit-learn called \u003ccode\u003eGridSearchCV\u003c/code\u003e that is specifically designed to search through a \"grid\" of hyperparameters! In this section we'll introduce how to use this tool.\u003c/p\u003e\n\n\u003ch2\u003eMachine Learning Pipelines\u003c/h2\u003e\n\n\u003cp\u003ePipelines are extremely useful for allowing data scientists to quickly and consistently transform data, train machine learning models, and make predictions.\u003c/p\u003e\n\n\u003cp\u003eBy now, you know that the data science process is a flow of activities, from inspecting the data to cleaning it, transforming it, running a model, and discussing the results. Wouldn't it be nice if there was a streamlined process to create nice machine learning workflows? Enter the \u003ccode\u003ePipeline\u003c/code\u003e class in scikit-learn!\u003c/p\u003e\n\n\u003cp\u003eIn this section, you'll learn how you can use a pipeline to integrate several steps of the machine learning workflow. Additionally, you'll compare several classification techniques with each other, and integrate grid search in your pipeline so you can tune several hyperparameters in each of the machine learning models while also avoiding data leakage.\u003c/p\u003e\n\n\u003ch2\u003ePickle and Model Deployment\u003c/h2\u003e\n\n\u003cp\u003eSo far, as soon as you shut down your notebook kernel, your model ceases to exist. If you wanted to use the model to make predictions again, you would need to re-train the model. This is time-consuming and makes your model a lot less useful.\u003c/p\u003e\n\n\u003cp\u003eLuckily there are techniques to \u003cem\u003epickle\u003c/em\u003e your model -- basically, to store the model for later, so that it can be loaded and can make predictions without being trained again. Pickled models are also typically used in the context of model deployment, where your model can be used as the backend of an API!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section only scratches the surface of the advanced modeling tools you might use as a data scientist. Get ready to optimize your workflow and get beyond the basics!\u003c/p\u003e","exportId":"model-tuning-and-pipelines-introduction"},{"id":456396,"title":"GridSearchCV","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gridsearchcv\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll explore the concept of parameter tuning to maximize our model performance using a combinatorial grid search!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDesign a parameter grid for use with scikit-learn's \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e to increase model performance through parameter tuning\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eParameter tuning\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've seen that the process of building and training a supervised learning model is an iterative one. Your first model rarely performs the best! There are multiple ways we can potentially improve model performance. Thus far, most of the techniques we've used have been focused on our data. We can get better data, or more data, or both. We can engineer certain features, or clean up the data by removing rows/variables that hurt model performance, like multicollinearity. \u003c/p\u003e\n\n\u003cp\u003eThe other major way to potentially improve model performance is to find good parameters to set when creating the model. For example, if we allow a decision tree to have too many leaves, the model will almost certainly overfit the data. Too few, and the model will underfit. However, each modeling problem is unique -- the same parameters could cause either of those situations, depending on the data, the task at hand, and the complexity of the model needed to best fit the data. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll learn how we can use a \u003cstrong\u003e\u003cem\u003ecombinatorial grid search\u003c/em\u003e\u003c/strong\u003e to find the best combination of parameters for a given model. \u003c/p\u003e\n\n\u003ch2\u003eGrid search\u003c/h2\u003e\n\n\u003cp\u003eWhen we set parameters in a model, the parameters are not independent of one another -- the value set for one parameter can have significant effects on other parameters, thereby affecting overall model performance. Consider the following grid.\u003c/p\u003e\n\n\u003ctable\u003e\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center\"\u003eParameter\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e1\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e2\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e3\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e4\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003ecriterion\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\"gini\"\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\"entropy\"\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003emax_depth\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e2\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003emin_samples_split\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e20\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eAll the parameters above work together to create the framework of the decision tree that will be trained. For a given problem, it may be the case that increasing the value of the parameter for \u003ccode\u003emin_samples_split\u003c/code\u003e generally improves model performance up to a certain point, by reducing overfitting. However, if the value for \u003ccode\u003emax_depth\u003c/code\u003e is too low or too high, this may doom the model to overfitting or underfitting, by having a tree with too many arbitrary levels and splits that overfit on noise, or limiting the model to nothing more than a \"stump\" by only allowing it to grow to one or two levels. \u003c/p\u003e\n\n\u003cp\u003eSo how do we know which combination of parameters is best? The only way we can really know for sure is to try \u003cstrong\u003e\u003cem\u003eevery single combination!\u003c/em\u003e\u003c/strong\u003e For this reason, grid search is sometimes referred to as an \u003cstrong\u003e\u003cem\u003eexhaustive search\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003ch2\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library provides an easy way to tune model parameters through an exhaustive search by using its \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\"\u003e\u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/a\u003e class, which can be found inside the \u003ccode\u003emodel_selection\u003c/code\u003e module. \u003ccode\u003eGridsearchCV\u003c/code\u003e combines \u003cstrong\u003e\u003cem\u003eK-Fold Cross-Validation\u003c/em\u003e\u003c/strong\u003e with a grid search of parameters. In order to do this, we must first create a \u003cstrong\u003e\u003cem\u003eparameter grid\u003c/em\u003e\u003c/strong\u003e that tells \u003ccode\u003esklearn\u003c/code\u003e which parameters to tune, and which values to try for each of those parameters. \u003c/p\u003e\n\n\u003cp\u003eThe following code snippet demonstrates how to use \u003ccode\u003eGridSearchCV\u003c/code\u003e to perform a parameter grid search using a sample parameter grid, \u003ccode\u003eparam_grid\u003c/code\u003e. Our parameter grid should be a dictionary, where the keys are the parameter names, and the values are the different parameter values we want to use in our grid search for each given key. After creating the dictionary, all you need to do is pass it to \u003ccode\u003eGridSearchCV()\u003c/code\u003e along with the classifier. You can also use K-fold cross-validation during this process, by specifying the \u003ccode\u003ecv\u003c/code\u003e parameter. In this case, we choose to use 3-fold cross-validation for each model created inside our grid search. \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eclf\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eDecisionTreeClassifier\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eparam_grid\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'criterion'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e'gini'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'entropy'\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'max_depth'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'min_samples_split'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e20\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003egs_tree\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eGridSearchCV\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclf\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eparam_grid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003egs_tree\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etrain_data\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etrain_labels\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003egs_tree\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebest_params_\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis code will run all combinations of the parameters above. The first model to be trained would be \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=1)\u003c/code\u003e using a 3-fold cross-validation, and recording the average score. Then, it will change one parameter, and repeat the process (e.g., \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=5)\u003c/code\u003e, and so on), keeping track of the overall performance of each model. Once it has tried every combination, the \u003ccode\u003eGridSearchCV\u003c/code\u003e object we created will automatically default the model that had the best score. We can even access the best combination of parameters by checking the \u003ccode\u003ebest_params_\u003c/code\u003e attribute! \u003c/p\u003e\n\n\u003ch2\u003eDrawbacks of \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eGridSearchCV is a great tool for finding the best combination of parameters. However, it is only as good as the parameters we put in our parameter grid -- so we need to be very thoughtful during this step! \u003c/p\u003e\n\n\u003cp\u003eThe main drawback of an exhaustive search such as \u003ccode\u003eGridsearchCV\u003c/code\u003e is that there is no way of telling what's best until we've exhausted all possibilities! This means training many versions of the same machine learning model, which can be very time consuming and computationally expensive. Consider the example code above -- we have three different parameters, with 2, 4, and 4 variations to try, respectively. We also set the model to use cross-validation with a value of 3, meaning that each model will be built 3 times, and their performances averaged together. If we do some simple math, we can see that this simple grid search we see above actually results in \u003ccode\u003e2 * 4 * 4 * 3 =\u003c/code\u003e \u003cstrong\u003e\u003cem\u003e96 different models trained!\u003c/em\u003e\u003c/strong\u003e For projects that involve complex models and/or very large datasets, the time needed to run a grid search can often be prohibitive. For this reason, be very thoughtful about the parameters you set -- sometimes the extra runtime isn't worth it -- especially when there's no guarantee that the model performance will improve!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about grid search, how to perform grid search, and the drawbacks associated with the method!\u003c/p\u003e","exportId":"gridsearchcv"},{"id":456398,"title":"Introduction to Pipelines","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pipelines-v2-1\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've learned a substantial number of different supervised learning algorithms. Now, it's time to learn about a handy tool used to integrate these algorithms into a single manageable pipeline.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain how pipelines can be used to combine various parts of a machine learning workflow\u003cbr\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy Use Pipelines?\u003c/h2\u003e\n\n\u003cp\u003ePipelines are extremely useful tools to write clean and manageable code for machine learning. Recall how we start preparing our dataset: we want to clean our data, transform it, potentially use feature selection, and then run a machine learning algorithm. Using pipelines, you can do all these steps in one go!\u003c/p\u003e\n\n\u003cp\u003ePipeline functionality can be found in scikit-learn's \u003ccode\u003ePipeline\u003c/code\u003e module. Pipelines can be coded in a very simple way:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003esklearn.pipeline\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Create the pipeline\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\u003cspan class=\"p\"\u003e([(\u003c/span\u003e\u003cspan class=\"s\"\u003e'mms'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eMinMaxScaler\u003c/span\u003e\u003cspan class=\"p\"\u003e()),\u003c/span\u003e\n                 \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'tree'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eDecisionTreeClassifier\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erandom_state\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e123\u003c/span\u003e\u003cspan class=\"p\"\u003e))])\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis pipeline will ensure that first we'll apply a Min-Max scaler on our data before fitting a decision tree. However, the \u003ccode\u003ePipeline()\u003c/code\u003e function above is only defining the sequence of actions to perform. In order to actually fit the model, you need to call the \u003ccode\u003e.fit()\u003c/code\u003e method like so: \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e# Fit to the training data\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_train\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_train\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen, to score the model on test data, you can call the \u003ccode\u003e.score()\u003c/code\u003e method like so: \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e# Calculate the score on test data\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escore\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_test\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA really good blog post on the basic ideas of pipelines can be found \u003ca href=\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eIntegrating Grid Search in Pipelines\u003c/h2\u003e\n\n\u003cp\u003eNote that the above pipeline simply creates one pipeline for a training set, and evaluates on a test set. Is it possible to create a pipeline that performs grid search? And cross-validation? Yes, it is!\u003c/p\u003e\n\n\u003cp\u003eFirst, you define the pipeline in the same way as above. Next, you create a parameter grid. When this is all done, you use the function \u003ccode\u003eGridSearchCV()\u003c/code\u003e, which you've seen before, and specify the pipeline as the estimator and the parameter grid. You also have to define how many folds you'll use in your cross-validation. \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e# Create the pipeline\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\u003cspan class=\"p\"\u003e([(\u003c/span\u003e\u003cspan class=\"s\"\u003e'mms'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eMinMaxScaler\u003c/span\u003e\u003cspan class=\"p\"\u003e()),\u003c/span\u003e\n                 \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'tree'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eDecisionTreeClassifier\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erandom_state\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e123\u003c/span\u003e\u003cspan class=\"p\"\u003e))])\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Create the grid parameter\n\u003c/span\u003e\u003cspan class=\"n\"\u003egrid\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[{\u003c/span\u003e\u003cspan class=\"s\"\u003e'tree__max_depth'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \n         \u003cspan class=\"s\"\u003e'tree__min_samples_split'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e]}]\u003c/span\u003e\n\n\n\u003cspan class=\"c1\"\u003e# Create the grid, with \"pipe\" as the estimator\n\u003c/span\u003e\u003cspan class=\"n\"\u003egridsearch\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eGridSearchCV\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eestimator\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                          \u003cspan class=\"n\"\u003eparam_grid\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003egrid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                          \u003cspan class=\"n\"\u003escoring\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'accuracy'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                          \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Fit using grid search\n\u003c/span\u003e\u003cspan class=\"n\"\u003egridsearch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_train\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_train\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Calculate the test score\n\u003c/span\u003e\u003cspan class=\"n\"\u003egridsearch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escore\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_test\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn article with a detailed workflow can be found \u003ca href=\"https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat, this wasn't too difficult! The proof of all this is in the pudding. In the next lab, you'll use this workflow to build pipelines applying classification algorithms you have learned so far in this module. \u003c/p\u003e","exportId":"introduction-to-pipelines"},{"id":456402,"title":"Pipelines in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pipelines-lab-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you will work with the \u003ca href=\"https://archive.ics.uci.edu/ml/datasets/wine+quality\"\u003eWine Quality Dataset\u003c/a\u003e. The goal of this lab is not to teach you a new classifier or even show you how to improve the performance of your existing model, but rather to help you streamline your machine learning workflows using scikit-learn pipelines. Pipelines let you keep your preprocessing and model building steps together, thus simplifying your cognitive load. You will see for yourself why pipelines are great by building the same KNN model twice in different ways. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eConstruct pipelines in scikit-learn \u003c/li\u003e\n\u003cli\u003eUse pipelines in combination with \u003ccode\u003eGridSearchCV()\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g9460c3b0812bc3c864b5800d801a5245"},{"id":456406,"title":"Refactoring Your Code to Use Pipelines","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-refactoring-with-pipelines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-refactoring-with-pipelines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-refactoring-with-pipelines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you will learn how to use the core features of scikit-learn pipelines to refactor existing machine learning preprocessing code into a portable pipeline format.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecall the benefits of using pipelines\u003c/li\u003e\n\u003cli\u003eDescribe the difference between a \u003ccode\u003ePipeline\u003c/code\u003e, a \u003ccode\u003eFeatureUnion\u003c/code\u003e, and a \u003ccode\u003eColumnTransformer\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003eIteratively refactor existing preprocessing code into a pipeline\u003c/li\u003e\n\u003c/ul\u003e","exportId":"gb884ce65901c5bbc5ad0879aa9be7100"},{"id":456414,"title":"Short Video: Pipelines","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802989?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","exportId":"short-video-pipelines"},{"id":456431,"title":"Pickle","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pickle\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003ePickle is an invaluable tool for saving objects.  In this lesson you will learn how to use it on various different Python data types.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the circumstances in which you would want to use a pickle file\u003c/li\u003e\n\u003cli\u003eWrite a pickle file\u003c/li\u003e\n\u003cli\u003eRead a pickle file\u003c/li\u003e\n\u003cli\u003eUse the \u003ccode\u003ejoblib\u003c/code\u003e library to pickle and load a scikit-learn class\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g0576788fe1f7929f3f8b2444c8156ef3"},{"id":456435,"title":"Pickling and Deploying Pipelines","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pickling-pipelines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickling-pipelines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickling-pipelines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have learned about scikit-learn pipelines and model pickling, it's time to bring it all together in a professional ML workflow!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUnderstand the purpose of deploying a machine learning model\u003c/li\u003e\n\u003cli\u003eUnderstand the cloud function approach to model deployment\u003c/li\u003e\n\u003cli\u003ePickle a scikit-learn pipeline\u003c/li\u003e\n\u003cli\u003eCreate a cloud function\u003c/li\u003e\n\u003c/ul\u003e","exportId":"g8492efd41e732bf37959b70989a9dcb6"},{"id":456439,"title":"Model Tuning and Pipelines - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-pipelines-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eMachine learning \u003cstrong\u003e\u003cem\u003epipelines\u003c/em\u003e\u003c/strong\u003e create a nice workflow to combine data manipulations, preprocessing, and modeling\u003c/li\u003e\n\u003cli\u003eMachine learning pipelines can be used along with \u003cstrong\u003e\u003cem\u003egrid search\u003c/em\u003e\u003c/strong\u003e to evaluate several parameter settings \n\n\u003cul\u003e\n\u003cli\u003eGrid search can considerably blow up computation time when computing for several parameters along with cross-validation\u003c/li\u003e\n\u003cli\u003eSome models are very sensitive to hyperparameter changes, so they should be chosen with care, and even with big grids a good outcome isn't always guaranteed\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003eMachine learning pipelines can also be \u003cstrong\u003e\u003cem\u003epickled\u003c/em\u003e\u003c/strong\u003e so that they can be used in the future without re-training\u003c/li\u003e\n\u003cli\u003eModel \u003cstrong\u003e\u003cem\u003edeployment\u003c/em\u003e\u003c/strong\u003e can be something as simple as pickling a model, or a more complex approach like a \u003cstrong\u003e\u003cem\u003ecloud function\u003c/em\u003e\u003c/strong\u003e that exposes model predictions through an HTTP API\u003c/li\u003e\n\u003c/ul\u003e","exportId":"model-tuning-and-pipelines-recap"}]},{"id":46984,"name":"Topic 30: Ensemble Methods","status":"started","unlockDate":null,"prereqs":[],"requirement":"all","sequential":false,"exportId":"g1296f971fd4327def9ab6c54b8f6e0a4","items":[{"id":456452,"title":"Topic 30 Lesson Priorities (Live)","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.3523%; height: 153px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eBagging\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Ensemble Methods - Introduction\" href=\"pages/ensemble-methods-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/ensemble-methods-introduction\" data-api-returntype=\"Page\"\u003eEnsemble Methods - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Ensemble Methods\" href=\"pages/ensemble-methods\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/ensemble-methods\" data-api-returntype=\"Page\"\u003eEnsemble Methods\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Random Forests\" href=\"pages/random-forests\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/random-forests\" data-api-returntype=\"Page\"\u003eRandom Forests\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Tree Ensembles and Random Forests - Lab\" href=\"assignments/gcbdd65aa18e5b390121d84ab623eec90\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12102\" data-api-returntype=\"Assignment\"\u003eTree Ensembles and Random Forests - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"GridSearchCV - Lab\" href=\"assignments/g24774d5f3764ee7c2cc27699b8e85222\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12104\" data-api-returntype=\"Assignment\"\u003eGridSearchCV - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.5396%; height: 155px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eBagging\u003c/em\u003e Lecture, Before \u003cem\u003eBoosting\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Bagging Exit Ticket\" href=\"quizzes/gbcb18a41f12d7478e63f428f7e764a2a\"\u003eBagging Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Gradient Boosting and Weak Learners\" href=\"pages/gradient-boosting-and-weak-learners\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/gradient-boosting-and-weak-learners\" data-api-returntype=\"Page\"\u003eGradient Boosting and Weak Learners\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Gradient Boosting - Lab\" href=\"assignments/g59ccab92e9380e16e1a668e43a6307ec\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12105\" data-api-returntype=\"Assignment\"\u003eGradient Boosting - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"XGBoost\" href=\"pages/xgboost\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/xgboost\" data-api-returntype=\"Page\"\u003eXGBoost\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"XGBoost - Lab\" href=\"assignments/ge3ed4d45a85151cc7115a93f81746615\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12106\" data-api-returntype=\"Assignment\"\u003eXGBoost - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Ensemble Methods\" href=\"quizzes/g01b43860f608fdfa4d009588e4178c7e\"\u003eQuiz: Ensemble Methods\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6335%; height: 56px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eBoosting\u003c/em\u003e Lecture, Before \u003cem\u003eClassification Workflow 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Boosting Exit Ticket\" href=\"quizzes/g918c8032ae84254b92f323de5e44d759\"\u003eBoosting Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.446%; height: 84px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Workflow 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Classification Workflow 2 Exit Ticket\" href=\"quizzes/gbd72b2305e9ba694fe616a4ad180890c\"\u003e\u003cstrong\u003eClassification Workflow 2 Exit Ticket\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Ensembles - Recap\" href=\"pages/ensembles-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/ensembles-recap\" data-api-returntype=\"Page\"\u003eEnsembles - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","exportId":"topic-30-lesson-priorities-live"},{"id":456458,"title":"Ensemble Methods - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about some of the most powerful machine learning algorithms: ensemble models! This lesson summarizes the topics we'll be covering in this section.\u003c/p\u003e\n\u003ch2\u003eEnsembles\u003c/h2\u003e\n\u003cp\u003eThe idea of ensembles is to bring together multiple models to use them to improve the quality of your predictions when compared to just using a single model. In many real-world problems and Kaggle competitions, ensemble methods tend to outperform any single model.\u003c/p\u003e\n\u003ch3\u003eEnsemble Methods\u003c/h3\u003e\n\u003cp\u003eWe start the section by providing an introduction to the concept of ensemble methods, explaining how they take advantage of the delphic technique (or \"wisdom of crowds\") where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.\u003c/p\u003e\n\u003cp\u003eWe also provide an introduction to the idea of bagging (Bootstrap Aggregation).\u003c/p\u003e\n\u003ch3\u003eRandom Forests\u003c/h3\u003e\n\u003cp\u003eWe then look at random forests - an ensemble method for decision trees that takes advantage of bagging and the subspace sampling method to create a \"forest\" of decision trees that provides consistently better predictions than any single decision tree.\u003c/p\u003e\n\u003ch3\u003eGridsearchCV\u003c/h3\u003e\n\u003cp\u003eWe will also introduce some of the common hyperparameters for tuning decision trees. In this lesson, we look at how you can use GridSearchCV to perform an exhaustive search across multiple hyperparameters and multiple possible values to come up with a better performing model.\u003c/p\u003e\n\u003ch3\u003eGradient Boosting and Weak Learners\u003c/h3\u003e\n\u003cp\u003eNext up, we introduce the concept of boosting which is at the heart of some of the most powerful ensemble methods such as Adaboost and Gradient Boosted Trees.\u003c/p\u003e\n\u003ch3\u003eXGBoost\u003c/h3\u003e\n\u003cp\u003eFinally, we end this section by introducing XGBoost (eXtreme Gradient Boosting) - the top gradient boosting algorithm currently in use.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eYou will often find yourself using a range of ensemble techniques to improve the performance of your models, so this section will introduce you to the techniques that will help you to improve the quality of your models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","exportId":"ensemble-methods-introduction"},{"id":456463,"title":"Ensemble Methods","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about \u003cstrong\u003e\u003cem\u003eensembles\u003c/em\u003e\u003c/strong\u003e and why they're such an effective technique for supervised learning. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by \"ensemble methods\"\u003c/li\u003e\n\u003cli\u003eExplain the concept of bagging as it applies to ensemble methods \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat are ensembles?\u003c/h2\u003e\n\n\u003cp\u003eIn Data Science, the term \u003cstrong\u003e\u003cem\u003eensemble\u003c/em\u003e\u003c/strong\u003e refers to an algorithm that makes use of more than one model to make a prediction. Typically, when people talk about ensembles, they are referring to Supervised Learning, although there has been some ongoing research on using ensembles for unsupervised learning tasks. Ensemble methods are typically more effective when compared with single-model results for supervised learning tasks. Most Kaggle competitions are won using ensemble methods, and \u003ca href=\"https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\"\u003emuch has been written\u003c/a\u003e about why they tend to be so successful for these tasks. \u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eConsider the following scenario -- you are looking to invest in a company, and you want to know if that company's stock will go up or down in the next year. Instead of just asking a single person, you have the following experts available to you:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eStock Broker\u003c/em\u003e\u003c/strong\u003e: This person makes correct predictions 80% of the time\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eFinance Professor\u003c/em\u003e\u003c/strong\u003e: This person is correct 65% of the time\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eInvestment Expert\u003c/em\u003e\u003c/strong\u003e: This person is correct 85% of the time\u003cbr\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIf we could only take advice from one person, we would pick the Investment Expert, and we can only be 85% sure that they are right.  \u003c/p\u003e\n\n\u003cp\u003eHowever, if we can use all three, we can combine their knowledge to increase our overall accuracy. If they all agree that the stock is a good investment, what is the overall accuracy of the combined prediction?\u003c/p\u003e\n\n\u003cp\u003eWe can calculate this by multiplying the chances that each of them are wrong together, which is \u003cimg class=\"equation_image\" title=\" 0.2 * 0.35 * 0.15 = 0.0105\\ error\\ rate\" src=\"https://learning.flatironschool.com/equation_images/%200.2%20*%200.35%20*%200.15%20=%200.0105%255C%20error%255C%20rate\" alt=\"{\" data-equation-content=\" 0.2 * 0.35 * 0.15 = 0.0105\\ error\\ rate\"\u003e, which means that our combined accuracy is \u003cimg class=\"equation_image\" title=\"1 - 0.0105 = 0.9895\" src=\"https://learning.flatironschool.com/equation_images/1%20-%200.0105%20=%200.9895\" alt=\"{\" data-equation-content=\"1 - 0.0105 = 0.9895\"\u003e, or \u003cstrong\u003e\u003cem\u003e98.95%\u003c/em\u003e\u003c/strong\u003e!  \u003c/p\u003e\n\n\u003cp\u003eObviously, this analogy is a bit of an oversimplification -- we're assuming that each prediction is independent, which is unlikely in the real world since there's likely some overlap between the things each person is using to make their prediction. We also haven't calculated the accuracy percentages for the cases where they disagree. However, the main point of this example is that when we combine predictions, we get better overall results. \u003c/p\u003e\n\n\u003ch2\u003eResiliency to variance\u003c/h2\u003e\n\n\u003cp\u003eEnsemble methods are analogous to \"Wisdom of the crowd\". This phrase refers to the phenomenon that the average estimate of all predictions typically outperforms any single prediction by a statistically significant margin -- often, quite a large one.  A Finance Professor named Jack Treynor once demonstrated this with the classic jar full of jellybeans. Professor Treynor asked all 850 of his students to guess the number of jellybeans in the jar. When he averaged the guesses, he found that of all the guesses in the class, only one student had guessed a better estimate than the group average. \u003c/p\u003e\n\n\u003cp\u003eThink back to what you've learned about sampling, inferential statistics, and the Central Limit theorem. The same magic is at work here. Estimators are rarely perfect. When Professor Treynor asked each student to provide an estimate of the number of jellybeans in the jar, he found that the estimates were normally distributed. This is where \"Wisdom of the crowd\" kicks in because we can expect the number of people who underestimate the number of jellybeans in the jar to be roughly equal to the number of people who overestimate the number of jellybeans. So we can safely assume the extra variance above and below the average essentially cancel each other out, leaving our average close to the ground truth value! \u003c/p\u003e\n\n\u003cp\u003eConsider the top-right example in this graphic that visually demonstrates high variance:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bias-and-variance.png\" alt=\"four targets showing low and high variance on the x-axis and lot and high bias on the y-axis\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eMost points miss the bullseye, but they are just as likely to miss in any direction. If we averaged all of these points, we would be extremely close to the bullseye! This is a great analogy for how ensemble methods work so well -- we know that no model is likely to make perfect estimates, so we have many of them make predictions, and average them, knowing that the overestimates and the underestimates will likely cancel out to be very close to the ground truth. The idea that the overestimates and underestimates will (at least partially) cancel each other out is sometimes referred to as \u003cstrong\u003e\u003cem\u003esmoothing\u003c/em\u003e\u003c/strong\u003e.  \u003c/p\u003e\n\n\u003ch3\u003eWhich models are used in ensembles?\u003c/h3\u003e\n\n\u003cp\u003eFor this section, we'll be focusing exclusively on tree-based ensemble methods, such as \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient boosted trees\u003c/em\u003e\u003c/strong\u003e. However, we can technically use any models in an ensemble! It's not uncommon to see \u003cstrong\u003e\u003cem\u003eModel stacking\u003c/em\u003e\u003c/strong\u003e, also called \u003cstrong\u003e\u003cem\u003eMeta-ensembling\u003c/em\u003e\u003c/strong\u003e, where multiple different models are stacked, and their predictions are aggregated. In this case, the more different the models are, the better! This is because the more different the models are, the more likely they have the potential to pick up on different characteristics of the data. It's not uncommon to see ensembles consisting of multiple logistic regressions, Naive Bayes classifiers, Tree-based models (including ensembles such as random forests), and even deep neural networks!  \u003c/p\u003e\n\n\u003cp\u003eFor a much more in-depth explanation of what model stacking looks like and why it is effective, take a look at this great \u003ca href=\"http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\"\u003earticle from Kaggle's blog, No Free Hunch!\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eBootstrap aggregation\u003c/h2\u003e\n\n\u003cp\u003eThe main concept that makes ensembling possible is \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e. Bootstrap aggregation is itself a combination of two ideas -- bootstrap resampling and aggregation. You're already familiar with bootstrap resampling from our section on the Central Limit theorem. It refers to the subsets of your dataset by sampling with replacement, much as we did to calculate our sample means when working with the Central Limit theorem. Aggregation is exactly as it sounds -- the practice of combining all the different estimates to arrive at a single estimate -- although the specifics for how we combine them are up to us. A common approach is to treat each classifier in the ensemble's prediction as a \"vote\" and let our overall prediction be the majority vote.  It's also common to see ensembles that take the arithmetic mean of all predictions, or compute a weighted average. \u003c/p\u003e\n\n\u003cp\u003eThe process for training an ensemble through bootstrap aggregation is as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eGrab a sizable sample from your dataset, with replacement \u003c/li\u003e\n\u003cli\u003eTrain a classifier on this sample\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eRepeat until all classifiers have been trained on their own sample from the dataset\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eWhen making a prediction, have each classifier in the ensemble make a prediction \u003c/li\u003e\n\u003cli\u003eAggregate all predictions from all classifiers into a single prediction, using the method of your choice\u003cbr\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png\" alt=\"flowchart of input sample being split into several bootstrap samples, then building several decision trees, then aggregation\"\u003e\u003c/p\u003e\n\n\u003cp\u003eDecision trees are often used because they are very sensitive to variance. On their own, this is a weakness. However, when aggregated together into an ensemble, this actually becomes a good thing!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what constitutes an \u003cstrong\u003eEnsemble\u003c/strong\u003e, and how \u003cstrong\u003eBagging\u003c/strong\u003e plays a central role in this. In the next lesson, we'll see how bagging is combined with another important technique to create one of the most effective ensemble algorithms available today -- \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e","exportId":"ensemble-methods"},{"id":456468,"title":"Random Forests","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about a powerful and popular ensemble method that makes use of decision trees -- a random forest!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how the random forest algorithm works \u003c/li\u003e\n\u003cli\u003eDescribe the subspace sampling method that makes random forests \"random\" \u003c/li\u003e\n\u003cli\u003eExplain the benefits and drawbacks of random forest models \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eUnderstanding the Random forest algorithm\u003c/h2\u003e\n\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003eRandom Forest\u003c/em\u003e\u003c/strong\u003e algorithm is a supervised learning algorithm that can be used both for classification and regression tasks. Decision trees are the cornerstone of random forests -- if you don't remember much about decision trees, now may be a good time to go back and review that section until you feel comfortable with the topic. \u003c/p\u003e\n\n\u003cp\u003ePut simply, the random forest algorithm is an ensemble of decision trees. However, you may recall that decision trees use a \u003cstrong\u003e\u003cem\u003egreedy algorithm\u003c/em\u003e\u003c/strong\u003e, meaning that given the same data, the algorithm will make a choice that maximizes information gain at every step. By itself, this presents a problem -- it doesn't matter how many trees we add to our forest if they're all the same tree! Trees trained on the same dataset will come out the exact same way every time -- there is no randomness to this algorithm. It doesn't matter if our forest has a million decision trees; if they are all exactly the same, then our performance will be no better than if we just had a single tree.\u003c/p\u003e\n\n\u003cp\u003eThink about this from a business perspective -- would you rather have a team at your disposal where everyone has exactly the same training and skills, or a team where each member has their own individual strengths and weaknesses? The second team will almost always do much better!\u003c/p\u003e\n\n\u003cp\u003eAs we learned when reading up on ensemble methods, variance is a good thing in any ensemble. So how do we create high variance among all the trees in our random forest? The answer lies in two clever techniques that the algorithm uses to make sure that each tree focuses on different things -- \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e and the \u003cstrong\u003e\u003cem\u003eSubspace Sampling Method\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2\u003eBagging\u003c/h2\u003e\n\n\u003cp\u003eThe first way to encourage differences among the trees in our forest is to train them on different samples of data. Although more data is generally better, if we gave every tree the entire dataset, we would end up with each tree being exactly the same. Because of this, we instead use \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e (AKA \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e) to obtain a portion of our data by sampling with replacement. For each tree, we sample two-thirds of our training data with replacement -- this is the data that will be used to build our tree. The remaining data is used as an internal test set to test each tree -- this remaining one-third is referred to as \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Data\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eOOB\u003c/em\u003e\u003c/strong\u003e. For each new tree created, the algorithm then uses the remaining one-third of data that wasn't sampled to calculate the \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Error\u003c/em\u003e\u003c/strong\u003e, in order to get a running, unbiased estimate of overall tree performance for each tree in the forest. \u003c/p\u003e\n\n\u003cp\u003eTraining each tree on its own individual \"bag\" of data is a great start for getting us some variability between the decision trees in our forest. However, with just bagging, all the trees are still focusing on all the same predictors. This allows for a potential weakness to affect all the trees at once -- if a predictor that usually provides strong signal provides bad information for a given observation, then it's likely that all the trees will fall for this false signal and make the wrong prediction. This is where the second major part of the Random forest algorithm comes in!\u003c/p\u003e\n\n\u003ch2\u003eSubspace sampling method\u003c/h2\u003e\n\n\u003cp\u003eAfter bagging the data, the random forest uses the \u003cstrong\u003e\u003cem\u003eSubspace sampling method\u003c/em\u003e\u003c/strong\u003e to further increase variability between the trees. Although it has a fancy mathematical-sounding name, all this method does is randomly select a subset of features to use as predictors for each node when training a decision tree, instead of using all predictors available at each node. \u003c/p\u003e\n\n\u003cp\u003eLet's pretend we're training our random forest on a dataset with 3000 rows and 10 columns. For each given tree, we would randomly \"bag\" 2000 rows with replacement. Next, we perform a subspace sample by randomly selecting a number of predictors at each node of a decision tree. Exactly how many predictors are used is a tunable parameter for this algorithm -- for simplicity's sake, let's assume we pick 6 predictors in this example. \u003c/p\u003e\n\n\u003cp\u003eThis brings us to the following pseudocode so far:\u003c/p\u003e\n\n\u003cp\u003eFor each tree in the dataset:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eBag 2/3 of the overall data -- in our example, 2000 rows \u003c/li\u003e\n\u003cli\u003eRandomly select a set number of features to use for training each node within this -- in this example, 6 features\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eTrain the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDrop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-random-forests/master/images/new_rf-diagram.png\" width=\"750\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eResiliency to overfitting\u003c/h3\u003e\n\n\u003cp\u003eOnce we've created our target number of trees, we'll be left with a random forest filled with a diverse set of decision trees that are trained on different sets of data, and also look at different subsets of features to make predictions. This amount of diversity among the trees in our forest will make for a model that is extremely resilient to noisy data, thus reducing the chance of overfitting.\u003c/p\u003e\n\n\u003cp\u003eTo understand why this is the case, let's put it in practical terms. Let's assume that of the 10 columns that we mentioned in our hypothetical dataset, column 2 correlates heavily with our target. However, there is still some noise in this dataset, and this column doesn't correlate \u003cem\u003eperfectly\u003c/em\u003e with our target -- there will be times where it suggests one class or another, but this isn't actually the case -- let's call these rows \"false signals\". In the case of a single decision tree, or even a forest where all trees focus on all the same predictors, we can expect to get the model to almost always get these false signal examples wrong. Why? Because the model will have learned to treat column 2 as a \"star player\" of sorts. When column 2 provides a false signal, our model will fall for it and get the prediction wrong. \u003c/p\u003e\n\n\u003cp\u003eNow, let's assume that we have a random forest complete with subspace sampling. If we randomly use 6 out of 10 predictors when creating each node of each tree, then this means that ~40% of the nodes of the trees in our forest won't even know column 2 exists! In the cases where column 2 provides a \"false signal\", the nodes of trees that use column 2 will likely make an incorrect prediction -- but that only matters to the ~60% that look at column 2. Our forest still contains another 40% of nodes within trees that are essentially \"immune\" to the false signal in column 2, because they don't use that predictor. In this way, the \"wisdom of the crowd\" buffers the performance of every constituent in that crowd. Although for any given example, some trees may draw the wrong conclusion from a particular predictor, the odds that \u003cem\u003eevery tree\u003c/em\u003e makes the same mistake because they looked at the same predictor is infinitesimally small!\u003c/p\u003e\n\n\u003ch3\u003eMaking predictions with random forests\u003c/h3\u003e\n\n\u003cp\u003eOnce we have trained all the trees in our random forest, we can effectively use it to make predictions! When given data to make predictions on, the algorithm provides only the appropriate features to each tree in the forest, gets that tree's individual prediction, and then aggregates all predictions together to determine the overall prediction that the algorithm will make for said data. In essence, each tree \"votes\" for the prediction that the forest will make, with the majority winning. \u003c/p\u003e\n\n\u003ch2\u003eBenefits and drawbacks\u003c/h2\u003e\n\n\u003cp\u003eLike any algorithm, the random forest comes with its own benefits and drawbacks. \u003c/p\u003e\n\n\u003ch3\u003eBenefits\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStrong performance\u003c/em\u003e\u003c/strong\u003e: The random forest algorithm usually has very strong performance on most problems, when compared with other classification algorithms. Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretability\u003c/em\u003e\u003c/strong\u003e:  Conveniently, since each tree in the random forest is a \u003cstrong\u003e\u003cem\u003eGlass-Box Model\u003c/em\u003e\u003c/strong\u003e (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well! You'll demonstrate this yourself in the upcoming lab, by inspecting feature importances for both individual trees and the entire random forest. \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eDrawbacks\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eComputational complexity\u003c/em\u003e\u003c/strong\u003e: Like any ensemble method, training multiple models means paying the computational cost of training each model. On large datasets, the runtime can be quite slow compared to other algorithms.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eMemory usage\u003c/em\u003e\u003c/strong\u003e: Another side effect of the ensembled nature of this algorithm, having multiple models means storing each in memory. Random forests tend to have a larger memory footprint than other models. Whereas a parametric model like a logistic regression just needs to store each of the coefficients, a random forest has to remember every aspect of every tree! It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens or even hundreds of MB. For data scientists working on modern computers, this isn't typically a problem -- however, there are special cases where the memory footprint can make this an untenable choice -- for instance, an app on a smartphone that uses machine learning may not be able to afford to spend that much disk space on a random forest model!\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e(Optional) Random forests White paper\u003c/h2\u003e\n\n\u003cp\u003eThis algorithm was not invented all at once -- there were several iterations by different researchers that built upon each previous idea. However, the version used today is the one created by Leo Breiman and Adele Cutler, who also own the trademark for the name \"random forest\". \u003c/p\u003e\n\n\u003cp\u003eAlthough not strictly necessary for understanding how to use random forests, we highly recommend taking a look at the following resources from Breiman and Cutler if you're interested in really digging into how random forests work:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf\"\u003eRandom forests paper\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm\"\u003eRandom forests website\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about a random forest, which is a powerful and popular ensemble method that uses decision trees!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-random-forests\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-random-forests\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"random-forests"},{"id":456472,"title":"Tree Ensembles and Random Forests - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tree-ensembles-random-forests-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll create some popular tree ensemble models such as a bag of trees and random forest to predict a person's salary based on information about them. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eTrain a random forest model using \u003ccode\u003escikit-learn\u003c/code\u003e\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eAccess, visualize, and interpret feature importances from an ensemble model \u003c/li\u003e\n\u003c/ul\u003e","exportId":"gcbdd65aa18e5b390121d84ab623eec90"},{"id":456475,"title":"GridSearchCV - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gridsearchcv-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll explore how to use scikit-learn's \u003ccode\u003eGridSearchCV\u003c/code\u003e class to exhaustively search through every combination of hyperparameters until we find optimal values for a given model.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDesign a parameter grid for use with scikit-learn's GridSearchCV \u003c/li\u003e\n\u003cli\u003eUse GridSearchCV to increase model performance through parameter tuning \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g24774d5f3764ee7c2cc27699b8e85222"},{"id":456490,"title":"Gradient Boosting and Weak Learners","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-boosting-and-weak-learners\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll explore one of the most powerful ensemble methods around -- gradient boosting!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare and contrast weak and strong learners and explain the role of weak learners in boosting algorithms\u003c/li\u003e\n\u003cli\u003eDescribe the process of boosting in Adaboost and Gradient Boosting\u003c/li\u003e\n\u003cli\u003eExplain the concept of a learning rate and the role it plays in gradient boosting algorithms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWeak learners and boosting\u003c/h2\u003e\n\u003cp\u003eThe first ensemble technique we learned about was \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which refers to training different models independently on different subsets of data by sampling with replacement. The goal of bagging is to create variability in the ensemble of models. The next ensemble technique we'll learn about is \u003cstrong\u003e\u003cem\u003eBoosting\u003c/em\u003e\u003c/strong\u003e. This technique is at the heart of some very powerful, top-of-class ensemble methods currently used in machine learning, such as \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn order to understand boosting, let's first examine the cornerstone of boosting algorithms -- \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWeak learners\u003c/h3\u003e\n\u003cp\u003eAll the models we've learned so far are \u003cstrong\u003e\u003cem\u003eStrong Learners\u003c/em\u003e\u003c/strong\u003e -- models with the goal of doing as well as possible on the classification or regression task they are given. The term \u003cstrong\u003e\u003cem\u003eWeak Learner\u003c/em\u003e\u003c/strong\u003e refers to simple models that do only slightly better than random chance. Boosting algorithms start with a single weak learner (tree methods are overwhelmingly used here), but technically, any model will do. Boosting works as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTrain a single weak learner\u003c/li\u003e\n\u003cli\u003eFigure out which examples the weak learner got wrong\u003c/li\u003e\n\u003cli\u003eBuild another weak learner that focuses on the areas the first weak learner got wrong\u003c/li\u003e\n\u003cli\u003eContinue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn this way, each new weak learner is specifically tuned to focus on the weak points of the previous weak learner(s). The more often an example is missed, the more likely it is that the next weak learner will be the one that can classify that example correctly. In this way, all the weak learners work together to make up a single strong learner.\u003c/p\u003e\n\u003ch2\u003eBoosting and random forests\u003c/h2\u003e\n\u003ch3\u003eSimilarities\u003c/h3\u003e\n\u003cp\u003eBoosting algorithms share some similarities with random forests, as well as some notable differences. Like random forests, boosting algorithms are an ensemble of many different models with high inter-group diversity. Boosting algorithms also aggregate the predictions of each constituent model into an overall prediction. Both algorithms also make use of tree models (although this isn't strictly required, in the case of boosting).\u003c/p\u003e\n\u003ch3\u003eDifferences\u003c/h3\u003e\n\u003ch4\u003e1: Independent vs. iterative\u003c/h4\u003e\n\u003cp\u003eThe difference is in the approach to training the trees. Whereas a random forest trains each tree independently and at the same time, boosting trains each tree iteratively. In a random forest model, how well or poorly a given tree does has no effect on any of the other trees since they are all trained at the same time. Boosting, on the other hand, trains trees one at a time, identifies the weak points for those trees, and then purposefully creates the next round of trees in such a way as to specialize in those weak points.\u003c/p\u003e\n\u003ch4\u003e2: Weak vs. strong\u003c/h4\u003e\n\u003cp\u003eAnother major difference between random forests and boosting algorithms is the overall size of the trees. In a random forest, each tree is a strong learner -- they would do just fine as a decision tree on their own. In boosting algorithms, trees are artificially limited to a very shallow depth (usually only 1 split), to ensure that each model is only slightly better than random chance. For this reason, boosting algorithms are also highly resilient against noisy data and overfitting. Since the individual weak learners are too simple to overfit, it is very hard to combine them in such a way as to overfit the training data as a whole -- especially when they focus on different things, due to the iterative nature of the algorithm.\u003c/p\u003e\n\u003ch4\u003e3: Aggregate predictions\u003c/h4\u003e\n\u003cp\u003eThe final major difference we'll talk about between the two is the way predictions are aggregated. Whereas in a random forest, each tree simply votes for the final result, boosting algorithms usually employ a system of weights to determine how important the input for each tree is. Since we know how well each weak learner performs on the dataset by calculating its performance at each step, we can see which weak learners do better on hard tasks. Think of it like this -- harder problems deserve more weight. If there are many learners in the overall ensemble that can get the same questions right, then that tree isn't super important -- other trees already provide the same value that it does. This tree will have its overall weight reduced. As more and more trees get a hard problem wrong, the \"reward\" for a tree getting that hard problem correct goes higher and higher. This \"reward\" is actually just a higher weight when calculating the overall vote. Intuitively, this makes sense -- trees that can do what few other trees can do are the ones that we should probably listen to more than others, as they are the most likely to get hard examples correct. Since other trees tend to get this wrong, we can expect to see a general split of about 50/50 among the trees that do not \"specialize\" in the hard problems. Since our \"specialized\" tree has more weight, its correct vote will carry more weight than the combined votes of the half of the \"unspecialized\" trees that get it wrong. It is worth noting that the \"specialized\" trees will often do quite poorly on the examples that are easy to predict. However, since these examples are easier, we can expect a strong majority of the trees in our ensemble to get it right, meaning that the combined, collective weight of their agreement will be enough to overrule the trees with higher weights that get it wrong.\u003c/p\u003e\n\u003ch2\u003eUnderstanding Adaboost and Gradient boosting\u003c/h2\u003e\n\u003cp\u003eThere are two main algorithms that come to mind when Data Scientists talk about boosting: \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e (short for Adaptive Boosting), and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e. Both are generally very effective, but they use different methods to achieve their results.\u003c/p\u003e\n\u003ch3\u003eAdaboost\u003c/h3\u003e\n\u003cp\u003eAdaboost was the first boosting algorithm invented. Although there have been marked improvements made to this algorithm, Adaboost still tends to be quite an effective algorithm! More importantly, it's a good starting place for understanding how boosting algorithms actually work.\u003c/p\u003e\n\u003cp\u003eIn Adaboost, each learner is trained on a subsample of the dataset, much like we saw with \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e. Initially, the bag is randomly sampled with replacement. However, each data point in the dataset has a weight assigned. As learners correctly classify an example, that example's weight is reduced. Conversely, when learners get an example wrong, the weight for that sample increases. In each iteration, these weights act as the probability that an item will be sampled into the \"bag\" which will be used to train the next weak learner. As the number of learners grows, you can imagine that the examples that are easy to get correct will become less and less prevalent in the samples used to train each new learner. This is a good thing -- if our ensemble already contains multiple learners that can correctly classify that example, then we don't need more that can do this. Instead, the \"bags\" of data will contain multiple instances of the hard examples, thereby increasing the likelihood that the learner will create a split that focuses on getting the hard example correct.\u003c/p\u003e\n\u003cp\u003eThe following diagram demonstrates how the weights change for each example as classifiers get them right and wrong.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_adaboost.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003ePay attention to the colors of the pluses and minuses -- pluses are meant to be in the blue section, and minuses are meant to be in the red. The decision boundary of the tree can be interpreted as the line drawn between the red and blue sections. As you can see above, examples that were misclassified are larger in the next iteration, while examples that were classified correctly are smaller. As we combine the decision boundaries of each new classifier, we end up with a classifier that correctly classifies all of the examples!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKey Takeaway:\u003c/em\u003e\u003c/strong\u003e Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive learner.\u003c/p\u003e\n\u003ch3\u003eGradient boosting\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e are a more advanced boosting algorithm that makes use of \u003cstrong\u003e\u003cem\u003eGradient Descent.\u003c/em\u003e\u003c/strong\u003e Much like Adaboost, gradient boosting starts with a weak learner that makes predictions on the dataset. The algorithm then checks this learner's performance, identifying examples that it got right and wrong. However, this is where the gradient boosting algorithm diverges from Adaboost's methodology. The model then calculates the \u003cstrong\u003e\u003cem\u003eResiduals\u003c/em\u003e\u003c/strong\u003e for each data point, to determine how far off the mark each prediction was. The model then combines these residuals with a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e to calculate the overall loss. There are many loss functions that are used -- the thing that matters most is that the loss function is \u003cstrong\u003e\u003cem\u003edifferentiable\u003c/em\u003e\u003c/strong\u003e so that we can use calculus to compute the gradient for the loss, given the inputs of the model. We then use the gradients and the loss as predictors to train the next tree against! In this way, we can use \u003cstrong\u003e\u003cem\u003eGradient Descent\u003c/em\u003e\u003c/strong\u003e to minimize the overall loss.\u003c/p\u003e\n\u003cp\u003eSince the loss is most heavily inflated by examples where the model was wrong, gradient descent will push the algorithm towards creating a new learner that will focus on these harder examples. If the next tree gets these right, then the loss goes down! In this way, gradient descent allows us to continually train and improve on the loss for each model to improve the overall performance of the ensemble as a whole by focusing on the \"hard\" examples that cause the loss to be high.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_gradient-boosting.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eLearning rates\u003c/h3\u003e\n\u003cp\u003eOften, we want to artificially limit the \"step size\" we take in gradient descent. Small, controlled changes in the parameters we're optimizing with gradient descent will mean that the overall process is slower, but the parameters are more likely to converge to their optimal values. The learning rate for your model is a small scalar meant to artificially reduce the step size in gradient descent. Learning rate is a tunable parameter for your model that you can set -- large learning rates get closer to the optimal values more quickly, but have trouble landing exactly at the optimal values because the step size is too big for the small distances it needs to travel when it gets close. Conversely, small learning rates means the model will take a longer time to get to the optimal parameters, but when it does get there, it will be extremely close to the optimal values, thereby providing the best overall performance for the model.\u003c/p\u003e\n\u003cp\u003eYou'll often see learning rates denoted by the symbol, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e -- this is the greek letter, \u003cstrong\u003e\u003cem\u003egamma\u003c/em\u003e\u003c/strong\u003e. Don't worry if you're still hazy on the concept of gradient descent -- we'll explore it in much more detail when we start studying deep learning!\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library contains some excellent implementations of Adaboost, as well as several different types of gradient boosting classifiers. These classifiers can be found in the \u003ccode\u003eensemble\u003c/code\u003e module, which you will make use of in the upcoming lesson.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e, and how they are used in various \u003cstrong\u003e\u003cem\u003eGradient Boosting\u003c/em\u003e\u003c/strong\u003e algorithms. We also learned about two specific algorithms -- \u003cstrong\u003e\u003cem\u003eAdaBoost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e, and we compared how they are similar and how they are different!\u003c/p\u003e","exportId":"gradient-boosting-and-weak-learners"},{"id":456494,"title":"Gradient Boosting - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-boosting-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll learn how to use both Adaboost and Gradient Boosting classifiers from scikit-learn!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse AdaBoost to make predictions on a dataset \u003c/li\u003e\n\u003cli\u003eUse Gradient Boosting to make predictions on a dataset \u003c/li\u003e\n\u003c/ul\u003e","exportId":"g59ccab92e9380e16e1a668e43a6307ec"},{"id":456499,"title":"XGBoost","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you are familiar with gradient boosting, you'll learn about the top gradient boosting algorithm currently in use -- XGBoost!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare XGBoost to other boosting algorithms \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\n\u003cp\u003eGradient boosting is one of the most powerful concepts in machine learning right now. As you've seen, the term \u003cem\u003egradient boosting\u003c/em\u003e refers to a class of algorithms, rather than any single one. The version with the highest performance right now is \u003cstrong\u003e\u003cem\u003eXGBoost\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eeXtreme Gradient Boosting\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible. There are many under-the-hood optimizations that allow XGBoost to train more quickly than any other library implementations of gradient boosting algorithms. For instance, XGBoost is configured in such a way that it parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases, such as distributing training across a cluster of computers, which is often a technique used to speed up computation. The algorithm even automatically handles missing values!\u003c/p\u003e\n\n\u003ch2\u003eInstalling \u003ccode\u003eXGBoost\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is an independent library that provides implementations in C++, Python, and other languages. Luckily, the open-source community has had the good sense to make the Python API for \u003ccode\u003eXGBoost\u003c/code\u003e mirror that of \u003ccode\u003esklearn\u003c/code\u003e, so using \u003ccode\u003eXGBoost\u003c/code\u003e feels no different than using any other supervised learning algorithm from \u003ccode\u003esklearn\u003c/code\u003e. The only downside is that it does not come packaged with \u003ccode\u003esklearn\u003c/code\u003e, so we must install it ourselves. \u003cstrong\u003econda\u003c/strong\u003e makes this quite easy. \u003c/p\u003e\n\n\u003cp\u003eAll you need to do is run the command \u003ccode\u003econda install py-xgboost\u003c/code\u003e in your terminal, and \u003ccode\u003econda\u003c/code\u003e will take care of the rest!\u003c/p\u003e\n\n\u003ch2\u003eUse cases\u003c/h2\u003e\n\n\u003cp\u003eXGBoost has risen to prominence by being the go-to algorithm for winning competitions on \u003ca href=\"https://www.kaggle.com/\"\u003eKaggle\u003c/a\u003e, a competitive data science platform. It is so common to see XGBoost cited as an algorithm used by the winners of Kaggle competitions that it has become a bit of a running joke in the community. \u003ca href=\"https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions\"\u003eThis page\u003c/a\u003e contains an (incomplete) list of all the recent competitions with place winners that used XGBoost for their solution!\u003c/p\u003e\n\n\u003cp\u003eXGBoost is a great choice for classification tasks. It provides best-in-class performance compared to other classification algorithms (with the exception of Deep Learning, which we'll learn more about soon).\u003c/p\u003e\n\n\u003ch2\u003eTakeaways\u003c/h2\u003e\n\n\u003cp\u003eWhen approaching a supervised learning problem, you should always use multiple algorithms, and compare the performances of the various models. There will always be use cases where some classes of models tend to outperform others. However, there are some models that generally outperform all the others -- XGBoost is at the top of this list! Make sure that this is an algorithm you're familiar with, as there are many situations where you'll find it quite useful!\u003c/p\u003e\n\n\u003cp\u003eYou can find the full documentation for XGBoost \u003ca href=\"https://xgboost.readthedocs.io/en/latest/\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what XGBoost is, and why it is so powerful and useful to Data Scientists!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-xgboost\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-xgboost\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"xgboost"},{"id":456504,"title":"XGBoost - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":null,"graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"must_view","completed":true,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003c/header\u003e\n\u003cp\u003eComplete this lab on your local computer.\u003c/p\u003e","exportId":"ge3ed4d45a85151cc7115a93f81746615"},{"id":456509,"title":"Quiz: Ensemble Methods","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g75d013b6043be830c29dacb4eb7a48ce","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":"min_score","requiredPoints":3.0,"completed":false,"content":"","exportId":"g01b43860f608fdfa4d009588e4178c7e"},{"id":456544,"title":"Ensembles - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":"must_view","completed":true,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple independent estimates are consistently more accurate than any single estimate, so ensemble techniques are a powerful way for improving the quality of your models\u003c/li\u003e\n\u003cli\u003eSometimes you'll use model stacking or meta-ensembles where you use a combination of different types of models for your ensemble\u003c/li\u003e\n\u003cli\u003eIt's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\u003c/li\u003e\n\u003cli\u003eBagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\u003c/li\u003e\n\u003cli\u003eBootstrap resampling uses multiple smaller samples from the test dataset to create independent estimates, and aggregate these estimates to make predictions\u003c/li\u003e\n\u003cli\u003eA random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling method to create variance among the trees\u003c/li\u003e\n\u003cli\u003eWith a random forest, for each tree, we sample two-thirds of the training data and the remaining third is used to calculate the out-of-bag error\u003c/li\u003e\n\u003cli\u003eIn addition, the Subspace Sampling method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eGridsearchCV\u003c/code\u003e is an exhaustive search technique for finding optimal combinations of hyperparameters\u003c/li\u003e\n\u003cli\u003eBoosting leverages an ensemble of weak learners (weak models) to create a strong combined model\u003c/li\u003e\n\u003cli\u003eBoosting (when compared to random forests) is an iterative rather than independent process, using each iteration to strengthen the weaknesses of the previous iterations\u003c/li\u003e\n\u003cli\u003eTwo of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\u003c/li\u003e\n\u003cli\u003eAdaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\u003c/li\u003e\n\u003cli\u003eGradient Boosting is a more advanced boosting algorithm that makes use of Gradient Descent\u003c/li\u003e\n\u003cli\u003eXGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible\u003c/li\u003e\n\u003c/ul\u003e","exportId":"ensembles-recap"}]},{"id":46992,"name":"üèÜ Milestones","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gbcbec1317916c833fa0f56e4889aaaaa","items":[{"id":456558,"title":"CODE ASSESSMENTS","type":"ContextModuleSubHeader","indent":0,"locked":false},{"id":456563,"title":"Checkpoints","type":"ContextModuleSubHeader","indent":1,"locked":false},{"id":456568,"title":"Object-Oriented Programming Checkpoint","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g620f162c9e68c4adb9e507b7e852183f"},{"id":456572,"title":"Machine Learning Fundamentals Checkpoint","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g75744bd363ff7dae6e80149164e7db83"},{"id":456577,"title":"Logistic Regression Checkpoint","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g7f84091a5e76258e0ccb89ca7bd645d5"},{"id":456582,"title":"Decision Trees Checkpoint","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g39a0387412e39beed4be8f195a098689"},{"id":456587,"title":"Code Challenge","type":"ContextModuleSubHeader","indent":1,"locked":false},{"id":456590,"title":"Phase 3 Code Challenge","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":16.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"g88357763d5f22d659df758ff649797ea"},{"id":456594,"title":"PHASE 3 PROJECT","type":"ContextModuleSubHeader","indent":0,"locked":false},{"id":456597,"title":"Project Overview","type":"ContextModuleSubHeader","indent":1,"locked":false},{"id":456606,"title":"Phase 3 Project Description","type":"WikiPage","indent":1,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-3-project-v2-3\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project-v2-3\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project-v2-3/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eCongratulations! You've made it through another \u003cem\u003eintense\u003c/em\u003e module, and now you're ready to show off your newfound Machine Learning skills!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-3-project-v2-3/main/images/smart.gif\" alt=\"awesome\"\u003e\u003c/p\u003e\n\u003cp\u003eAll that remains in Phase 3 is to put your new skills to use with another large project!\u003c/p\u003e\n\u003cp\u003eIn this project description, we will cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProject Overview\u003c/li\u003e\n\u003cli\u003eDeliverables\u003c/li\u003e\n\u003cli\u003eGrading\u003c/li\u003e\n\u003cli\u003eGetting Started\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eFor this project, you will engage in the full data science process from start to finish, solving a \u003cstrong\u003eclassification\u003c/strong\u003e problem using a \u003cstrong\u003edataset of your choice\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eBusiness Problem and Data\u003c/h3\u003e\n\u003cp\u003eSimilar to the Phase 2 project, it is up to you to define a stakeholder and business problem. Unlike the Phase 2 project, you are also responsible for choosing a dataset.\u003c/p\u003e\n\u003cp\u003eFor complete details, see \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset\"\u003ePhase 3 Project - Choosing a Dataset\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\u003ch4\u003eClassification\u003c/h4\u003e\n\u003cp\u003eRecall the distinction between \u003cem\u003eclassification\u003c/em\u003e and \u003cem\u003eregression\u003c/em\u003e models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClassification is used when the target variable is a \u003cem\u003ecategory\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eRegression is used when the target variable is a \u003cem\u003enumeric value\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(Categorical data may be represented in the data as numbers, e.g. 0 and 1, but they are not truly numeric values. If you're unsure, ask yourself \"is a target value of 1 \u003cem\u003eone more than\u003c/em\u003e a target value of 0\"; if it is one more, that is a regression target, if not, that is a classification target.)\u003c/p\u003e\n\u003cp\u003eYou already practiced performing a regression analysis in Phase 2, and you will have additional opportunities to work on regression problems in later phases, but \u003cstrong\u003efor this project, you must be modeling a classification problem\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003eFindings and Recommendations\u003c/h4\u003e\n\u003cp\u003eIn the previous two projects, the framing was primarily \u003cem\u003edescriptive\u003c/em\u003e and \u003cem\u003einferential\u003c/em\u003e, meaning that you were trying to understand the distributions of variables and the relationship between them. For this project you can still use these techniques, but make sure you are also using a \u003cstrong\u003e\u003cem\u003epredictive\u003c/em\u003e\u003c/strong\u003e approach.\u003c/p\u003e\n\u003cp\u003eA predictive \u003cem\u003efinding\u003c/em\u003e might include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHow well your model is able to predict the target\u003c/li\u003e\n\u003cli\u003eWhat features are most important to your model\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA predictive \u003cem\u003erecommendation\u003c/em\u003e might include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe contexts/situations where the predictions made by your model would and would not be useful for your stakeholder and business problem\u003c/li\u003e\n\u003cli\u003eSuggestions for how the business might modify certain input variables to achieve certain target results\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eIterative Approach to Modeling\u003c/h4\u003e\n\u003cp\u003eThe expectations from the Phase 2 project still stand:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eYou should demonstrate an iterative approach to modeling. This means that you must build multiple models. Begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. After you finish refining your models, you should provide 1-3 paragraphs in the notebook discussing your final model.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWith the additional techniques you have learned in Phase 3, be sure to explore:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eModel features and preprocessing approaches\u003c/li\u003e\n\u003cli\u003eDifferent kinds of models (logistic regression, k-nearest neighbors, decision trees, etc.)\u003c/li\u003e\n\u003cli\u003eDifferent model hyperparameters\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAt minimum you must build three models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple, interpretable baseline model (logistic regression or single decision tree)\u003c/li\u003e\n\u003cli\u003eA more-complex model (e.g. random forest)\u003c/li\u003e\n\u003cli\u003eA version of either the simple model or more-complex model with tuned hyperparameters\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eClassification Metrics\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eYou must choose appropriate classification metrics and use them to evaluate your models.\u003c/strong\u003e Choosing the right classification metrics is a key data science skill, and should be informed by data exploration and the business problem itself. You must then use this metric to evaluate your model performance using both training and testing data.\u003c/p\u003e\n\u003ch2\u003eDeliverables\u003c/h2\u003e\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe deliverables requirements are almost the same as in the Phase 1 and Phase 2 projects. \u003cstrong\u003e\u003cem\u003eThe only difference between the Phase 2 and Phase 3 project checklist is that the \"Regression Results\" element has been replaced with an \"Evaluation\" element.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e\n\u003cp\u003eRecall that the non-technical presentation is a slide deck presenting your analysis to \u003cstrong\u003e\u003cem\u003ebusiness stakeholders\u003c/em\u003e\u003c/strong\u003e, and should be presented live as well as submitted in PDF form on Canvas.\u003c/p\u003e\n\u003cp\u003eWe recommend that you follow this structure, although the slide titles should be specific to your project:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBeginning\n\u003cul\u003e\n\u003cli\u003eOverview\u003c/li\u003e\n\u003cli\u003eBusiness and Data Understanding\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMiddle\n\u003cul\u003e\n\u003cli\u003eModeling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEnd\n\u003cul\u003e\n\u003cli\u003eRecommendations\u003c/li\u003e\n\u003cli\u003eNext Steps\u003c/li\u003e\n\u003cli\u003eThank you\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMake sure that your discussion of classification modeling is geared towards a non-technical audience! Assume that their prior knowledge of machine learning is minimal. You don't need to explain the details of your model implementations, but you should explain why classification is useful for the problem context. Make sure you translate any metrics or feature importances into their plain language implications.\u003c/p\u003e\n\u003cp\u003eThe graded elements for the non-technical presentation are the same as in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-1-project-v2-3#deliverables\"\u003ePhase 1\u003c/a\u003e and Phase 2.\u003c/p\u003e\n\u003ch3\u003eJupyter Notebook\u003c/h3\u003e\n\u003cp\u003eRecall that the Jupyter Notebook is a notebook that uses Python and Markdown to present your analysis to a \u003cstrong\u003e\u003cem\u003edata science audience\u003c/em\u003e\u003c/strong\u003e. You will submit the notebook in PDF format on Canvas as well as in \u003ccode\u003e.ipynb\u003c/code\u003e format in your GitHub repository.\u003c/p\u003e\n\u003cp\u003eThe graded elements for the Jupyter Notebook are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBusiness Understanding\u003c/li\u003e\n\u003cli\u003eData Understanding\u003c/li\u003e\n\u003cli\u003eData Preparation\u003c/li\u003e\n\u003cli\u003eModeling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eCode Quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eGitHub Repository\u003c/h3\u003e\n\u003cp\u003eRecall that the GitHub repository is the cloud-hosted directory containing all of your project files as well as their version history.\u003c/p\u003e\n\u003cp\u003eThe requirements are the same as in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-1-project-v2-3#github-repository\"\u003ePhase 1\u003c/a\u003e and Phase 2, except for the required sections in the \u003ccode\u003eREADME.md\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor this project, the \u003ccode\u003eREADME.md\u003c/code\u003e file should contain:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOverview\u003c/li\u003e\n\u003cli\u003eBusiness and Data Understanding\n\u003cul\u003e\n\u003cli\u003eExplain your stakeholder audience and dataset choice here\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eModeling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eConclusion\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eJust like in Phase 1 and 2, the \u003ccode\u003eREADME.md\u003c/code\u003e file should be the bridge between your non technical presentation and the Jupyter Notebook. It should not contain the code used to develop your analysis, but should provide a more in-depth explanation of your methodology and analysis than what is described in your presentation slides.\u003c/p\u003e\n\u003ch2\u003eGrading\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo pass this project, you must pass each project rubric objective.\u003c/em\u003e\u003c/strong\u003e The project rubric objectives for Phase 3 are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAttention to Detail\u003c/li\u003e\n\u003cli\u003eML Communication\u003c/li\u003e\n\u003cli\u003eData Preparation for Machine Learning\u003c/li\u003e\n\u003cli\u003eNonparametric and Ensemble Modeling\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eAttention to Detail\u003c/h3\u003e\n\u003cp\u003eJust like in Phase 1 and 2, this rubric objective is based on your completion of checklist items. \u003cstrong\u003e\u003cem\u003eIn Phase 3, you need to complete 80% (8 out of 10) or more of the checklist elements in order to pass the Attention to Detail objective.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE THAT THE PASSING BAR IS HIGHER IN PHASE 3 THAN IT WAS IN PHASE 2!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe standard will increase with each Phase, until you will be required to complete all elements to pass Phase 5 (Capstone).\u003c/p\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003e90% or more of the project checklist items are complete\u003c/p\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003e80% of the project checklist items are complete\u003c/p\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003e70% of the project checklist items are complete\u003c/p\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003e60% or fewer of the project checklist items are complete\u003c/p\u003e\n\u003ch3\u003eML Communication\u003c/h3\u003e\n\u003cp\u003eRecall that communication is one of the key data science \"soft skills\". In Phase 3, we are specifically focusing on ML Communication. We define ML Communication as:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCommunicate the \u003cstrong\u003eperformance\u003c/strong\u003e of and \u003cstrong\u003einsights\u003c/strong\u003e generated by machine learning models to diverse audiences via writing, live presentation, and visualization\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHigh-quality ML Communication includes rationale, results, limitations, and recommendations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRationale:\u003c/strong\u003e Explaining why you are using machine learning rather than a simpler form of data analysis\n\u003cul\u003e\n\u003cli\u003eWhat about the problem or data is suitable for this form of analysis?\u003c/li\u003e\n\u003cli\u003eFor a data science audience, this includes your reasoning for the changes you applied while iterating between models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResults:\u003c/strong\u003e Describing the classification metrics\n\u003cul\u003e\n\u003cli\u003eYou can report multiple metrics for a single model, but make sure that indicate a reason for which metrics you are using (and don't try to use all of them at once)\u003c/li\u003e\n\u003cli\u003eFor a business audience, make sure you connect any metrics to real-world implications. You do not need to get into the details of how the model works.\u003c/li\u003e\n\u003cli\u003eFor a data science audience, you don't need to explain what a metric is, but make sure you explain why you chose that particular one.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e Identifying the limitations and/or uncertainty present in your analysis\n\u003cul\u003e\n\u003cli\u003eAre there certain kinds of records where model performance is worse? If you used this model in production, what kinds of problems might that cause?\u003c/li\u003e\n\u003cli\u003eIn general, this should be more in-depth for a data science audience and more surface-level for a business audience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecommendations:\u003c/strong\u003e Interpreting the model results and limitations in the context of the business problem\n\u003cul\u003e\n\u003cli\u003eWhat should stakeholders \u003cem\u003edo\u003c/em\u003e with this information?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003eCommunicates the rationale, results, limitations, and specific recommendations generated by a classification model\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee above for an extended explanation of these terms.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003eSuccessfully communicates model metrics without any major errors\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe minimum requirement is to communicate the \u003cem\u003eresults\u003c/em\u003e, meaning at least one overall model metric for your final model. See the Approaching Objective section for an explanation of what a \"major error\" means.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003eCommunicates model metrics with at least one major error\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA major error means that some aspect of your explanation is fundamentally incorrect. For example, if you report a regression metric for a classification model, that would be a major error. Another example would be if you report the model's performance on the training data, rather than the model's performance on the test data.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003eDoes not communicate model metrics\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt is not sufficient just to display the \u003ccode\u003eclassification_report\u003c/code\u003e or confusion matrix for a given model. You need to focus on one or more specific metrics that are important for your business case.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eData Preparation for Machine Learning\u003c/h3\u003e\n\u003cp\u003eWe define this objective as:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eApplying appropriate preprocessing and feature engineering steps to tabular data in preparation for predictive modeling\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis builds on the data preparation requirement from the Phase 2 project; you still need to ensure that you have a strategy for dealing with missing and non-numeric data.\u003c/p\u003e\n\u003cp\u003eFor the Phase 3 project, make sure you also consider:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePreventing Data Leakage:\u003c/strong\u003e As you prepare data for modeling, make sure that you are correctly applying data preparation techniques so that your model's performance on test data realistically represents how it would perform on unseen data. For scikit-learn transformers specifically, \u003cstrong\u003e\u003cem\u003emake sure that you do not fit the transformer on the test data\u003c/em\u003e\u003c/strong\u003e. Instead, fit the transformer on the training data and use it to transform both the train and test data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScaling:\u003c/strong\u003e If you are using a distance-based model algorithm (e.g. kNN or logistic regression with regularization), make sure you scale your data prior to fitting the model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFeature engineering is encouraged but not required for this project.\u003c/p\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003eGoes above and beyond with data preparation, such as feature engineering or using pipelines\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRelevant examples of feature engineering will depend on your choice of dataset and business problem.\u003c/p\u003e\n\u003cp\u003ePipelines are the best-practice approach to data preparation that avoids leakage, but they can get complicated very quickly. We therefore do not recommend that you use pipelines in your initial modeling approach, but rather that you refactor to use pipelines if you have time.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003eSuccessfully prepares data for modeling, using a final holdout dataset that is transformed by (but not fitted on) transformers used to prepare training data AND scaling data when appropriate\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee the descriptions above for explanations of how to use transformers and scaling.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003ePrepares some data successfully, but has at least one major error\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA major error means that some aspect of your data preparation is fundamentally incorrect. Some examples of major errors include: (1) fitting transformers on test data, (2) not performing a train-test split, (3) not scaling data that is used in a distance-based model.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003eDoes not prepare data for modeling\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis includes projects where data is partially prepared, but the model is unable to run.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eNonparametric and Ensemble Modeling\u003c/h3\u003e\n\u003cp\u003eThis builds on the linear modeling requirement from the Phase 2 project. Your project should consider the different types of models that have been covered in the course so far and whether they are appropriate or inappropriate for the dataset and business case you are working with.\u003c/p\u003e\n\u003cp\u003eYour final model can still be a linear model (e.g. logistic regression) but you should explore at least one nonparametric model (e.g. decision tree) as well and articulate why one or the other is a better approach.\u003c/p\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003eGoes above and beyond in the modeling process, such as articulating why a given model type is best suited to the problem or correctly using scikit-learn models not covered in the curriculum\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAnother way you might go above and beyond would be to create custom Python classes, possibly inheriting from scikit-learn classes.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003eUses at least two types of scikit-learn model and tunes at least one hyperparameter in a justifiable way without any major errors\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee the \"Iterative Approach to Modeling\" section above for a more-lengthy explanation.\u003c/p\u003e\n\u003cp\u003eOnce again, ideally you would include written justifications for each model iteration, but at minimum the iterations must be \u003cem\u003ejustifiable\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eFor an explanation of \"major errors\", see the description under \"Approaching Objective\".\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003eBuilds multiple classification models with at least one major error\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA major error means that some aspect of your modeling approach is fundamentally incorrect.\u003c/p\u003e\n\u003cp\u003eOnce again, the number one major error to avoid is including the target as one of your features. If you are getting metrics that are \"too good to be true\", make sure that you removed the target (\u003ccode\u003ey\u003c/code\u003e) from your data before fitting the model.\u003c/p\u003e\n\u003cp\u003eOther examples of major errors include: using a numeric target value (since this is a classification project), not starting with a baseline model (e.g. proceeding directly to a Random Forest model), or not tuning hyperparameters in a justifiable way (e.g. reducing regularization on a model that is overfitting)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003eDoes not build multiple classification models\u003c/p\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003ePlease start by reviewing the contents of this project description. If you have any questions, please ask your instructor ASAP.\u003c/p\u003e\n\u003cp\u003eOnce you are ready to begin the project, you will need to complete the \u003cem\u003e\u003cstrong\u003e\u003ca title=\"Phase 3 Project Proposal\" href=\"quizzes/gda45aa7fe7bca1a5bf280d1c988cc36c\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5539/quizzes/34866\" data-api-returntype=\"Quiz\"\u003eProject Proposal\u003c/a\u003e\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eRecall that more information is available in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset\"\u003ePhase 3 Project - Choosing a Dataset\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo get started with project development, create a new repository on GitHub. For this project, we recommend that you do not fork the template repository, but rather that you make a new repository from scratch, starting by going to \u003ca href=\"https://github.com/new\"\u003egithub.com/new\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis project is an opportunity to expand your data science toolkit by evaluating, choosing, and working with new datasets. Spending time up front making sure you have a good dataset for a solvable problem will help avoid the major problems that can sometimes derail data science projects. You've got this!\u003c/p\u003e","exportId":"phase-3-project-description"},{"id":456610,"title":"Phase 3 Project - Choosing a Dataset","type":"WikiPage","indent":1,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-3-choosing-a-dataset\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003cp\u003eYou have the option to either choose a dataset from a curated list or propose your own dataset not on the list. The goal is to choose a dataset appropriate to the type of business problem and/or classification methods that most interests you. \u003cstrong\u003eIt is up to you to define a stakeholder and business problem appropriate to the dataset you choose.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eIf you choose a dataset from the curated list, inform your instructor which dataset you chose and jump right into the project. If you would like to propose your own dataset, run the dataset and business problem by your instructor for approval before starting your project.\u003c/p\u003e\n\n\u003ch2\u003eYour Get Hired 'Game Plan'\u003c/h2\u003e\n\n\u003cp\u003eHelp set yourself up for success by being strategic about your project/dataset choices.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eAlready know what your job search focus will be?\u003c/strong\u003e Consider choosing a dataset that relates to the companies/industries you are interested in and the types of business problems/data they navigate day to day. Doing so demonstrates your subject matter knowledge in their area, significantly elevating your relevance and value as a candidate -- we've seen this strategy WOW companies time and time again!\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStill exploring what type of role you would like to get once you graduate?\u003c/strong\u003e That's okay! Try to focus on a topic or problem that you are interested in and passionate about. Doing so will help you produce a better project overall that you enjoy creating and that you can speak about confidently and naturally.\u003c/p\u003e\n\n\u003cp\u003eComing out of Flatiron School your projects will be listed on your resume and will showcase your specific subject matter knowledge and interest/passions once you're job seeking. Help yourself put your best foot forward and make the strongest first impression possible.\u003c/p\u003e\n\n\u003cp\u003eHere are two grads who successfully did just this...\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis student was interested in working with government and public sector data, and focused specifically on traffic data and safety. They utilized the Chicago Car Crashes dataset in \u003ca href=\"https://github.com/jmarkowi/Chicago-Crashes\"\u003eone project\u003c/a\u003e* then later created a bike lane image dataset from multiple sources for their \u003ca href=\"https://github.com/jmarkowi/NYC_bike_lanes\"\u003ecapstone project\u003c/a\u003e*. Based on their combination of technical skills and subject-matter expertise, this student landed a government consulting role at \u003cstrong\u003e\u003cem\u003eASR Analytics\u003c/em\u003e\u003c/strong\u003e where they work to prevent identity theft in tax fraud.\u003c/p\u003e\n\n\u003cp\u003eThis student (\u003ca href=\"https://github.com/kbaranko/NYC-Building-Energy-Intensity/blob/master/README.md\"\u003eGitHub link here\u003c/a\u003e*) focused on working in the clean energy sector, and created their project \u003cem\u003eNYC Building Energy Density\u003c/em\u003e using data from the 2016 Energy and Water Data Disclosure for New York City Local Law 84. The student landed a role at \u003cstrong\u003e\u003cem\u003eKevala\u003c/em\u003e\u003c/strong\u003e, a clean energy software company, in under two months of job seeking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e*\u003cem\u003eKeep in mind that the Flatiron School Data Science program has changed over time, so these projects may or may not reflect the current project requirements. They are intended as inspiration for your dataset/project choice.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch2\u003eCurated List of Datasets\u003c/h2\u003e\n\n\u003cp\u003eYou may select any of the datasets below - we provide brief descriptions of each. Follow the links to learn more about the dataset and business problems before making a final decision.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eIf you are feeling overwhelmed or behind, we recommend you choose dataset #1.\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch3\u003e1) \u003ca href=\"https://www.kaggle.com/becksddf/churn-in-telecoms-dataset\"\u003eSyriaTel Customer Churn\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eBuild a classifier to predict whether a customer will (\"soon\") stop doing business with SyriaTel, a telecommunications company. This is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\n\u003cp\u003eMost naturally, your audience here would be the telecom business itself, interested in reducing how much money is lost because of customers who don't stick around very long. The question you can ask is: are there any predictable patterns here?\u003c/p\u003e\n\n\u003ch3\u003e2) \u003ca href=\"https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/\"\u003eTanzanian Water Wells\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eThis dataset is part of an active competition until April 7, 2023!\u003c/p\u003e\n\n\u003cp\u003eTanzania, as a developing country, struggles with providing clean water to its population of over 57,000,000. There are many water points already established in the country, but some are in need of repair while others have failed altogether.\u003c/p\u003e\n\n\u003cp\u003eBuild a classifier to predict the condition of a water well, using information about the sort of pump, when it was installed, etc. Your audience could be an NGO focused on locating wells needing repair, or the Government of Tanzania looking to find patterns in non-functional wells to influence how new wells are built. Note that this is a \u003cstrong\u003eternary\u003c/strong\u003e classification problem by default, but can be engineered to be binary.\u003c/p\u003e\n\n\u003ch3\u003e3) \u003ca href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/\"\u003eH1N1 and Seasonal Flu Vaccines\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eThis dataset is part of an active competition until March 31, 2022!\u003c/p\u003e\n\n\u003cp\u003eAs the world struggles to vaccinate the global population against COVID-19, an understanding of how people‚Äôs backgrounds, opinions, and health behaviors are related to their personal vaccination patterns can provide guidance for future public health efforts. Your audience could be someone guiding those public health efforts.\u003c/p\u003e\n\n\u003cp\u003eThis challenge: can you predict whether people got H1N1 and seasonal flu vaccines using data collected in the National 2009 H1N1 Flu Survey? This is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem, but there are two potential targets: whether the survey respondent received the seasonal flu vaccine, or whether the respondent received the H1N1 flu vaccine. Please choose just one of these potential targets for your minimum viable project.\u003c/p\u003e\n\n\u003ch3\u003e4) \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\"\u003eChicago Car Crashes\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eNote this links also to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3\"\u003eVehicle Data\u003c/a\u003e and to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d\"\u003eDriver/Passenger Data\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eBuild a classifier to predict the primary contributory cause of a car accident, given information about the car, the people in the car, the road conditions etc. You might imagine your audience as a Vehicle Safety Board who's interested in reducing traffic accidents, or as the City of Chicago who's interested in becoming aware of any interesting patterns. \u003c/p\u003e\n\n\u003cp\u003eThis is a \u003cstrong\u003emulti-class\u003c/strong\u003e classification problem. You will almost certainly want to bin, trim or otherwise limit the number of target categories on which you ultimately predict. Note that some primary contributory causes have very few samples, for example.\u003c/p\u003e\n\n\u003ch3\u003e5) \u003ca href=\"https://data.seattle.gov/Public-Safety/Terry-Stops/28ny-9ts8\"\u003eTerry Traffic Stops\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eIn \u003ca href=\"https://www.oyez.org/cases/1967/67\"\u003eTerry v. Ohio\u003c/a\u003e, a landmark Supreme Court case in 1967-8, the court found that a police officer was not in violation of the \"unreasonable search and seizure\" clause of the Fourth Amendment, even though he stopped and frisked a couple of suspects only because their behavior was suspicious. Thus was born the notion of \"reasonable suspicion\", according to which an agent of the police may e.g. temporarily detain a person, even in the absence of clearer evidence that would be required for full-blown arrests etc. Terry Stops are stops made of suspicious drivers.\u003c/p\u003e\n\n\u003cp\u003eBuild a classifier to predict whether an arrest was made after a Terry Stop, given information about the presence of weapons, the time of day of the call, etc. This is a binary classification problem.\u003c/p\u003e\n\n\u003cp\u003eNote that this dataset also includes information about gender and race. You may use this data as well. You could conceivably pitch your project as an inquiry into whether race (of officer or of subject) plays a role in whether or not an arrest is made.\u003c/p\u003e\n\n\u003cp\u003eIf you do elect to make use of race or gender data, be aware that this can make your project a highly sensitive one; your discretion will be important, as well as your transparency about how you use the data and the ethical issues surrounding it.\u003c/p\u003e\n\n\u003ch2\u003eProposing Your Own Dataset\u003c/h2\u003e\n\n\u003cp\u003eSourcing new data is a valuable skill for data scientists, but it requires a great deal of care. An inappropriate dataset or an unclear business problem can lead you to spend a lot of time on a project that delivers underwhelming results. The guidelines below will help you complete a project that demonstrates your ability to engage in the full data science process.\u003c/p\u003e\n\n\u003cp\u003eOnce you've sourced your own dataset and identified the business problem you want to solve with it, \u003cstrong\u003eyou must run them by your instructor for approval.\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch3\u003eData Guidelines\u003c/h3\u003e\n\n\u003cp\u003eYour dataset must be:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAppropriate for classification.\u003c/strong\u003e It should have a categorical outcome or the data needed to engineer one.   \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eUsable to solve a specific business problem.\u003c/strong\u003e This solution must rely on your classification model.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSomewhat complex.\u003c/strong\u003e It should contain a minimum of 1000 rows and 10 features.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eUnfamiliar.\u003c/strong\u003e It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. Titanic).\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eManageable.\u003c/strong\u003e Stick to datasets that you can model using the techniques introduced in Phase 3.\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003eProblem First, or Data First?\u003c/h3\u003e\n\n\u003cp\u003eThere are two ways that you can source your own dataset: \u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e. The less time you have to complete the project, the more strongly we recommend a Data First approach to this project.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e: Start with a problem that you are interested in that you could potentially solve with a classification model. Then look for data that you could use to solve that problem. This approach is high-risk, high-reward: Very rewarding if you are able to solve a problem you are invested in, but frustrating if you end up sinking lots of time in without finding appropriate data. To mitigate the risk, set a firm limit for the amount of time you will allow yourself to look for data before moving on to the Data First approach.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e: Take a look at some of the most popular internet repositories of cool data sets we've listed below. If you find a data set that's particularly interesting for you, then it's totally okay to build your problem around that data set.\u003c/p\u003e\n\n\u003ch3\u003ePotential Data Sources\u003c/h3\u003e\n\n\u003cp\u003eThere are plenty of amazing places that you can get your data from. We recommend you start looking at data sets in some of these resources first:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets.php\"\u003eUCI Machine Learning Datasets Repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets\"\u003eKaggle Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/awesomedata/awesome-public-datasets\"\u003eAwesome Datasets Repo on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLocal data portals for state and local government resources\n\n\u003cul\u003e\n\u003cli\u003eExamples: \u003ca href=\"https://opendata.cityofnewyork.us/\"\u003eNYC\u003c/a\u003e, \u003ca href=\"http://data.houstontx.gov/\"\u003eHouston\u003c/a\u003e, \u003ca href=\"https://data.seattle.gov/\"\u003eSeattle\u003c/a\u003e, \u003ca href=\"https://data.ca.gov/\"\u003eCalifornia\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://insideairbnb.com/\"\u003eInside AirBNB\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://data.fivethirtyeight.com/\"\u003eFiveThirtyEight‚Äôs data portal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\"\u003eData is Plural‚Äôs Archive Spreadsheet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.reddit.com/r/datasets/\"\u003eDatasets Subreddit\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","exportId":"phase-3-project-choosing-a-dataset"},{"id":456613,"title":"Phase 3 Project Checklist","type":"ExternalUrl","indent":1,"locked":false,"requirement":null,"completed":false,"content":"https://docs.google.com/document/d/1KCWL-2eQ4PFkEIwOXSDrycF4pGLEYC0CYOfWOQQgozo/edit?usp=sharing"},{"id":456616,"title":"Submit Your Project Here","type":"ContextModuleSubHeader","indent":1,"locked":false},{"id":456637,"title":"Phase 3 Project - GitHub Repository URL","type":"Assignment","indent":1,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":"2022-10-07T23:59:59-04:00","lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","exportId":"gb72bb79ecbf4e6a1d3f538f126ac6d8c"},{"id":456640,"title":"BLOG POST","type":"ContextModuleSubHeader","indent":0,"locked":false},{"id":456645,"title":"Blogging Overview","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003e‚ú®Have fun and happy blogging!‚ú®\u003c/p\u003e","exportId":"blogging-overview"},{"id":456650,"title":"Phase 3 Blog Post","type":"Assignment","indent":0,"locked":false,"submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron School‚Äôs requirements.\u003c/span\u003e\u003c/p\u003e","exportId":"gaad3ce28f6a3f4a5c4b8da821048c163"}]},{"id":46998,"name":"APPENDIX: More OOP","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"gfd45e1b463d698d320c9d9f01d71f34b","items":[{"id":456665,"title":"Object Oriented Attributes with Functions","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb9ac29340ae2e7a63aeadabeacc31527"},{"id":456670,"title":"Object Oriented Attributes With Functions - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gb22c89bceacbf963e06408ab0cbd3424"},{"id":456676,"title":"Object Oriented Shopping Cart - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g1755c0c703e3acfea9f4882b8959a352"},{"id":456679,"title":"Building an Object-Oriented Simulation","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn a bit more about best practices for running simulations in the real world using object-oriented programming!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse inheritance to write nonredundant code\u003c/li\u003e\n\u003cli\u003eCreate methods that calculate statistics of the attributes of an object\u003c/li\u003e\n\u003cli\u003eCreate object-oriented data models that describe the real world with multiple classes and subclasses and interaction between classes\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCreating Stochastic Simulations\u003c/h2\u003e\n\n\u003cp\u003eAs a capstone for everything you've learned about object-oriented programming, you'll be creating a \u003cstrong\u003e\u003cem\u003eHerd Immunity Simulation\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/master/images/herd_immunity.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/dataisbeautiful/comments/5v72fw/how_herd_immunity_works_oc/\"\u003egif created by u/theotheredmund\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThis simulation is meant to model the effects that vaccinations have on the way a communicable disease spreads through a population. The simulation you're building depends on just a few statistics from the CDC (Center for Disease Control):\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003er0\u003c/code\u003e, the average number of people a contagious person infects before they are no longer contagious (because they got better or they died)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emortality_rate\u003c/code\u003e, the percentage chance a person infected with a disease will die from it \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe main workflow of this simulation is as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003ePerson()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ealive\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evaccinated\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehas_been_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enewly_infected\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003eSimulation()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epopulation\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evirus_name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_time_steps\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003er0\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epercent_pop_vaccinated\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCreate methods for our \u003ccode\u003eSimulation()\u003c/code\u003e class that will cover each step of the simulation. \u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIn order for our simulation to work, you'll need to define some rules for it:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eEach infected person will \"interact\" with 100 random people from the \u003ccode\u003epopulation\u003c/code\u003e. If the person the infected individual interacts with is sick, vaccinated, or has had the disease before, nothing happens. However, if the person the infected individual interacts with is healthy, unvaccinated, and has not been infected yet, then that person becomes infected. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt the end of each round, the following things happen:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eAll currently infected people either get better from the disease or die, with the chance of death corresponding to the mortality rate of the disease \u003c/li\u003e\n\u003cli\u003eAll people that were newly infected during this round become the new infected for the next round\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe simulation continues for the set number of time steps.  Any time someone dies or gets infected, log it in a text file called \u003ccode\u003e'simulation_logs.txt'\u003c/code\u003e.  Once the simulation is over, write some code to quickly parse the text logs into data and visualize the results, so that you can run multiple simulations and answer questions like: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIf vaccination rates for {disease x} dropped by 5%, how many more people become infected in an epidemic? How many more die?\u003c/li\u003e\n\u003cli\u003eWhat does the spread of {disease x} through a population look like?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf this all seems a bit daunting, don't worry! You'll be provided with much more detail as you build this step-by-step during the lab. \u003c/p\u003e\n\n\u003cp\u003eWith that, go ahead and take a look at this cool simulation lab!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you reviewed some best practices for simulating things in the real world using object-oriented programming!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"building-an-object-oriented-simulation"},{"id":456686,"title":"Building an Object-Oriented Simulation - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g7df6fb0d0aa358bcfc133f47e6a121c8"}]},{"id":46999,"name":"APPENDIX: More Linear Algebra","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"g5596af70046cac2b6a11c0ac6f65c416","items":[{"id":456696,"title":"Vector Addition and Broadcasting in NumPy - Code Along","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g607f57af8b1267deb9b5c49c5c1b7ea2"},{"id":456699,"title":"Vectors and Matrices in Numpy - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g0760252c10be6de4644b3386a48bc65b"},{"id":456703,"title":"Properties of Dot Product - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfd090dd8ae4044fef856f70185578e8f"},{"id":456707,"title":"Pure Python vs. Numpy - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"gfd0d759c515d090524907fb11efc4ad1"}]},{"id":47003,"name":"APPENDIX: More On Derivatives","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"ga8eec96cc39efd1f36ab460882860c2c","items":[{"id":456719,"title":"Rules for Derivatives - Lab","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"geae69a859277d4635ccd9f62b82d1703"},{"id":456723,"title":"Derivatives: the Chain Rule","type":"Assignment","indent":0,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g6a9260d7fd3edc533d3b4ae5aabcd3e6"},{"id":456728,"title":"Gradient to Cost Function - Appendix","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-to-cost-function-appendix\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll find the details on how to compute the partial derivatives in the \"Gradient to cost function\" lesson.\u003c/p\u003e\n\u003ch2\u003eComputing the First Partial Derivative\u003c/h2\u003e\n\u003cp\u003eLet's start with taking the \u003cstrong\u003epartial derivative\u003c/strong\u003e with respect to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=%20%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow this is a tricky function to take the derivative of. So we can use functional composition followed by the chain rule to make it easier. Using functional composition, we can rewrite our function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e as two functions:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)%5Cmspace%7B5ex%7D\"\u003e -- set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y-%5Chat%7By%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2%5Cmspace%7B4ex%7D\"\u003e -- now \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e is a function of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J=g%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow using the chain rule to find the partial derivative with respect to a change in the slope, gives us:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdm%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))*%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eBecause \u003cstrong\u003eg\u003c/strong\u003e is a function of \u003cstrong\u003em\u003c/strong\u003e we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e and\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJ\u003c/strong\u003e is a function of \u003cstrong\u003eg (which is a function of m\u003c/strong\u003e) we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e .\u003c/p\u003e\n\u003cp\u003eOur next step is to solve these derivatives individually.\u003c/p\u003e\n\u003cp\u003eFirst:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%202*g(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20(mx%20%2bb))\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20mx%20-%20b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%5Cfrac%7Bdg%7D%7Bdm%7Dy%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Dmx%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%200-x-0\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=-x\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEach of the terms are treated as constants, except for the middle term.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNow plugging these back into our chain rule [1] we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%5Ccolor%7Bblue%7D%7B(2*g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B-x%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B21.75ex%7D=%202*(y%20-%20(mx%20%2b%20b))*-x\"\u003e\u003c/p\u003e\n\u003cp\u003eSo\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=2*(y%20-%20(mx%20%2b%20b))*-x\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B15.75ex%7D=%20-2x*(y%20-%20(mx%20%2b%20b%20))\"\u003e\u003c/p\u003e\n\u003ch2\u003eComputing the Second Partial Derivative\u003c/h2\u003e\n\u003cp\u003eOk, now let's calculate the partial derivative with respect to a change in the y-intercept. We express this mathematically with the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20b%7DJ(m,%20b)%20=%20%5Cfrac%7BdJ%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eThen once again, we use functional composition following by the chain rule. So we view our cost function as the same two functions \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSo applying the chain rule, to this same function composition, we get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B2%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdb%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)*%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, our next step is to calculate these partial derivatives individually.\u003c/p\u003e\n\u003cp\u003eFrom our earlier calculation of the partial derivative, we know that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%20=%20%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2%20=%202*g(m,b)\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe only thing left to calculate is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b)%20)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdg%7D%7Bdb%7D(y-mx-b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdb%7D%7Bdb%7Dy-%5Cfrac%7Bdb%7D%7Bdb%7Dmx-%5Cfrac%7Bdg%7D%7Bdb%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=0-0-1\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we plug our terms into our chain rule [2] and get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%20%5Ccolor%7Bblue%7D%7B2*g(m,b)%7D*%5Ccolor%7Bred%7D%7B-1%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B16ex%7D=-2*(y%20-%20(mx%20%2b%20b))\"\u003e\u003c/p\u003e","exportId":"gradient-to-cost-function-appendix"}]},{"id":47005,"name":"APPENDIX: Support Vector Machines","status":"completed","unlockDate":null,"prereqs":[],"requirement":null,"sequential":false,"exportId":"ge26361bb2028e44bc8f4647e986e3b9e","items":[{"id":456739,"title":"Support Vector Machines - Introduction","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eA Support Vector Machine (SVM) is a type of classifier which modifies the loss function for optimization to not only take into account overall accuracy metrics of the resulting predictions, but also to maximize the decision boundary between the data points. In essence, this further helps tune the classifier as a good balance between underfitting and overfitting.\u003c/p\u003e\n\n\u003ch2\u003eSupport Vector Machines\u003c/h2\u003e\n\n\u003cp\u003eIn addition to optimizing for accuracy, support vector machines add a slack component, trading in accuracy to increase the distance between data points and the decision boundary. This provides an interesting perspective that can help formalize the intuitive visual choices a human would make in balancing precision and generalization to strike a balance between overfitting and underfitting.\u003c/p\u003e\n\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\n\u003cp\u003eInitially, you'll explore linear support vector machines that divide data points into their respective groups by drawing hyperplanes using the dimensions from the feature space. In practice, these have limitations and the dataset may not be cleanly separable. As a result, kernel functions are an additional tool that can be used. Essentially, kernels reproject data onto a new parameter space using combinations of existing features. From there, the same process of applying SVMs to this transformed space can then be employed.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eSupport Vector Machines are a powerful algorithm and may have the top performance among the out of the box classifiers from scikit-learn. Moreover, learning to properly tune SVMs is critical. In the upcoming labs and lessons, you'll investigate and apply these concepts.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-svm-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-svm-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","exportId":"support-vector-machines-introduction"},{"id":456743,"title":"Introduction to Support Vector Machines","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-support-vector-machines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBy now you've learned a few techniques for classification. You touched upon it when talking about Naive Bayes, and again when you saw some supervised learning techniques such as logistic regression and decision trees. Now it's time for another popular classification technique -- Support Vector Machines.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe what is meant by margin classifiers\u003c/li\u003e\n\u003cli\u003eDescribe the mathematical components underlying soft and max-margin classifiers\u003c/li\u003e\n\u003cli\u003eCompare and contrast max-margin classifiers and soft-margin classifiers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe idea\u003c/h2\u003e\n\u003cp\u003eThe idea behind Support Vector Machines (also referred to as SVMs) is that you perform classification by finding the separation line or (in higher dimensions) \"hyperplane\" that maximizes the distance between two classes. Taking a look at the concept visually helps make sense of the process.\u003c/p\u003e\n\u003cp\u003eImagine you have a dataset containing two classes:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_1.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eIn SVM, you want to find a hyperplane or \"decision boundary\" that divides one class from the other. Which one works best?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_3.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThis would be a good line:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile this seems intuitive, there are other decision boundaries which also separate the classes. Which one is best? Rather than solely focus on the final accuracy of the model, Support Vector Machines aim to \u003cstrong\u003emaximize the margin\u003c/strong\u003e between the decision boundary and the various data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_4.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThe margin is defined as the distance between the separating line (hyperplane) and the training set cases that are closest to this hyperplane. These cases define \"support vectors\". The support vectors in this particular case are highlighted in the image below. As you can see, the max-margin hyperplane is the midpoint between the two lines defined by the support vectors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Max Margin classifier\u003c/h2\u003e\n\u003cp\u003eWhy would you bother maximizing the margins? Don't these other hyperplanes discriminate just as well? Remember that you are fitting the hyperplane on your training data. Imagine you start looking at your test data, which will slightly differ from your training data.\u003c/p\u003e\n\u003cp\u003eAssuming your test set is big enough and randomly drawn from your entire dataset, you might end up with a test case as shown in the image below. This test case diverts a little bit from the training set cases observed earlier. While the max-margin classifier would classify this test set case correctly, the hyperplane closer to the right would have been classified this case incorrectly. Of course, this is just one example and other test cases will end up in different spots. Nonetheless, the purpose of choosing the max-margin classifier is to minimize the generalization error when applying the model to future unseen data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_test2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eBefore diving into the underlying mathematics, take a look at the image again:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can start exploring the mathematics behind the image. First, define some numeric labels for the two classes. Set the circles to be -1 and the diamonds to be 1. Normally, 0 and 1 are used for class labels but in this particular case using -1 and 1 simplifies the mathematics.\u003c/p\u003e\n\u003cp\u003eNow some terminology: The lines defined by the support vectors are the negative (to the left) and the positive (to the right) hyperplanes, respectively. These hyperplanes are defined by two terms: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e term is called the \u003cstrong\u003eweight vector\u003c/strong\u003e and contains the weights that are used in the classification.\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e term is called the \u003cstrong\u003ebias\u003c/strong\u003e and functions as an offset term. If there were no bias term, the hyperplane would always go through the origin which would not be very generalizable!\u003c/p\u003e\n\u003cp\u003eThe equation describing the positive hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bpos%7D%20=1\"\u003e\u003c/p\u003e\n\u003cp\u003eand the equation describing the negative hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bneg%7D%20=-1\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember, your goal is to maximize the separation between the two hyperplanes. To do this, first subtract the negative hyperplane's equation from the positive hyperplane's equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T(x_%7Bpos%7D-x_%7Bneg%7D)%20=%202\"\u003e\u003c/p\u003e\n\u003cp\u003eNext, normalize \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e by dividing both sides of the equation by its norm, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7C%20w%20%7C%7C=%20%5Csqrt%7B%5Csum%5Em_%7Bj-1%7Dw_j%5E2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eDividing the former expression by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e yields the equation below. The left side of the resulting equation can be interpreted as the distance between the positive and negative hyperplanes. This is the \u003cstrong\u003emargin\u003c/strong\u003e you're trying to maximize.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bw_T(x_%7Bpos%7D-x_%7Bneg%7D)%7D%7B%5ClVert%20w%20%5CrVert%7D%20=%20%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective of the SVM is then maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e under the constraint that the samples are classified correctly. Mathematically,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThese equations basically say that all negative samples should fall on the left side of the negative hyperplane, whereas all the positive samples should fall on the right of the positive hyperplane. This can also be written in one line as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20(b%20%2b%20w_Tx%5E%7B(i)%7D%20)%5Cgeq%201\"\u003e for each \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e means we're minimizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5ClVert%20w%20%5CrVert\"\u003e , or, as is done in practice because it seems to be easier to be minimized, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2\"\u003e .\u003c/p\u003e\n\u003ch2\u003eThe Soft Margin classifier\u003c/h2\u003e\n\u003cp\u003eIntroducing slack variables \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cxi\"\u003e . The idea for introducing slack variables is that the linear constraints need to be relaxed for data that are not linearly separable, as not relaxing the constraints might lead to the algorithm that doesn't converge.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201-%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1%2b%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective function (AKA the function you want to minimize) is\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2%2b%20C(%5Csum_i%20%5Cxi%5E%7B(i)%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eYou're basically adding these slack variables in your objective function, making clear that you want to minimize the amount of slack you allow for. You can tune this with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e as shown in the above equation. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will define how much slack we're allowing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA big value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the left: misclassifications are heavily punished, so the optimization prioritizes classifying correctly over having a big margin.\u003c/li\u003e\n\u003cli\u003eA small value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the right: it is OK to have some misclassifications, in order to gain a bigger margin overall. (This can help avoid overfitting to the training data.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_C.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now understand both max-margin classifiers as well as soft-margin classifiers. In the next lab, you'll try to code these fairly straightforward linear classifiers from scratch!\u003c/p\u003e","exportId":"introduction-to-support-vector-machines"},{"id":456749,"title":"Building an SVM from Scratch - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g3103e11723b224df8b8f7fa054ac87a5"},{"id":456754,"title":"Building an SVM using scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g45fda3bf4a0608d78bd3d255c954a28b"},{"id":456761,"title":"The Kernel Trick","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-kernel-trick\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn how to create SVMs with non-linear decision boundaries data using kernels!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine the kernel trick and explain why it is important in an SVM model\u003c/li\u003e\n\u003cli\u003eDescribe a radial basis function kernel\u003c/li\u003e\n\u003cli\u003eDescribe a sigmoid kernel\u003c/li\u003e\n\u003cli\u003eDescribe a polynomial kernel\u003c/li\u003e\n\u003cli\u003eDetermine when it is best to use specific kernels within SVM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNon-linear problems: The Kernel trick\u003c/h2\u003e\n\u003cp\u003eIn the previous lab, you looked at a plot where a linear boundary was clearly not sufficient to separate the two classes cleanly. Another example where a linear boundary would not work well is shown below. How would you draw a max margin classifier here? The intuitive solution is to draw an arc around the circles, separating them from the surrounding diamonds. To generate non-linear boundaries such as this, you use what is known as a kernel.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_nonlin.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe idea behind kernel methods is to create (nonlinear) combinations of the original features and project them onto a higher-dimensional space. For example, take a look at how this dataset could be transformed with an appropriate kernel from a two-dimensional dataset onto a new three-dimensional feature space.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_kernel.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eTypes of kernels\u003c/h2\u003e\n\u003cp\u003eThere are several kernels, and an overview can be found in this lesson, as well as in the scikit-learn documentation \u003ca href=\"https://scikit-learn.org/stable/modules/svm.html#kernel-functions\"\u003ehere\u003c/a\u003e. The idea is that kernels are inner products in a transformed space.\u003c/p\u003e\n\u003ch3\u003eThe Linear kernel\u003c/h3\u003e\n\u003cp\u003eThe linear kernel is, as you've seen, the default kernel and simply creates linear decision boundaries. The linear kernel is represented by the inner product of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clangle%20x,%20x'%20%5Crangle\"\u003e . It is important to note that some kernels have additional parameters that can be specified and knowing how these parameters work is critical to tuning SVMs.\u003c/p\u003e\n\u003ch3\u003eThe RBF kernel\u003c/h3\u003e\n\u003cp\u003eThere are two parameters when training an SVM with the \u003cem\u003eR\u003c/em\u003eadial \u003cem\u003eB\u003c/em\u003easis \u003cem\u003eF\u003c/em\u003eunction: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e .\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e is common to all SVM kernels. Again, by tuning the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e parameter when using kernels, you can provide a trade-off between misclassification of the training set and simplicity of the decision function. A high \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will classify as many samples correctly as possible (and might potentially lead to overfitting)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e defines how much influence a single training example has. The larger \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e is, the closer other examples must be to be affected\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe RBF kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cexp%7B(-%5Cgamma%20%5ClVert%20%20x%20-%20%20x'%20%5CrVert%5E2)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eGamma has a strong effect on the results: a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e that is too large will lead to overfitting, while a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e which is too small will lead to underfitting (kind of like a simple linear boundary for a complex problem).\u003c/p\u003e\n\u003cp\u003eIn scikit-learn, you can specify a value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e using the parameter \u003ccode\u003egamma\u003c/code\u003e. The default \u003ccode\u003egamma\u003c/code\u003e value is \"auto\", if no other gamma is specified, gamma is set to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1/%5Ctext%7Bnumber_of_features%7D\"\u003e . You can find more on parameters in the RBF kernel \u003ca href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eThe Polynomial kernel\u003c/h3\u003e\n\u003cp\u003eThe Polynomial kernel is specified as\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cgamma%20%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)%5Ed\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e can be specified by the parameter \u003ccode\u003edegree\u003c/code\u003e. The default degree is 3.\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e can be specified by the parameter \u003ccode\u003ecoef0\u003c/code\u003e. The default is 0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Sigmoid kernel\u003c/h3\u003e\n\u003cp\u003eThe sigmoid kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctanh%20(%20%5Cgamma%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis kernel is similar to the signoid function in logistic regression.\u003c/p\u003e\n\u003ch2\u003eSome more notes on SVC, NuSVC, and LinearSVC\u003c/h2\u003e\n\u003ch3\u003eNuSVC\u003c/h3\u003e\n\u003cp\u003eNuSVC is similar to SVC, but adds an additional parameter, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e , which controls the number of support vectors and training errors. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e jointly creates an upper bound on training errors and a lower bound on support vectors.\u003c/p\u003e\n\u003cp\u003eJust like SVC, NuSVC implements the \"one-against-one\" approach when there are more than 2 classes. This means that when there are n classes, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bn*(n-1)%7D%7B2%7D\"\u003e classifiers are created, and each one classifies samples in 2 classes.\u003c/p\u003e\n\u003ch3\u003eLinearSVC\u003c/h3\u003e\n\u003cp\u003eLinearSVC is similar to SVC, but instead of the \"one-versus-one\" method, a \"one-vs-rest\" method is used. So in this case, when there are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classes, just \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classifiers are created, and each one classifies samples in 2 classes, the one of interest, and all the other classes. This means that SVC generates more classifiers, so in cases with many classes, LinearSVC actually tends to scale better.\u003c/p\u003e\n\u003ch2\u003eProbabilities and predictions\u003c/h2\u003e\n\u003cp\u003eYou can make predictions using support vector machines. The SVC decision function gives a probability score per class. However, this is not done by default. You'll need to set the \u003ccode\u003eprobability\u003c/code\u003e argument equal to \u003ccode\u003eTrue\u003c/code\u003e. Scikit-learn internally performs cross-validation to compute the probabilities, so you can expect that setting \u003ccode\u003eprobability\u003c/code\u003e to \u003ccode\u003eTrue\u003c/code\u003e makes the calculations longer. For large datasets, computation can take considerable time to execute.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now have a basic understanding of how to use kernel functions in Support Vector Machines. You'll do just that in the upcoming lab!\u003c/p\u003e","exportId":"the-kernel-trick"},{"id":456767,"title":"Kernels in scikit-learn - Lab","type":"Assignment","indent":1,"locked":false,"submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","exportId":"g9e64eee30e747fc4c6498bd573227ec2"},{"id":456771,"title":"Support Vector Machines - Recap","type":"WikiPage","indent":0,"locked":false,"requirement":null,"completed":false,"content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-svm-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs you saw, support vector machines are another classification tool to add to your repertoire. While computationally expensive, they can be powerful tools providing substantial performance gains in many instances.\u003c/p\u003e\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\u003cp\u003eProbably the most important information worth reviewing is some of the various kernel functions that you can apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRadial Basis Function (RBF)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ec\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePolynomial Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e , which can be specified using \u003ccode\u003edegree\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eSigmoid Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAlso recall that in general, \u003ccode\u003ec\u003c/code\u003e is the parameter for balancing standard accuracy metrics for tuning classifiers with the decision boundary distance.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWhile it may appear that this section was a bit brief, Support Vector Machines are a powerful algorithm that deserve attention, so make sure you investigate them properly. Moreover, learning to properly tune SVMs using kernels and an appropriate \u003ccode\u003ec\u003c/code\u003e value is critical.\u003c/p\u003e","exportId":"support-vector-machines-recap"},{"id":456776,"title":"Quiz: Support Vector Machines","type":"Quizzes::Quiz","indent":2,"locked":false,"assignmentExportId":"g60e839b12dfaa060967cee6c080f0eda","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null,"requirement":null,"completed":false,"content":"","exportId":"gb7c0c54f1a48881f9ef20382bb2f26a5"}]}],"pages":[{"exportId":"topic-28-lesson-priorities-live","title":"Topic 28 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 100%; height: 217px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eNaive Bayes\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 29.8418%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.00988%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003ca title=\"Bayesian Classification - Introduction\" href=\"pages/bayesian-classification-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/bayesian-classification-introduction\" data-api-returntype=\"Page\"\u003eBayesian Classification - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 28px;\"\u003e\u003cstrong\u003e \u003ca title=\"Classifiers with Bayes\" href=\"pages/classifiers-with-bayes\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/classifiers-with-bayes\" data-api-returntype=\"Page\"\u003eClassifiers with Bayes\u003c/a\u003e \u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e \u003ca title=\"Gaussian Naive Bayes\" href=\"assignments/gc08f06121b1cb6900720308cdce032e8\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186981\" data-api-returntype=\"Assignment\"\u003eGaussian Naive Bayes\u003c/a\u003e \u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003ca title=\"Gaussian Naive Bayes - Lab\" href=\"assignments/gb77f66d2830c911912d6ef073be94510\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186982\" data-api-returntype=\"Assignment\"\u003eGaussian Naive Bayes - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Document Classification with Naive Bayes\" href=\"assignments/g236db4ca49d5baf1e702753a44377c26\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186975\" data-api-returntype=\"Assignment\"\u003eDocument Classification with Naive Bayes\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e \u003ca title=\"Document Classification with Naive Bayes - Lab\" href=\"assignments/gad6d501b5a276265442395228b9f671b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186976\" data-api-returntype=\"Assignment\"\u003eDocument Classification with Naive Bayes - Lab\u003c/a\u003e \u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Bayes Classification\" href=\"quizzes/g072d490545d2ff5e9675b66373a4d996\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30649\" data-api-returntype=\"Quiz\"\u003eQuiz: Bayes Classification\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.8127%; height: 82px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eNaive Bayes\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 29.8418%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.00988%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 29.8418%;\"\u003e\u003ca title=\"Short Video: Naive Bayes by Hand\" href=\"pages/short-video-naive-bayes-by-hand\"\u003eShort Video: Naive Bayes by Hand\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Gaussian Naive Bayes Exit Ticket\" href=\"quizzes/gac9c11dae98412bec642d898197f115a\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30643\" data-api-returntype=\"Quiz\"\u003eGaussian Naive Bayes Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 29.8418%; height: 27px;\"\u003e\u003ca title=\"Bayesian Classification - Recap\" href=\"pages/bayesian-classification-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/bayesian-classification-recap\" data-api-returntype=\"Page\"\u003eBayesian Classification - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.00988%; height: 27px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","frontPage":false},{"exportId":"topic-25-lesson-priorities-live","title":"Topic 25 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.859%; height: 123px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eClassification Metrics 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.4896%;\"\u003e\u003ca title=\"Classification Metrics - Introduction\" href=\"pages/classification-metrics-introduction\"\u003eClassification Metrics - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca class=\"inline_disabled\" href=\"assignments/gee82ca01b6cbad81fe60f2bfd74f5c6f\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12075\" data-api-returntype=\"Assignment\"\u003eConfusion Matrices\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Visualizing Confusion Matrices - Lab\" href=\"assignments/gfc4cabc49a2b5825bcd2280f966121f2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12076\" data-api-returntype=\"Assignment\"\u003eVisualizing Confusion Matrices - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/evaluation-metrics\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/evaluation-metrics\" data-api-returntype=\"Page\"\u003eEvaluation Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Evaluating Logistic Regression Models - Lab\" href=\"assignments/g163b0dfb05702dcac7b6cd220709bd40\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12078\" data-api-returntype=\"Assignment\"\u003eEvaluating Logistic Regression Models - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.859%; height: 113px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Metrics 1\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eClassification Metrics 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Classification Metrics 1 Exit Ticket\" href=\"quizzes/g496587f746349f84af2ab1d3597060af\"\u003eClassification Metrics 1 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"ROC Curves and AUC\" href=\"assignments/g4bde6387fe8446d8a78599461096f6fa\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12079\" data-api-returntype=\"Assignment\"\u003eROC Curves and AUC\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"ROC Curves and AUC - Lab\" href=\"assignments/g7dd679196a289cb69567a310143e0a69\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12080\" data-api-returntype=\"Assignment\"\u003eROC Curves and AUC - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.4896%;\"\u003e\u003ca title=\"Logistic Regression Model Comparisons - Lab\" href=\"assignments/ge33ea8e6409baf98644fba31c69392ac\"\u003eLogistic Regression Model Comparisons - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Class Imbalance Problems\" href=\"assignments/g3a698bf676d43a6258d675063aefc220\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12081\" data-api-returntype=\"Assignment\"\u003eClass Imbalance Problems\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Class Imbalance Problems - Lab\" href=\"assignments/gc7eb5852a1d8a6a65e79c5a0b1b487fa\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12418\" data-api-returntype=\"Assignment\"\u003eClass Imbalance Problems - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.4896%;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Classification Metrics\" href=\"quizzes/gfbc831faf7b76124ddf50175a5d466d8\"\u003eQuiz: Classification Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.7653%; height: 48px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Metrics 2\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eClassification Workflow 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Classification Metrics 2 Exit Ticket\" href=\"quizzes/g7e7a19244813ae045fb81ce6e5ee782d\"\u003eClassification Metrics 2 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 95.9527%; height: 99px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Workflow 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.4896%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 10.237%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 28px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Classification Workflow 1 Exit Ticket\" href=\"quizzes/g88bf554f6b9c73e7af697f7b2c831308\"\u003eClassification Workflow 1 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; text-align: center; height: 28px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Logistic Regression - Cumulative Lab\" href=\"quizzes/g307f021a855d355fe1b11925047cb804\"\u003e‚≠êÔ∏è Logistic Regression - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.4896%; height: 29px;\"\u003e\u003ca title=\"Classification Metrics - Recap\" href=\"pages/classification-metrics-recap\"\u003eClassification Metrics - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 10.237%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","frontPage":false},{"exportId":"bayesian-classification-recap","title":"Bayesian Classification - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-recap-v2-1\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features \u003c/li\u003e\n\u003cli\u003eThis assumption (that the underlying features are independent) is why Naive Bayes algorithm is considered naive -- because this is almost never true. However, Naives Bayes can prove to be quite efficient \u003c/li\u003e\n\u003cli\u003eExpanding to multiple features, the multinomial Bayes' formula is:\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\" src=\"/equation_images/%20%255CLarge%20P(y|x_1,%20x_2,%20...,%20x_n)%20=%20%255Cfrac{P(y)%255Cprod_{i}^{n}P(x_i|y)}{P(x_1,%20x_2,%20...,%20x_n)}\" alt=\"{\" data-equation-content=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\"\u003e\u003c/p\u003e \u003cp\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFinally, you saw how Naive Bayes algorithm can be used for document classification by classifying YouTube videos into the appropriate topic, and classifying documents as \"spam\" or \"no spam\"\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eDue to insufficient text preprocessing (which you will learn how to do in a later module), the performance of this algorithm was trivial \u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"k-nearest-neighbors","title":"K-Nearest Neighbors","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about a supervised learning algorithm, \u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e; and how you can use it to make predictions for classification and regression tasks!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how KNN makes classifications\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is K-Nearest Neighbors?\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eK-Nearest Neighbors\u003c/em\u003e\u003c/strong\u003e (or KNN, for short) is a supervised learning algorithm that can be used for both \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e tasks. However, in this section, we will cover KNN only in the context of classification. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between two points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a \u003cstrong\u003e\u003cem\u003eSupervised learning algorithm\u003c/em\u003e\u003c/strong\u003e, you must also have the labels for each point in the dataset, or else you wouldn't know what to predict!\u003c/p\u003e\n\n\u003ch2\u003eFitting the model\u003c/h2\u003e\n\n\u003cp\u003eKNN is unique compared to other classifiers in that it does almost nothing during the \"fit\" step, and all the work during the \"predict\" step. During the \"fit\" step, KNN just stores all the training data and corresponding labels. No distances are calculated at this point. \u003c/p\u003e\n\n\u003ch2\u003eMaking predictions with K\u003c/h2\u003e\n\n\u003cp\u003eAll the magic happens during the \"predict\" step. During this step, KNN takes a point that you want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the \u003ccode\u003eK\u003c/code\u003e closest points, or \u003cstrong\u003e\u003cem\u003eNeighbors\u003c/em\u003e\u003c/strong\u003e, and examines the labels of each. You can think of each of the K-closest points getting to 'vote' about the predicted class. Naturally, they all vote for the same class that they belong to. The majority wins, and the algorithm predicts the point in question as whichever class has the highest count among all of the k-nearest neighbors.\u003c/p\u003e\n\n\u003cp\u003eIn the following animation, K=3: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-k-nearest-neighbors/master/images/knn.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://gfycat.com/wildsorrowfulchevrotain\"\u003egif source\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eWhen using KNN, you can use \u003cstrong\u003e\u003cem\u003eManhattan\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eEuclidean\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eMinkowski distance\u003c/em\u003e\u003c/strong\u003e, or any other distance metric. Choosing an appropriate distance metric is essential and will depend on the context of the problem at hand.\u003c/p\u003e\n\n\u003ch2\u003eEvaluating model performance\u003c/h2\u003e\n\n\u003cp\u003eHow to evaluate the model performance depends on whether you're using the model for a classification or regression task. KNN can be used for regression (by averaging the target scores from each of the K-nearest neighbors), as well as for both binary and multicategorical classification tasks. \u003c/p\u003e\n\n\u003cp\u003eEvaluating classification performance for KNN works the same as evaluating performance for any other classification algorithm -- you need a set of predictions, and the corresponding ground-truth labels for each of the points you made a prediction on. You can then compute evaluation metrics such as \u003cstrong\u003e\u003cem\u003ePrecision, Recall, Accuracy, F1-Score\u003c/em\u003e\u003c/strong\u003e etc. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat! Now that you know how the KNN classifier works, you'll implement KNN using Python from scratch in the next lab.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-k-nearest-neighbors\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"support-vector-machines-recap","title":"Support Vector Machines - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-svm-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAs you saw, support vector machines are another classification tool to add to your repertoire. While computationally expensive, they can be powerful tools providing substantial performance gains in many instances.\u003c/p\u003e\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\u003cp\u003eProbably the most important information worth reviewing is some of the various kernel functions that you can apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRadial Basis Function (RBF)\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ec\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePolynomial Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e , which can be specified using \u003ccode\u003edegree\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003eSigmoid Kernel\n\u003col\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e , which can be specified using \u003ccode\u003egamma\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e , which can be specified using \u003ccode\u003ecoef0\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAlso recall that in general, \u003ccode\u003ec\u003c/code\u003e is the parameter for balancing standard accuracy metrics for tuning classifiers with the decision boundary distance.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWhile it may appear that this section was a bit brief, Support Vector Machines are a powerful algorithm that deserve attention, so make sure you investigate them properly. Moreover, learning to properly tune SVMs using kernels and an appropriate \u003ccode\u003ec\u003c/code\u003e value is critical.\u003c/p\u003e","frontPage":false},{"exportId":"data-science-processes","title":"Data Science Processes","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-data-science-processes\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-data-science-processes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs discussed, this section is all about synthesizing your skills in order to work through a full Data Science workflow. In this lesson, you'll take a look at some general outlines for how Data Scientists organize their workflow and conceptualize their process.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the different data science process frameworks\u003c/li\u003e\n\u003cli\u003eCompare and contrast popular data science process frameworks such as CRISP-DM, KDD, OSEMN\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is a Data Science Process?\u003c/h2\u003e\n\n\u003cp\u003eData Science projects are often complex, with many stakeholders, data sources, and goals. Due to this, the Data Science community has created several methodologies for helping organize and structure Data Science Projects.  In this lesson, you'll explore three of the most popular methodologies -- \u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e, and \u003cstrong\u003e\u003cem\u003eOSEMN\u003c/em\u003e\u003c/strong\u003e, and explore how you can make use of them to keep your projects well-structured and organized. \u003c/p\u003e\n\n\u003ch2\u003eCRoss-Industry Standard Process for Data Mining (CRISP-DM)\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_crisp-dm.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eCRISP-DM\u003c/em\u003e\u003c/strong\u003e is probably the most popular Data Science process in the Data Science world right now. Take a look at the visualization above to get a feel for CRISP-DM. Notice that CRISP-DM is an iterative process!\u003c/p\u003e\n\n\u003cp\u003eLet's take a look at the individual steps involved in CRISP-DM.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eBusiness Understanding:\u003c/em\u003e\u003c/strong\u003e  This stage is all about gathering facts and requirements. Who will be using the model you build? How will they be using it? How will this help the goals of the business or organization overall? Data Science projects are complex, with many moving parts and stakeholders. They're also time intensive to complete or modify. Because of this, it is very important that the Data Science team working on the project has a deep understanding of what the problem is, and how the solution will be used. Consider the fact that many stakeholders involved in the project may not have technical backgrounds, and may not even be from the same organization.  Stakeholders from one part of the organization may have wildly different expectations about the project than stakeholders from a different part of the organization -- for instance, the sales team may be under the impression that a recommendation system project is meant to increase sales by recommending upsells to current customers, while the marketing team may be under the impression that the project is meant to help generate new leads by personalizing product recommendations in a marketing email. These are two very different interpretations of a recommendation system project, and it's understandable that both departments would immediately assume that the primary goal of the project is one that helps their organization. As a Data Scientist, it's up to you to clarify the requirements and make sure that everyone involved understands what the project is and isn't. \u003c/p\u003e\n\n\u003cp\u003eDuring this stage, the goal is to get everyone on the same page and to provide clarity on the scope of the project for everyone involved, not just the Data Science team. Generate and answer as many contextual questions as you can about the project. \u003c/p\u003e\n\n\u003cp\u003eGood questions for this stage include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eWho are the stakeholders in this project? Who will be directly affected by the creation of this project?\u003c/li\u003e\n\u003cli\u003eWhat business problem(s) will this Data Science project solve for the organization?\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eWhat problems are inside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat problems are outside the scope of this project?\u003c/li\u003e\n\u003cli\u003eWhat data sources are available to us?\u003c/li\u003e\n\u003cli\u003eWhat is the expected timeline for this project? Are there hard deadlines (e.g. \"must be live before holiday season shopping\") or is this an ongoing project?\u003c/li\u003e\n\u003cli\u003eDo stakeholders from different parts of the company or organization all have the exact same understanding about what this project is and isn't?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Understanding:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce we have a solid understanding of the business implications for this project, we move on to understanding our data. During this stage, we'll aim to get a solid understanding of the data needed to complete the project.  This step includes both understanding where our data is coming from, as well as the information contained within the data. \u003c/p\u003e\n\n\u003cp\u003eConsider the following questions when working through this stage:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eWhat data is available to us? Where does it live? Do we have the data, or can we scrape/buy/source the data from somewhere else?\u003c/li\u003e\n\u003cli\u003eWho controls the data sources, and what steps are needed to get access to the data?\u003c/li\u003e\n\u003cli\u003eWhat is our target?\u003c/li\u003e\n\u003cli\u003eWhat predictors are available to us?\u003c/li\u003e\n\u003cli\u003eWhat data types are the predictors we'll be working with?\u003c/li\u003e\n\u003cli\u003eWhat is the distribution of our data?\u003c/li\u003e\n\u003cli\u003eHow many observations does our dataset contain? Do we have a lot of data? Only a little? \u003c/li\u003e\n\u003cli\u003eDo we have enough data to build a model? Will we need to use resampling methods?\u003c/li\u003e\n\u003cli\u003eHow do we know the data is correct? How is the data collected? Is there a chance the data could be wrong?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Preparation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce we have a strong understanding of our data, we can move onto preparing the data for our modeling steps. \u003c/p\u003e\n\n\u003cp\u003eDuring this stage, we'll want to handle the following issues:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDetecting and dealing with missing values\u003c/li\u003e\n\u003cli\u003eData type conversions (e.g. numeric data mistakenly encoded as strings)\u003c/li\u003e\n\u003cli\u003eChecking for and removing multicollinearity (correlated predictors)\u003c/li\u003e\n\u003cli\u003eNormalizing our numeric data\u003c/li\u003e\n\u003cli\u003eConverting categorical data to numeric format through one-hot encoding\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModeling:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eOnce we have clean data, we can begin modeling! Remember, modeling, as with any of these other steps, is an iterative process. During this stage, we'll try to build and tune models to get the highest performance possible on our task. \u003c/p\u003e\n\n\u003cp\u003eConsider the following questions during the modeling step:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIs this a classification task? A regression task? Something else?\u003c/li\u003e\n\u003cli\u003eWhat models will we try?\u003c/li\u003e\n\u003cli\u003eHow do we deal with overfitting?\u003c/li\u003e\n\u003cli\u003eDo we need to use regularization or not?\u003c/li\u003e\n\u003cli\u003eWhat sort of validation strategy will we be using to check that our model works well on unseen data?\u003c/li\u003e\n\u003cli\u003eWhat loss functions will we use?\u003c/li\u003e\n\u003cli\u003eWhat threshold of performance do we consider as successful?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eEvaluation:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eDuring this step, we'll evaluate the results of our modeling efforts. Does our model solve the problems that we outlined all the way back during step 1? Why or why not? Often times, evaluating the results of our modeling step will raise new questions, or will cause us to consider changing our approach to the problem.  Notice from the CRISP-DM diagram above, that the \"Evaluation\" step is unique in that it points to both \u003cem\u003eBusiness Understanding\u003c/em\u003e and \u003cem\u003eDeployment\u003c/em\u003e.  As we mentioned before, Data Science is an iterative process -- that means that given the new information our model has provided, we'll often want to start over with another iteration, armed with our newfound knowledge! Perhaps the results of our model showed us something important that we had originally failed to consider the goal of the project or the scope.  Perhaps we learned that the model can't be successful without more data, or different data. Perhaps our evaluation shows us that we should reconsider our approach to cleaning and structuring the data, or how we frame the project as a whole (e.g. realizing we should treat the problem as a classification rather than a regression task). In any of these cases, it is totally encouraged to revisit the earlier steps.  \u003c/p\u003e\n\n\u003cp\u003eOf course, if the results are satisfactory, then we instead move onto deployment!\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eDeployment:\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, we'll focus on moving our model into production and automating as much as possible. Everything before this serves as a proof-of-concept or an investigation.  If the project has proved successful, then you'll work with stakeholders to determine the best way to implement models and insights.  For example, you might set up an automated ETL (Extract-Transform-Load) pipelines of raw data in order to feed into a database and reformat it so that it is ready for modeling. During the deployment step, you'll actively work to determine the best course of action for getting the results of your project into the wild, and you'll often be involved with building everything needed to put the software into production. \u003c/p\u003e\n\n\u003cp\u003eThis is one of the most rewarding steps of the entire Data Science process -- getting to see your work go live!\u003c/p\u003e\n\n\u003ch2\u003eKnowledge Discovery in Databases\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_kdd.png\" width=\"800\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKnowledge Discovery in Databases\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eKDD\u003c/em\u003e\u003c/strong\u003e is considered the oldest Data Science process. The creation of this process is credited to Gregory Piatetsky-Shapiro, who also runs the ever-popular Data Science blog, \u003ca href=\"https://www.kdnuggets.com/\"\u003ekdnuggets\u003c/a\u003e. If you're interested, read the original white paper on KDD, which can be found \u003ca href=\"https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1992.pdf\"\u003ehere\u003c/a\u003e!\u003c/p\u003e\n\n\u003cp\u003eThe KDD process is quite similar to the CRISP-DM process. The diagram above illustrates every step of the KDD process, as well as the expected output at each stage. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eSelection\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, you'll focus on selecting your problem, and the data that will help you answer it. This stage works much like the first stage of CRISP-DM -- you begin by focusing on developing an understanding of the domain the problem resides in (e.g. marketing, finance, increasing customer sales, etc), the previous work done in this domain, and the goals of the stakeholders involved with the process.  \u003c/p\u003e\n\n\u003cp\u003eOnce you've developed a strong understanding of the goals and the domain, you'll work to establish where your data is coming from, and which data will be useful to you.  Organizations and companies usually have a ton of data, and only some of it will be relevant to the problem you're trying to solve.  During this stage, you'll focus on examining the data sources available to you and gathering the data that you deem useful for the project.  \u003c/p\u003e\n\n\u003cp\u003eThe output of this stage is the dataset you'll be using for the Data Science project. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePreprocessing\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThe preprocessing stage is pretty straightforward -- the goal of this stage is to \"clean\" the data by preprocessing it.  For text data, this may include things like tokenization.  You'll also identify and deal with issues like outliers and/or missing data in this stage.  \u003c/p\u003e\n\n\u003cp\u003eIn practice, this stage often blurs with the \u003cem\u003eTransformation\u003c/em\u003e stage. \u003c/p\u003e\n\n\u003cp\u003eThe output of this stage is preprocessed data that is more \"clean\" than it was at the start of this stage -- although the dataset is not quite ready for modeling yet. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTransformation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, you'll take your preprocessed data and transform it in a way that makes it more ideal for modeling.  This may include steps like feature engineering and dimensionality reduction.  At this stage, you'll also deal with things like checking for and removing multicollinearity from the dataset. Categorical data should also be converted to numeric format through one-hot encoding during this step.\u003c/p\u003e\n\n\u003cp\u003eThe output of this stage is a dataset that is now ready for modeling. All null values and outliers are removed, categorical data has been converted to a format that a model can work with, and the dataset is generally ready for experimentation with modeling.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData Mining\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThe Data Mining stage refers to using different modeling techniques to try and build a model that solves the problem we're after -- often, this is a classification or regression task. During this stage, you'll also define your parameters for given models, as well as your overall criteria for measuring the performance of a model.  \u003c/p\u003e\n\n\u003cp\u003eYou may be wondering what Data Mining is, and how it relates to Data Science. In practice, it's just an older term that essentially means the same thing as Data Science. Dr. Piatetsky-Shapiro defines Data Mining as \"the non-trivial extraction of implicit, previously unknown and potentially useful information from data.\"  Making of things such as Machine Learning algorithms to find insights in large datasets that aren't immediately obvious without these algorithms is at the heart of the concept of Data Mining, just as it is in Data Science. In a pragmatic sense, this is why the terms Data Mining and Data Science are typically used interchangeably, although the term Data Mining is considered an older term that isn't used as often nowadays. \u003c/p\u003e\n\n\u003cp\u003eThe output of this stage results from a fit to the data for the problem we're trying to solve.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretation/Evaluation\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this final stage of KDD, we focus on interpreting the \"patterns\" discovered in the previous step to help us make generalizations or predictions that help us answer our original question. During this stage, you'll consolidate everything you've learned to present it to stakeholders for guiding future actions. Your output may be a presentation that you use to communicate to non-technical managers or executives (never discount the importance of knowing PowerPoint as a Data Scientist!).  Your conclusions for a project may range from \"this approach didn't work\" or \"we need more data about {X}\" to \"this is ready for production, let's build it!\".  \u003c/p\u003e\n\n\u003ch2\u003eOSEMN\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-data-science-processes/master/images/new_osemn.png\" width=\"800\"\u003e\n\u003ca href=\"https://www.kdnuggets.com/2018/02/data-science-command-line-book-exploring-data.html\" target=\"_blank\"\u003eAdapted from: KDNuggets\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThis brings us to the Data Science process we'll be using during this section -- OSEMN (sometimes referred as OSEMiN, and pronounced \"OH-sum\", rhymes with \"possum\"). This is the most straightforward of the Data Science processes discussed so far. Note that during this process, just like the others, the stages often blur together. It is completely acceptable (and often a best practice!) to float back and forth between stages as you learn new things about your problem, dataset, requirements, etc.  It's quite common to get to the modeling step and realize that you need to scrub your data a bit more or engineer a different feature and jump back to the \"Scrub\" stage, or go all the way back to the \"Obtain\" stage when you realize your current data isn't sufficient to solve this problem. As with any of these frameworks, OSEMN is meant to be treated more like a set of guidelines for structuring your project than set-in-stone steps that cannot be violated.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eObtain\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eAs with CRISP-DM and KDD, this step involves understanding stakeholder requirements, gathering information on the problem, and finally, sourcing data that we think will be necessary for solving this problem. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eScrub\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this stage, we'll focus on preprocessing our data.  Important steps such as identifying and removing null values, dealing with outliers, normalizing data, and feature engineering/feature selection are handled around this stage.  The line with this stage really blurs with the \u003cem\u003eExplore\u003c/em\u003e stage, as it is common to only realize that certain columns require cleaning or preprocessing as a result of the visualizations and explorations done during Step 3.  \u003c/p\u003e\n\n\u003cp\u003eNote that although technically, categorical data should be one-hot encoded during this step, in practice, it's usually done after data exploration.  This is because it is much less time-consuming to visualize and explore a few columns containing categorical data than it is to explore many different dummy columns that have been one-hot encoded. \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eExplore\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThis step focuses on getting to know the dataset you're working with. As mentioned above, this step tends to blend with the \u003cem\u003eScrub\u003c/em\u003e step mentioned above.  During this step, you'll create visualizations to really get a feel for your dataset.  You'll focus on things such as understanding the distribution of different columns, checking for multicollinearity, and other tasks like that.  If your project is a classification task, you may check the balance of the different classes in your dataset.  If your problem is a regression task, you may check that the dataset meets the assumptions necessary for a regression task.  \u003c/p\u003e\n\n\u003cp\u003eAt the end of this step, you should have a dataset ready for modeling that you've thoroughly explored and are extremely familiar with.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eModel\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eThis step, as with the last two frameworks, is also pretty self-explanatory. It consists of building and tuning models using all the tools you have in your data science toolbox.  In practice, this often means defining a threshold for success, selecting machine learning algorithms to test on the project, and tuning the ones that show promise to try and increase your results.  As with the other stages, it is both common and accepted to realize something, jump back to a previous stage like \u003cem\u003eScrub\u003c/em\u003e or \u003cem\u003eExplore\u003c/em\u003e, and make some changes to see how it affects the model.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpret\u003c/em\u003e\u003c/strong\u003e:\u003c/p\u003e\n\n\u003cp\u003eDuring this step, you'll interpret the results of your model(s), and communicate results to stakeholders.  As with the other frameworks, communication is incredibly important! During this stage, you may come to realize that further investigation is needed, or more data.  That's totally fine -- figure out what's needed, go get it, and start the process over! If your results are satisfactory to all stakeholders involved, you may also go from this stage right into putting your model into production and automating processes necessary to support it.  \u003c/p\u003e\n\n\u003ch2\u003eA Note On Communicating Results\u003c/h2\u003e\n\n\u003cp\u003eRegardless of the quality of your results, it's very important that you be aware of the business requirements and stakeholder expectations at all times! Generally, no matter which of the above processes you use, you'll communicate your results in a two-pronged manner: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eA short, high-level presentation covering your question, process, and results meant for non-technical audiences\u003c/li\u003e\n\u003cli\u003eA detailed Jupyter Notebook demonstrating your entire process meant for technical audiences\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn general, you can see why Data Scientists love Jupyter Notebooks! It is very easy to format results in a reproducible, easy-to-understand way.  Although a detailed Jupyter Notebook may seem like the more involved of the two deliverables listed above, the high-level presentation is often the hardest! Just remember -- even if the project took you/your team over a year and utilized the most cutting-edge machine learning techniques available, you still need to be able to communicate your results in about 5 slides (using graphics, not words, whenever possible!), in a 5 minute presentation in a way that someone that can't write code can still understand and be convinced by!\u003c/p\u003e\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the different data science process frameworks including CRISP-DM, KDD, and OSEMN. You also learned that the data science process is iterative and that a typical data science project involves many different stakeholders who may not have a technical background. As such, it's important to recognize that data scientists must be able to communicate their findings in a non-technical way.\u003c/p\u003e","frontPage":false},{"exportId":"object-oriented-programming-recap","title":"Object-Oriented Programming - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-recap-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you learned about Object-oriented programming (OOP) as a foundational practice for software development and programming.\u003c/p\u003e\n\u003ch2\u003eOOP overview\u003c/h2\u003e\n\u003cp\u003eYou now know all about OOP and how to define classes. Like functions, using classes in your programming can save you a lot of time by eliminating repetitive tasks. Classes go further than functions by allowing you to persist data. After all, class methods are fairly analogous to functions, while attributes add functionality by acting as data storage containers.\u003c/p\u003e\n\u003ch2\u003eClass structure\u003c/h2\u003e\n\u003cp\u003eAs you saw, the most basic class definition starts off with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there, you then saw how you can further define class methods:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003eclass ClassName:\n    def method_1(self):\n        pass # Ideally a method does something, but you get the point\n    def method_2(self):\n        print('This is a pretty useless second method.')\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou also learned about \u003ccode\u003eself\u003c/code\u003e. Specifically, that \u003ccode\u003eself\u003c/code\u003e is the default parameter used to define methods. This is necessary since instance methods implicitly pass in the object itself as an argument during execution.\u003c/p\u003e\n\u003ch2\u003eCreating instances\u003c/h2\u003e\n\u003cp\u003eRecall that after you define a class, you can then create instances of that class to bring it to life and use it! You're probably wondering what all of this has to do with data science. In turns out you'll use OOP principles when you start working with common data science libraries. For example, you might import the \u003ccode\u003eLinearRegression\u003c/code\u003e class from the scikit-learn library in order to create a regression model!\u003c/p\u003e\n\u003cp\u003eRemember, creating an instance of a class would look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"python\"\u003efrom sklearn.linear_model import LinearRegression() \n\nreg = LinearRegression() \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you create an instance object of the class, you can then use all the methods associated with that class!\u003c/p\u003e\n\u003ch2\u003eLevel up\u003c/h2\u003e\n\u003cp\u003eIf you would like to dive deeper into OOP and learn some advanced topics, you can check out the additional OOP lessons and labs in the Appendix.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eCongrats, you now have a solid foundation in OOP! You first learned how to define a class and methods. Then you learned how to create an instance of a class and define instance attributes. These skills will be useful for collaboration and writing concise, modular code!\u003c/p\u003e","frontPage":false},{"exportId":"short-video-cross-validation","title":"Short Video: Cross-Validation","type":"WikiPage","content":"\u003cdiv style=\"padding: 62.5% 0 0 0; position: relative;\"\u003e\u003ciframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" title=\"cross-val_phase2_gd\" src=\"https://player.vimeo.com/video/713478191?h=8a86a87a5e\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" allowfullscreen=\"allowfullscreen\" allow=\"autoplay; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"finding-the-best-value-for-k","title":"Finding the Best Value for K","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll investigate how changing the value for K can affect the performance of the model, and how to use this to find the best value for K.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eConduct a parameter search to find the optimal value for K \u003c/li\u003e\n\u003cli\u003eExplain how KNN is related to the curse of dimensionality \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eFinding the optimal number of neighbors\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've got a strong understanding of how the K-Nearest Neighbors algorithm works, but you likely have at least one lingering question‚Äî\u003cstrong\u003e\u003cem\u003ewhat is the best value to use for K\u003c/em\u003e\u003c/strong\u003e? There's no set number that works best. If there was, it wouldn't be called \u003cstrong\u003e\u003cem\u003eK\u003c/em\u003e\u003c/strong\u003e-nearest neighbors. While the best value for K is not immediately obvious for any problem, there are some strategies that you can use to select a good or near optimal value. \u003c/p\u003e\n\n\u003ch2\u003eK, overfitting, and underfitting\u003c/h2\u003e\n\n\u003cp\u003eIn general, the smaller K is, the tighter the \"fit\" of the model. Remember that with supervised learning, you want to fit a model to the data as closely as possible without \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e to patterns in the training set that don't generalize.  This can happen if your model pays too much attention to every little detail and makes a very complex decision boundary. Conversely, if your model is overly simplistic, then you may have \u003cstrong\u003e\u003cem\u003eunderfit\u003c/em\u003e\u003c/strong\u003e the model, limiting its potential. A visual explanation helps demonstrate this concept in practice:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/fit_fs.png\" width=\"700\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen K is small, any given prediction only takes into account a very small number of points around it to make the prediction. If K is too small, this can end up with a decision boundary that looks like the overfit picture on the right. \u003c/p\u003e\n\n\u003cp\u003eConversely, as K grows larger, it takes into account more and more points, that are farther and farther away from the point in question, increasing the overall size of the region taken into account. If K grows too large, then the model begins to underfit the data. \u003c/p\u003e\n\n\u003cp\u003eIt's important to try to find the best value for K by iterating over a multiple values and comparing performance at each step. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/best_k_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see from the image above, \u003ccode\u003ek=1\u003c/code\u003e and \u003ccode\u003ek=3\u003c/code\u003e will provide different results! \u003c/p\u003e\n\n\u003ch2\u003eIterating over values of K\u003c/h2\u003e\n\n\u003cp\u003eSince the model arrives at a prediction by voting, it makes sense that you should only use odd values for k, to avoid ties and subsequent arbitrary guesswork. By adding this constraint (an odd value for k) the model will never be able to evenly split between two classes. From here, finding an optimal value of K requires some iterative investigation.\u003c/p\u003e\n\n\u003cp\u003eThe best way to find an optimal value for K is to choose a minimum and maximum boundary and try them all! In practice, this means:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eFit a KNN classifier for each value of K \u003c/li\u003e\n\u003cli\u003eGenerate predictions with that model\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate and evaluate a performance metric using the predictions the model made \u003c/li\u003e\n\u003cli\u003eCompare the results for every model and find the one with the lowest overall error, or highest overall score!\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/master/images/plot_fs.png\" width=\"550\"\u003e\u003c/p\u003e\n\n\u003cp\u003eA common way to find the best value for K at a glance is to plot the error for each value of K. Find the value for K where the error is lowest. If this graph continued into higher values of K, we would likely see the error numbers go back up as K increased. \u003c/p\u003e\n\n\u003ch2\u003eKNN and the curse of dimensionality\u003c/h2\u003e\n\n\u003cp\u003eNote that KNN isn't the best choice for extremely large datasets, and/or models with high dimensionality. This is because the time complexity (what computer scientists call \"Big O\", which you saw briefly earlier) of this algorithm is exponential. As you add more data points to the dataset, the number of operations needed to complete all the steps of the algorithm grows exponentially! That said, for smaller datasets, KNN often works surprisingly well, given the simplicity of the overall algorithm. However, if your dataset contains millions of rows and thousands of columns, you may want to choose another algorithm, as the algorithm may not run in any reasonable amount of time;in some cases, it could quite literally take years to complete! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you learned how to determine the best value for K and that the KNN algorithm may not necessarily be the best choice for large datasets due to the large amount of time it can take for the algorithm to run. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-finding-the-best-value-for-k\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-finding-the-best-value-for-k/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"the-kernel-trick","title":"The Kernel Trick","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-kernel-trick\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-kernel-trick/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn how to create SVMs with non-linear decision boundaries data using kernels!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefine the kernel trick and explain why it is important in an SVM model\u003c/li\u003e\n\u003cli\u003eDescribe a radial basis function kernel\u003c/li\u003e\n\u003cli\u003eDescribe a sigmoid kernel\u003c/li\u003e\n\u003cli\u003eDescribe a polynomial kernel\u003c/li\u003e\n\u003cli\u003eDetermine when it is best to use specific kernels within SVM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eNon-linear problems: The Kernel trick\u003c/h2\u003e\n\u003cp\u003eIn the previous lab, you looked at a plot where a linear boundary was clearly not sufficient to separate the two classes cleanly. Another example where a linear boundary would not work well is shown below. How would you draw a max margin classifier here? The intuitive solution is to draw an arc around the circles, separating them from the surrounding diamonds. To generate non-linear boundaries such as this, you use what is known as a kernel.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_nonlin.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003cp\u003eThe idea behind kernel methods is to create (nonlinear) combinations of the original features and project them onto a higher-dimensional space. For example, take a look at how this dataset could be transformed with an appropriate kernel from a two-dimensional dataset onto a new three-dimensional feature space.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-kernel-trick/master/images/new_SVM_kernel.png\" width=\"500\"\u003e\u003c/p\u003e\n\u003ch2\u003eTypes of kernels\u003c/h2\u003e\n\u003cp\u003eThere are several kernels, and an overview can be found in this lesson, as well as in the scikit-learn documentation \u003ca href=\"https://scikit-learn.org/stable/modules/svm.html#kernel-functions\"\u003ehere\u003c/a\u003e. The idea is that kernels are inner products in a transformed space.\u003c/p\u003e\n\u003ch3\u003eThe Linear kernel\u003c/h3\u003e\n\u003cp\u003eThe linear kernel is, as you've seen, the default kernel and simply creates linear decision boundaries. The linear kernel is represented by the inner product of the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Clangle%20x,%20x'%20%5Crangle\"\u003e . It is important to note that some kernels have additional parameters that can be specified and knowing how these parameters work is critical to tuning SVMs.\u003c/p\u003e\n\u003ch3\u003eThe RBF kernel\u003c/h3\u003e\n\u003cp\u003eThere are two parameters when training an SVM with the \u003cem\u003eR\u003c/em\u003eadial \u003cem\u003eB\u003c/em\u003easis \u003cem\u003eF\u003c/em\u003eunction: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e .\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe parameter \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e is common to all SVM kernels. Again, by tuning the \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e parameter when using kernels, you can provide a trade-off between misclassification of the training set and simplicity of the decision function. A high \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will classify as many samples correctly as possible (and might potentially lead to overfitting)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e defines how much influence a single training example has. The larger \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e is, the closer other examples must be to be affected\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe RBF kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cexp%7B(-%5Cgamma%20%5ClVert%20%20x%20-%20%20x'%20%5CrVert%5E2)%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eGamma has a strong effect on the results: a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e that is too large will lead to overfitting, while a \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e which is too small will lead to underfitting (kind of like a simple linear boundary for a complex problem).\u003c/p\u003e\n\u003cp\u003eIn scikit-learn, you can specify a value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=gamma\"\u003e using the parameter \u003ccode\u003egamma\u003c/code\u003e. The default \u003ccode\u003egamma\u003c/code\u003e value is \"auto\", if no other gamma is specified, gamma is set to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=1/%5Ctext%7Bnumber_of_features%7D\"\u003e . You can find more on parameters in the RBF kernel \u003ca href=\"https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eThe Polynomial kernel\u003c/h3\u003e\n\u003cp\u003eThe Polynomial kernel is specified as\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=(%5Cgamma%20%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)%5Ed\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=d\"\u003e can be specified by the parameter \u003ccode\u003edegree\u003c/code\u003e. The default degree is 3.\u003c/li\u003e\n\u003cli\u003e\n\u003cimg src=\"https://render.githubusercontent.com/render/math?math=r\"\u003e can be specified by the parameter \u003ccode\u003ecoef0\u003c/code\u003e. The default is 0.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eThe Sigmoid kernel\u003c/h3\u003e\n\u003cp\u003eThe sigmoid kernel is specified as:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ctanh%20(%20%5Cgamma%5Clangle%20%20x%20-%20%20x'%20%5Crangle%2br)\"\u003e\u003c/p\u003e\n\u003cp\u003eThis kernel is similar to the signoid function in logistic regression.\u003c/p\u003e\n\u003ch2\u003eSome more notes on SVC, NuSVC, and LinearSVC\u003c/h2\u003e\n\u003ch3\u003eNuSVC\u003c/h3\u003e\n\u003cp\u003eNuSVC is similar to SVC, but adds an additional parameter, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e , which controls the number of support vectors and training errors. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cnu\"\u003e jointly creates an upper bound on training errors and a lower bound on support vectors.\u003c/p\u003e\n\u003cp\u003eJust like SVC, NuSVC implements the \"one-against-one\" approach when there are more than 2 classes. This means that when there are n classes, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bn*(n-1)%7D%7B2%7D\"\u003e classifiers are created, and each one classifies samples in 2 classes.\u003c/p\u003e\n\u003ch3\u003eLinearSVC\u003c/h3\u003e\n\u003cp\u003eLinearSVC is similar to SVC, but instead of the \"one-versus-one\" method, a \"one-vs-rest\" method is used. So in this case, when there are \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classes, just \u003cimg src=\"https://render.githubusercontent.com/render/math?math=n\"\u003e classifiers are created, and each one classifies samples in 2 classes, the one of interest, and all the other classes. This means that SVC generates more classifiers, so in cases with many classes, LinearSVC actually tends to scale better.\u003c/p\u003e\n\u003ch2\u003eProbabilities and predictions\u003c/h2\u003e\n\u003cp\u003eYou can make predictions using support vector machines. The SVC decision function gives a probability score per class. However, this is not done by default. You'll need to set the \u003ccode\u003eprobability\u003c/code\u003e argument equal to \u003ccode\u003eTrue\u003c/code\u003e. Scikit-learn internally performs cross-validation to compute the probabilities, so you can expect that setting \u003ccode\u003eprobability\u003c/code\u003e to \u003ccode\u003eTrue\u003c/code\u003e makes the calculations longer. For large datasets, computation can take considerable time to execute.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now have a basic understanding of how to use kernel functions in Support Vector Machines. You'll do just that in the upcoming lab!\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-decision-trees","title":"Introduction to Decision Trees","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-decision-trees\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll take a look at \u003cstrong\u003e\u003cem\u003edecision tree classifiers\u003c/em\u003e\u003c/strong\u003e. These are rule-based classifiers and belong to the first generation of modern AI. Despite the fact that this algorithm has been used in practice for decades, its simplicity and effectiveness for routine classification tasks is still on par with more sophisticated approaches. They are quite common in the business world because they have decent effectiveness without sacrificing explainability. Let's get started!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe a decision tree algorithm in terms of graph architecture\u003c/li\u003e\n\u003cli\u003eDescribe how decision trees are used to create partitions in a sample space\u003c/li\u003e\n\u003cli\u003eDescribe the training and prediction process of a decision tree\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eFrom graphs to decision trees\u003c/h2\u003e\n\u003cp\u003eWe have seen basic classification algorithms (a.k.a classifiers), including Naive Bayes and logistic regression, in earlier sections. A decision tree is a different type of classifier that performs a \u003cstrong\u003erecursive partition of the sample space\u003c/strong\u003e. In this lesson, you will get a conceptual understanding of how this is achieved.\u003c/p\u003e\n\u003cp\u003eA decision tree comprises of decisions that originate from a chosen point in sample space. If you are familiar with Graph theory, a tree is a \u003cstrong\u003edirected acyclic graph with a root called \"root node\" that has no incoming edges\u003c/strong\u003e. All other nodes have one (and only one) incoming edge. Nodes having outgoing edges are known as \u003cstrong\u003einternal\u003c/strong\u003e nodes. All other nodes are called \u003cstrong\u003eleaves\u003c/strong\u003e. Nodes with an incoming edge, but no outgoing edges, are called \u003cstrong\u003eterminal nodes\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eDirected Acyclic Graphs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn computer science and mathematics, a directed graph is a collection of nodes and edges such that edges can be traversed only in a specified direction (eg, from node A to node B, but not from node B to node A). An acyclic graph is a graph such that it is impossible for a node to be visited twice along any path from one node to another. So, a directed acyclic graph (or, a DAG) is a directed graph with no cycles. A DAG has a \u003cstrong\u003etopological ordering\u003c/strong\u003e, or, a sequence of the nodes such that every edge is directed from earlier to later in the sequence.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003ePartitioning the sample space\u003c/h2\u003e\n\u003cp\u003eSo, a decision tree is effectively a DAG, such as the one seen below where \u003cstrong\u003eeach internal node partitions the sample space into two (or more) sub-spaces\u003c/strong\u003e. These nodes are partitioned according to some discrete function that takes the attributes of the sample space as input.\u003c/p\u003e\n\u003cp\u003eIn the simplest and most frequent case, each internal node considers a single attribute so that space is partitioned according to the attribute‚Äôs value. In the case of numeric attributes, the condition refers to a range.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt1.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is the basic idea behind decision trees: every internal node checks for a condition and performs a decision, and every terminal node (AKA leaf node) represents a discrete class. Decision tree induction is closely related to \u003cstrong\u003erule induction\u003c/strong\u003e. In essence, a decision tree is a just series of IF-ELSE statements (rules). Each path from the root of a decision tree to one of its leaves can be transformed into a rule simply by combining the decisions along the path to form the antecedent, and taking the leaf‚Äôs class prediction as the consequence (IF-ELSE statements follow the form: IF \u003cem\u003eantecedent\u003c/em\u003e THEN \u003cem\u003econsequence\u003c/em\u003e ).\u003c/p\u003e\n\u003ch2\u003eDefinition\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA decision tree is a DAG type of classifier where each internal node represents a choice between a number of alternatives and each leaf node represents a classification. An unknown (or test) instance is routed down the tree according to the values of the attributes in the successive nodes. When the instance reaches a leaf, it is classified according to the label assigned to the corresponded leaf.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt2.png\" width=\"850\"\u003e\u003c/p\u003e\n\u003cp\u003eA real dataset would usually have a lot more features than the example above and will create much bigger trees, but the idea will remain exactly the same. The idea of feature importance is crucial to decision trees, since selecting the correct feature to make a split on will affect the complexity and efficacy of the classification process. Regression trees are represented in the same manner, but instead they predict continuous values like the price of a house.\u003c/p\u003e\n\u003ch2\u003eTraining process\u003c/h2\u003e\n\u003cp\u003eThe process of training a decision tree and predicting the target features of a dataset is as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePresent a dataset of training examples containing features/predictors and a target (similar to classifiers we have seen earlier).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain the tree model by making splits for the target using the values of predictors. Which features to use as predictors gets selected following the idea of feature selection and uses measures like \"\u003cstrong\u003einformation gain\u003c/strong\u003e\" and \"\u003cstrong\u003eGini Index\u003c/strong\u003e\". We shall cover these shortly.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe tree is grown until some \u003cstrong\u003estopping criteria\u003c/strong\u003e is achieved. This could be a set depth of the tree or any other similar measure.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eShow a new set of features to the tree, with an unknown class and let the example propagate through a trained tree. The resulting leaf node represents the class prediction for this example datum.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-decision-trees/master/images/dt3.png\" width=\"650\"\u003e\u003c/p\u003e\n\u003ch2\u003eSplitting criteria\u003c/h2\u003e\n\u003cp\u003eThe training process of a decision tree can be generalized as \"\u003cstrong\u003erecursive binary splitting\u003c/strong\u003e\".\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn this procedure, all the features are considered and different split points are tried and tested using some \u003cstrong\u003ecost function\u003c/strong\u003e. The split with the lowest cost is selected.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThere are a couple of algorithms used to build a decision tree:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eCART (Classification and Regression Trees)\u003c/strong\u003e uses the Gini Index as a metric\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eID3 (Iterative Dichotomiser 3)\u003c/strong\u003e uses the entropy function and information gain as metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eGreedy search\u003c/h2\u003e\n\u003cp\u003eWe need to determine the attribute that \u003cstrong\u003ebest\u003c/strong\u003e classifies the training data, and use this attribute at the root of the tree. At each node, we repeat this process creating further splits, until a leaf node is achieved, i.e., all data gets classified.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis means we are performing a top-down, greedy search through the space of possible decision trees.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn order to identify the best attribute for ID3 classification trees, we use the \"information gain\" criteria. Information gain (IG) measures how much \"information\" a feature gives us about the class. Decision trees always try to maximize information gain. So, the attribute with the highest information gain will be split on first.\u003c/p\u003e\n\u003cp\u003eLet's move on to the next lesson where we shall look into these criteria with simple examples.\u003c/p\u003e\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\"\u003eR2D3\u003c/a\u003e: This is highly recommended for getting a visual introduction to decision trees. Excellent animations explaining the training and prediction stages shown above.\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"http://www.dataversity.net/introduction-machine-learning-decision-trees/\"\u003eDataversity: Decision Trees Intro\u003c/a\u003e: A quick and visual introduction to DTs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-dags.html\"\u003eDirected Acyclic Graphs\u003c/a\u003e: This would help relate early understanding of graph computation to decision tree architectures.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we saw an introduction to decision trees as simple yet effective classifiers. We looked at how decision trees partition the sample space based on learning rules from a given dataset. We also looked at how feature selection for splitting the tree is of such high importance. Next, we shall look at information gain criteria used for feature selection.\u003c/p\u003e","frontPage":false},{"exportId":"the-gradient-in-gradient-descent","title":"The Gradient in Gradient Descent","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-the-gradient-in-gradient-descent\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs you know, we entered our discussion of derivatives to determine the size and direction of a step with which to move along a cost curve.  We first used a derivative in a single variable function to see how the output of our cost curve changed with respect to change a change in one of our regression line's variables.  Then we learned about partial derivatives to see how a \u003cem\u003ethree-dimensional cost curve\u003c/em\u003e responded to a change in the regression line.  \u003c/p\u003e\n\n\u003cp\u003eHowever, we have not yet explicitly showed how partial derivatives apply to gradient descent.\u003c/p\u003e\n\n\u003cp\u003eWell, that's what we hope to show in this lesson: explain how we can use partial derivatives to find the path to minimize our cost function, and thus find our \"best fit\" regression line.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine a gradient in relation to gradient descent\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is the gradient?\u003c/h2\u003e\n\n\u003cp\u003eNow gradient descent literally means that we are taking the shortest path to \u003cem\u003edescend\u003c/em\u003e towards our minimum.  However, it is somewhat easier to understand gradient \u003cem\u003eascent\u003c/em\u003e than descent, and the two are quite related, so that's where we'll begin.  Gradient ascent, as you could guess, simply means that we want to move in the direction of steepest ascent.\u003c/p\u003e\n\n\u003cp\u003eNow moving in the direction of greatest ascent for a function \u003cimg class=\"equation_image\" title=\"f(x,y)\" src=\"https://learning.flatironschool.com/equation_images/f(x,y)\" alt=\"{\" data-equation-content=\"f(x,y)\"\u003e, means that our next step is a step some distance in the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction and some distance in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction which is the steepest upward at that point.\u003c/p\u003e\n\n\u003cp\u003eNote how this is a different task from what we have previously worked on for multivariable functions.   So far, we have used partial derivatives to calculate the \u003cstrong\u003egain\u003c/strong\u003e from moving directly in either the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction or the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.  \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHere, in finding gradient ascent, our task is not to calculate the gain from a move in either the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e or \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.  Instead, our task is to \u003cstrong\u003efind some combination of a change in \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e,\u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e that brings the largest change in output\u003c/strong\u003e.  \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eSo if you look at the path our climbers are taking in the picture above, \u003cem\u003ethat\u003c/em\u003e is the direction of gradient ascent.  If they tilt their path to the right or left, they will no longer be moving along the steepest upward path.\u003c/p\u003e\n\n\u003cp\u003eThe direction of the greatest rate of increase of a function is called the gradient.  We denote the gradient with the nabla, which comes from the Greek word for harp, which is kind of what it looks like: \u003cimg class=\"equation_image\" title=\"\\nabla \" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla\" alt=\"{\" data-equation-content=\"\\nabla \"\u003e.  So we can denote the gradient of a function, \u003cimg class=\"equation_image\" title=\"f(x, y)\" src=\"https://learning.flatironschool.com/equation_images/f(x,%20y)\" alt=\"{\" data-equation-content=\"f(x, y)\"\u003e, with \u003cimg class=\"equation_image\" title=\"\\nabla f(x, y) \" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla%20f(x,%20y)\" alt=\"{\" data-equation-content=\"\\nabla f(x, y) \"\u003e.\u003c/p\u003e\n\n\u003ch2\u003eCalculating the gradient\u003c/h2\u003e\n\n\u003cp\u003eNow how do we find the direction for the greatest rate of increase?  We use partial derivatives.  Here's why.\u003c/p\u003e\n\n\u003cp\u003eAs we know, the partial derivative \u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}\"\u003e calculates the change in output from moving a little bit in the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction, and the partial derivative \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}\" src=\"/equation_images/%255Cfrac{df}{dy}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}\"\u003e calculates the change in output from moving in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.  Because with gradient ascent our goal is to make a nudge in \u003cimg class=\"equation_image\" title=\"x, y\" src=\"https://learning.flatironschool.com/equation_images/x,%20y\" alt=\"{\" data-equation-content=\"x, y\"\u003e that produces the greatest change in output, if \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy} \u003e \\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dy}%20\u003e%20%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy} \u003e \\frac{df}{dx}\"\u003e, we should make that move more in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction than the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction, and vice versa.  That is, we want to get the biggest bang for our buck.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/Denali.jpg\" alt=\"hikers on Denali\"\u003e\u003c/p\u003e\n\n\u003cp\u003eLet's relate this again to mountain climbers. Imagine the vertical edge on the left is our y-axis and the horizontal edge is on the bottom is our x-axis.  For the climber in the yellow jacket, imagine his step size is three feet. A step straight along the y-axis will move him further upwards than a step along the x-axis.  So in taking that step, he should direct himself more towards the y-axis than the x-axis.  That will produce a bigger increase per step size.\u003c/p\u003e\n\n\u003cp\u003eIn fact, the direction of greatest ascent for a function,  \u003cimg class=\"equation_image\" title=\"\\nabla f(x, y)\" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla%20f(x,%20y)\" alt=\"{\" data-equation-content=\"\\nabla f(x, y)\"\u003e, is the direction which is a proportion of \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}\" src=\"/equation_images/%255Cfrac{df}{dy}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}\"\u003e steps in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction and \u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}\"\u003e in the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  So, for example, if \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}\" src=\"/equation_images/%255Cfrac{df}{dy}\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}\"\u003e = 5 and \u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}\" src=\"/equation_images/%255Cfrac{df}{dx}\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}\"\u003e = 1, the direction of gradient ascent is five times more in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction than the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  And this seems to be the path, more or less that our climbers are taking - some combination of \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e and \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e, but tilted more towards the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction.\u003c/p\u003e\n\n\u003ch2\u003eApplying Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eNow that we have a better understanding of a gradient, let's apply our understanding to a multivariable function.  Here is a plot of a function:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"f(x,y) = 2x + 3y \" src=\"https://learning.flatironschool.com/equation_images/f(x,y)%20=%202x%20+%203y\" alt=\"{\" data-equation-content=\"f(x,y) = 2x + 3y \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" alt=\"2d plane projected within a 3d plot\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eImagine being at the bottom left of the graph at the point \u003cimg class=\"equation_image\" title=\"x = 1\" src=\"https://learning.flatironschool.com/equation_images/x%20=%201\" alt=\"{\" data-equation-content=\"x = 1\"\u003e, \u003cimg class=\"equation_image\" title=\"y = 1\" src=\"https://learning.flatironschool.com/equation_images/y%20=%201\" alt=\"{\" data-equation-content=\"y = 1\"\u003e.  What would be the direction of steepest ascent?  It seems, just sizing it up visually, that we should move both in the positive \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction and the positive \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  Looking more carefully, it seems we should move \u003cstrong\u003emore\u003c/strong\u003e in the \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e direction than the \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e direction.  Let's see what our technique of taking the partial derivative indicates.   \u003c/p\u003e\n\n\u003cp\u003eThe gradient of the function \u003cimg class=\"equation_image\" title=\"f(x,y)\" src=\"https://learning.flatironschool.com/equation_images/f(x,y)\" alt=\"{\" data-equation-content=\"f(x,y)\"\u003e, that is \u003cimg class=\"equation_image\" title=\" \\nabla f(x,y) = 2x + 3y \" src=\"https://learning.flatironschool.com/equation_images/%20%255Cnabla%20f(x,y)%20=%202x%20+%203y\" alt=\"{\" data-equation-content=\" \\nabla f(x,y) = 2x + 3y \"\u003e is the following: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\frac{df}{dx}(2x + 3y) = 2 \" src=\"/equation_images/%255Cfrac{df}{dx}(2x%20+%203y)%20=%202\" alt=\"{\" data-equation-content=\"\\frac{df}{dx}(2x + 3y) = 2 \"\u003e and \u003cimg class=\"equation_image\" title=\"\\frac{df}{dy}(2x + 3y) = 3 \" src=\"/equation_images/%255Cfrac{df}{dy}(2x%20+%203y)%20=%203\" alt=\"{\" data-equation-content=\"\\frac{df}{dy}(2x + 3y) = 3 \"\u003e.\u003c/p\u003e\n\n\u003cp\u003eSo what this tells us is to move in the direction of greatest ascent for the function \u003cimg class=\"equation_image\" title=\"f(x,y) = 2x + 3y \" src=\"https://learning.flatironschool.com/equation_images/f(x,y)%20=%202x%20+%203y\" alt=\"{\" data-equation-content=\"f(x,y) = 2x + 3y \"\u003e, is to move up three and to the right two.  So we would expect our path of greatest ascent to look like the following.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/gradient-plot.png\" alt=\"line graph plotting y = 1.5x\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-the-gradient-in-gradient-descent/master/images/new_gradDescinDesc.png\" alt=\"2d plane projected within a 3d plot\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSo this path maps up well to what we see visually.  That is the idea behind gradient descent.  The gradient is the partial derivative with respect to each type of variable of a multivariable function, in this case \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e and \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e.  And the importance of the gradient is that its direction is the direction of steepest ascent.  The negative gradient, that is the negative of each of the partial derivatives, is the direction of steepest descent.  So our direction of gradient descent for the graph above is \u003cimg class=\"equation_image\" title=\"x = -2\" src=\"https://learning.flatironschool.com/equation_images/x%20=%20-2\" alt=\"{\" data-equation-content=\"x = -2\"\u003e, \u003cimg class=\"equation_image\" title=\"y = -3\" src=\"https://learning.flatironschool.com/equation_images/y%20=%20-3\" alt=\"{\" data-equation-content=\"y = -3\"\u003e.  And looking at the two graphs above, it seems that the steepest downward direction is just the opposite of the steepest upward direction.  We get that by mathematically by simply taking the multiplying our partial derivatives by negative one.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you saw how to use gradient descent to find the direction of steepest descent.  You saw that the direction of steepest descent is generally some combination of a change in your variables to produce the greatest negative rate of change.  \u003c/p\u003e\n\n\u003cp\u003eYou first how saw how to calculate the gradient \u003cstrong\u003eascent\u003c/strong\u003e, or the gradient \u003cimg class=\"equation_image\" title=\"\\nabla \" src=\"https://learning.flatironschool.com/equation_images/%255Cnabla\" alt=\"{\" data-equation-content=\"\\nabla \"\u003e, by calculating the partial derivative of a function with respect to the variables of the function.  So \u003cimg class=\"equation_image\" title=\"\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} \" src=\"/equation_images/%255Cnabla%20f(x,%20y)%20=%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y},%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}\" alt=\"{\" data-equation-content=\"\\nabla f(x, y) = \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta x} \"\u003e.  This means that to take the path of greatest ascent, you should move \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta y} \" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y}\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta y} \"\u003e divided by \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta x} \" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta x} \"\u003e.  So for example, when \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta y}f(x, y)  = 3 \" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y}f(x,%20y)%20%20=%203\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta y}f(x, y)  = 3 \"\u003e , and \u003cimg class=\"equation_image\" title=\" \\frac{\\delta f}{\\delta x}f(x, y)  = 2\" src=\"/equation_images/%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}f(x,%20y)%20%20=%202\" alt=\"{\" data-equation-content=\" \\frac{\\delta f}{\\delta x}f(x, y)  = 2\"\u003e, you traveled in line with a slope of 3/2.\u003c/p\u003e\n\n\u003cp\u003eFor gradient descent, that is to find the direction of greatest decrease, you simply reverse the direction of your partial derivatives and move in \u003cimg class=\"equation_image\" title=\" - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}\" src=\"/equation_images/%20-%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20y},%20-%20%255Cfrac{%255Cdelta%20f}{%255Cdelta%20x}\" alt=\"{\" data-equation-content=\" - \\frac{\\delta f}{\\delta y}, - \\frac{\\delta f}{\\delta x}\"\u003e. \u003c/p\u003e","frontPage":false},{"exportId":"phase-3-project-choosing-a-dataset","title":"Phase 3 Project - Choosing a Dataset","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-3-choosing-a-dataset\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003cp\u003eYou have the option to either choose a dataset from a curated list or propose your own dataset not on the list. The goal is to choose a dataset appropriate to the type of business problem and/or classification methods that most interests you. \u003cstrong\u003eIt is up to you to define a stakeholder and business problem appropriate to the dataset you choose.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eIf you choose a dataset from the curated list, inform your instructor which dataset you chose and jump right into the project. If you would like to propose your own dataset, run the dataset and business problem by your instructor for approval before starting your project.\u003c/p\u003e\n\n\u003ch2\u003eYour Get Hired 'Game Plan'\u003c/h2\u003e\n\n\u003cp\u003eHelp set yourself up for success by being strategic about your project/dataset choices.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eAlready know what your job search focus will be?\u003c/strong\u003e Consider choosing a dataset that relates to the companies/industries you are interested in and the types of business problems/data they navigate day to day. Doing so demonstrates your subject matter knowledge in their area, significantly elevating your relevance and value as a candidate -- we've seen this strategy WOW companies time and time again!\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStill exploring what type of role you would like to get once you graduate?\u003c/strong\u003e That's okay! Try to focus on a topic or problem that you are interested in and passionate about. Doing so will help you produce a better project overall that you enjoy creating and that you can speak about confidently and naturally.\u003c/p\u003e\n\n\u003cp\u003eComing out of Flatiron School your projects will be listed on your resume and will showcase your specific subject matter knowledge and interest/passions once you're job seeking. Help yourself put your best foot forward and make the strongest first impression possible.\u003c/p\u003e\n\n\u003cp\u003eHere are two grads who successfully did just this...\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThis student was interested in working with government and public sector data, and focused specifically on traffic data and safety. They utilized the Chicago Car Crashes dataset in \u003ca href=\"https://github.com/jmarkowi/Chicago-Crashes\"\u003eone project\u003c/a\u003e* then later created a bike lane image dataset from multiple sources for their \u003ca href=\"https://github.com/jmarkowi/NYC_bike_lanes\"\u003ecapstone project\u003c/a\u003e*. Based on their combination of technical skills and subject-matter expertise, this student landed a government consulting role at \u003cstrong\u003e\u003cem\u003eASR Analytics\u003c/em\u003e\u003c/strong\u003e where they work to prevent identity theft in tax fraud.\u003c/p\u003e\n\n\u003cp\u003eThis student (\u003ca href=\"https://github.com/kbaranko/NYC-Building-Energy-Intensity/blob/master/README.md\"\u003eGitHub link here\u003c/a\u003e*) focused on working in the clean energy sector, and created their project \u003cem\u003eNYC Building Energy Density\u003c/em\u003e using data from the 2016 Energy and Water Data Disclosure for New York City Local Law 84. The student landed a role at \u003cstrong\u003e\u003cem\u003eKevala\u003c/em\u003e\u003c/strong\u003e, a clean energy software company, in under two months of job seeking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e*\u003cem\u003eKeep in mind that the Flatiron School Data Science program has changed over time, so these projects may or may not reflect the current project requirements. They are intended as inspiration for your dataset/project choice.\u003c/em\u003e\u003c/p\u003e\n\n\u003ch2\u003eCurated List of Datasets\u003c/h2\u003e\n\n\u003cp\u003eYou may select any of the datasets below - we provide brief descriptions of each. Follow the links to learn more about the dataset and business problems before making a final decision.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eIf you are feeling overwhelmed or behind, we recommend you choose dataset #1.\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch3\u003e1) \u003ca href=\"https://www.kaggle.com/becksddf/churn-in-telecoms-dataset\"\u003eSyriaTel Customer Churn\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eBuild a classifier to predict whether a customer will (\"soon\") stop doing business with SyriaTel, a telecommunications company. This is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem.\u003c/p\u003e\n\n\u003cp\u003eMost naturally, your audience here would be the telecom business itself, interested in reducing how much money is lost because of customers who don't stick around very long. The question you can ask is: are there any predictable patterns here?\u003c/p\u003e\n\n\u003ch3\u003e2) \u003ca href=\"https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/\"\u003eTanzanian Water Wells\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eThis dataset is part of an active competition until April 7, 2023!\u003c/p\u003e\n\n\u003cp\u003eTanzania, as a developing country, struggles with providing clean water to its population of over 57,000,000. There are many water points already established in the country, but some are in need of repair while others have failed altogether.\u003c/p\u003e\n\n\u003cp\u003eBuild a classifier to predict the condition of a water well, using information about the sort of pump, when it was installed, etc. Your audience could be an NGO focused on locating wells needing repair, or the Government of Tanzania looking to find patterns in non-functional wells to influence how new wells are built. Note that this is a \u003cstrong\u003eternary\u003c/strong\u003e classification problem by default, but can be engineered to be binary.\u003c/p\u003e\n\n\u003ch3\u003e3) \u003ca href=\"https://www.drivendata.org/competitions/66/flu-shot-learning/\"\u003eH1N1 and Seasonal Flu Vaccines\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eThis dataset is part of an active competition until March 31, 2022!\u003c/p\u003e\n\n\u003cp\u003eAs the world struggles to vaccinate the global population against COVID-19, an understanding of how people‚Äôs backgrounds, opinions, and health behaviors are related to their personal vaccination patterns can provide guidance for future public health efforts. Your audience could be someone guiding those public health efforts.\u003c/p\u003e\n\n\u003cp\u003eThis challenge: can you predict whether people got H1N1 and seasonal flu vaccines using data collected in the National 2009 H1N1 Flu Survey? This is a \u003cstrong\u003ebinary\u003c/strong\u003e classification problem, but there are two potential targets: whether the survey respondent received the seasonal flu vaccine, or whether the respondent received the H1N1 flu vaccine. Please choose just one of these potential targets for your minimum viable project.\u003c/p\u003e\n\n\u003ch3\u003e4) \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if\"\u003eChicago Car Crashes\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eNote this links also to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-Vehicles/68nd-jvt3\"\u003eVehicle Data\u003c/a\u003e and to \u003ca href=\"https://data.cityofchicago.org/Transportation/Traffic-Crashes-People/u6pd-qa9d\"\u003eDriver/Passenger Data\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eBuild a classifier to predict the primary contributory cause of a car accident, given information about the car, the people in the car, the road conditions etc. You might imagine your audience as a Vehicle Safety Board who's interested in reducing traffic accidents, or as the City of Chicago who's interested in becoming aware of any interesting patterns. \u003c/p\u003e\n\n\u003cp\u003eThis is a \u003cstrong\u003emulti-class\u003c/strong\u003e classification problem. You will almost certainly want to bin, trim or otherwise limit the number of target categories on which you ultimately predict. Note that some primary contributory causes have very few samples, for example.\u003c/p\u003e\n\n\u003ch3\u003e5) \u003ca href=\"https://data.seattle.gov/Public-Safety/Terry-Stops/28ny-9ts8\"\u003eTerry Traffic Stops\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eIn \u003ca href=\"https://www.oyez.org/cases/1967/67\"\u003eTerry v. Ohio\u003c/a\u003e, a landmark Supreme Court case in 1967-8, the court found that a police officer was not in violation of the \"unreasonable search and seizure\" clause of the Fourth Amendment, even though he stopped and frisked a couple of suspects only because their behavior was suspicious. Thus was born the notion of \"reasonable suspicion\", according to which an agent of the police may e.g. temporarily detain a person, even in the absence of clearer evidence that would be required for full-blown arrests etc. Terry Stops are stops made of suspicious drivers.\u003c/p\u003e\n\n\u003cp\u003eBuild a classifier to predict whether an arrest was made after a Terry Stop, given information about the presence of weapons, the time of day of the call, etc. This is a binary classification problem.\u003c/p\u003e\n\n\u003cp\u003eNote that this dataset also includes information about gender and race. You may use this data as well. You could conceivably pitch your project as an inquiry into whether race (of officer or of subject) plays a role in whether or not an arrest is made.\u003c/p\u003e\n\n\u003cp\u003eIf you do elect to make use of race or gender data, be aware that this can make your project a highly sensitive one; your discretion will be important, as well as your transparency about how you use the data and the ethical issues surrounding it.\u003c/p\u003e\n\n\u003ch2\u003eProposing Your Own Dataset\u003c/h2\u003e\n\n\u003cp\u003eSourcing new data is a valuable skill for data scientists, but it requires a great deal of care. An inappropriate dataset or an unclear business problem can lead you to spend a lot of time on a project that delivers underwhelming results. The guidelines below will help you complete a project that demonstrates your ability to engage in the full data science process.\u003c/p\u003e\n\n\u003cp\u003eOnce you've sourced your own dataset and identified the business problem you want to solve with it, \u003cstrong\u003eyou must run them by your instructor for approval.\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch3\u003eData Guidelines\u003c/h3\u003e\n\n\u003cp\u003eYour dataset must be:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eAppropriate for classification.\u003c/strong\u003e It should have a categorical outcome or the data needed to engineer one.   \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eUsable to solve a specific business problem.\u003c/strong\u003e This solution must rely on your classification model.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eSomewhat complex.\u003c/strong\u003e It should contain a minimum of 1000 rows and 10 features.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eUnfamiliar.\u003c/strong\u003e It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. Titanic).\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eManageable.\u003c/strong\u003e Stick to datasets that you can model using the techniques introduced in Phase 3.\u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003eProblem First, or Data First?\u003c/h3\u003e\n\n\u003cp\u003eThere are two ways that you can source your own dataset: \u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e. The less time you have to complete the project, the more strongly we recommend a Data First approach to this project.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProblem First\u003c/em\u003e\u003c/strong\u003e: Start with a problem that you are interested in that you could potentially solve with a classification model. Then look for data that you could use to solve that problem. This approach is high-risk, high-reward: Very rewarding if you are able to solve a problem you are invested in, but frustrating if you end up sinking lots of time in without finding appropriate data. To mitigate the risk, set a firm limit for the amount of time you will allow yourself to look for data before moving on to the Data First approach.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eData First\u003c/em\u003e\u003c/strong\u003e: Take a look at some of the most popular internet repositories of cool data sets we've listed below. If you find a data set that's particularly interesting for you, then it's totally okay to build your problem around that data set.\u003c/p\u003e\n\n\u003ch3\u003ePotential Data Sources\u003c/h3\u003e\n\n\u003cp\u003eThere are plenty of amazing places that you can get your data from. We recommend you start looking at data sets in some of these resources first:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://archive.ics.uci.edu/ml/datasets.php\"\u003eUCI Machine Learning Datasets Repository\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/datasets\"\u003eKaggle Datasets\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/awesomedata/awesome-public-datasets\"\u003eAwesome Datasets Repo on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLocal data portals for state and local government resources\n\n\u003cul\u003e\n\u003cli\u003eExamples: \u003ca href=\"https://opendata.cityofnewyork.us/\"\u003eNYC\u003c/a\u003e, \u003ca href=\"http://data.houstontx.gov/\"\u003eHouston\u003c/a\u003e, \u003ca href=\"https://data.seattle.gov/\"\u003eSeattle\u003c/a\u003e, \u003ca href=\"https://data.ca.gov/\"\u003eCalifornia\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://insideairbnb.com/\"\u003eInside AirBNB\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://data.fivethirtyeight.com/\"\u003eFiveThirtyEight‚Äôs data portal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\"\u003eData is Plural‚Äôs Archive Spreadsheet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.reddit.com/r/datasets/\"\u003eDatasets Subreddit\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"k-nearest-neighbors-recap","title":"K-Nearest Neighbors - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you'll briefly review some of the key concepts covered in this section including KNN's computational complexity and how to properly tune a model using scikit-learn. \u003c/p\u003e\n\n\u003ch2\u003eK-Nearest Neighbors\u003c/h2\u003e\n\n\u003cp\u003eAs you saw, KNN is an intuitive algorithm: to generate a prediction for a given data point, it finds the k-nearest data points and then predicts the majority class of these k points.\u003c/p\u003e\n\n\u003ch3\u003eComputational complexity\u003c/h3\u003e\n\n\u003cp\u003eAlso of note is the computational complexity of the KNN algorithm. As the number of data points and features increase, the required calculations increases exponentially! As such, KNN is extremely resource intensive for large datasets.\u003c/p\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eYou learned about Minkowski distance and two cases of Minkowski distance: Euclidean and Manhattan distance. Other distance metrics such as Hamming distance can even be used to compare strings! (Hamming distance can be used to offer typo correction-suggestions for instance by comparing similar words generated by changing only one or two letters from the mistyped word). \u003c/p\u003e\n\n\u003ch2\u003eModel tuning in scikit-learn\u003c/h2\u003e\n\n\u003cp\u003eRemember that model tuning encapsulates the entire gamut of the data science process from problem formulation and preprocessing through hyperparameter tuning. Furthermore, you also need to choose a validation method to determine the model's ability to generalize to new cases such as train-test split or cross-validation. Good models require careful thought, ample preprocessing, and exploration followed by hyperparameter tuning.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWell done! You have added another algorithm in your toolset. Even though KNN doesn't scale well to larger datasets, it has many useful applications from recommendations to classification. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"ensemble-methods","title":"Ensemble Methods","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about \u003cstrong\u003e\u003cem\u003eensembles\u003c/em\u003e\u003c/strong\u003e and why they're such an effective technique for supervised learning. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain what is meant by \"ensemble methods\"\u003c/li\u003e\n\u003cli\u003eExplain the concept of bagging as it applies to ensemble methods \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat are ensembles?\u003c/h2\u003e\n\n\u003cp\u003eIn Data Science, the term \u003cstrong\u003e\u003cem\u003eensemble\u003c/em\u003e\u003c/strong\u003e refers to an algorithm that makes use of more than one model to make a prediction. Typically, when people talk about ensembles, they are referring to Supervised Learning, although there has been some ongoing research on using ensembles for unsupervised learning tasks. Ensemble methods are typically more effective when compared with single-model results for supervised learning tasks. Most Kaggle competitions are won using ensemble methods, and \u003ca href=\"https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/\"\u003emuch has been written\u003c/a\u003e about why they tend to be so successful for these tasks. \u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eConsider the following scenario -- you are looking to invest in a company, and you want to know if that company's stock will go up or down in the next year. Instead of just asking a single person, you have the following experts available to you:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eStock Broker\u003c/em\u003e\u003c/strong\u003e: This person makes correct predictions 80% of the time\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eFinance Professor\u003c/em\u003e\u003c/strong\u003e: This person is correct 65% of the time\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eInvestment Expert\u003c/em\u003e\u003c/strong\u003e: This person is correct 85% of the time\u003cbr\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIf we could only take advice from one person, we would pick the Investment Expert, and we can only be 85% sure that they are right.  \u003c/p\u003e\n\n\u003cp\u003eHowever, if we can use all three, we can combine their knowledge to increase our overall accuracy. If they all agree that the stock is a good investment, what is the overall accuracy of the combined prediction?\u003c/p\u003e\n\n\u003cp\u003eWe can calculate this by multiplying the chances that each of them are wrong together, which is \u003cimg class=\"equation_image\" title=\" 0.2 * 0.35 * 0.15 = 0.0105\\ error\\ rate\" src=\"https://learning.flatironschool.com/equation_images/%200.2%20*%200.35%20*%200.15%20=%200.0105%255C%20error%255C%20rate\" alt=\"{\" data-equation-content=\" 0.2 * 0.35 * 0.15 = 0.0105\\ error\\ rate\"\u003e, which means that our combined accuracy is \u003cimg class=\"equation_image\" title=\"1 - 0.0105 = 0.9895\" src=\"https://learning.flatironschool.com/equation_images/1%20-%200.0105%20=%200.9895\" alt=\"{\" data-equation-content=\"1 - 0.0105 = 0.9895\"\u003e, or \u003cstrong\u003e\u003cem\u003e98.95%\u003c/em\u003e\u003c/strong\u003e!  \u003c/p\u003e\n\n\u003cp\u003eObviously, this analogy is a bit of an oversimplification -- we're assuming that each prediction is independent, which is unlikely in the real world since there's likely some overlap between the things each person is using to make their prediction. We also haven't calculated the accuracy percentages for the cases where they disagree. However, the main point of this example is that when we combine predictions, we get better overall results. \u003c/p\u003e\n\n\u003ch2\u003eResiliency to variance\u003c/h2\u003e\n\n\u003cp\u003eEnsemble methods are analogous to \"Wisdom of the crowd\". This phrase refers to the phenomenon that the average estimate of all predictions typically outperforms any single prediction by a statistically significant margin -- often, quite a large one.  A Finance Professor named Jack Treynor once demonstrated this with the classic jar full of jellybeans. Professor Treynor asked all 850 of his students to guess the number of jellybeans in the jar. When he averaged the guesses, he found that of all the guesses in the class, only one student had guessed a better estimate than the group average. \u003c/p\u003e\n\n\u003cp\u003eThink back to what you've learned about sampling, inferential statistics, and the Central Limit theorem. The same magic is at work here. Estimators are rarely perfect. When Professor Treynor asked each student to provide an estimate of the number of jellybeans in the jar, he found that the estimates were normally distributed. This is where \"Wisdom of the crowd\" kicks in because we can expect the number of people who underestimate the number of jellybeans in the jar to be roughly equal to the number of people who overestimate the number of jellybeans. So we can safely assume the extra variance above and below the average essentially cancel each other out, leaving our average close to the ground truth value! \u003c/p\u003e\n\n\u003cp\u003eConsider the top-right example in this graphic that visually demonstrates high variance:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bias-and-variance.png\" alt=\"four targets showing low and high variance on the x-axis and lot and high bias on the y-axis\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eMost points miss the bullseye, but they are just as likely to miss in any direction. If we averaged all of these points, we would be extremely close to the bullseye! This is a great analogy for how ensemble methods work so well -- we know that no model is likely to make perfect estimates, so we have many of them make predictions, and average them, knowing that the overestimates and the underestimates will likely cancel out to be very close to the ground truth. The idea that the overestimates and underestimates will (at least partially) cancel each other out is sometimes referred to as \u003cstrong\u003e\u003cem\u003esmoothing\u003c/em\u003e\u003c/strong\u003e.  \u003c/p\u003e\n\n\u003ch3\u003eWhich models are used in ensembles?\u003c/h3\u003e\n\n\u003cp\u003eFor this section, we'll be focusing exclusively on tree-based ensemble methods, such as \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient boosted trees\u003c/em\u003e\u003c/strong\u003e. However, we can technically use any models in an ensemble! It's not uncommon to see \u003cstrong\u003e\u003cem\u003eModel stacking\u003c/em\u003e\u003c/strong\u003e, also called \u003cstrong\u003e\u003cem\u003eMeta-ensembling\u003c/em\u003e\u003c/strong\u003e, where multiple different models are stacked, and their predictions are aggregated. In this case, the more different the models are, the better! This is because the more different the models are, the more likely they have the potential to pick up on different characteristics of the data. It's not uncommon to see ensembles consisting of multiple logistic regressions, Naive Bayes classifiers, Tree-based models (including ensembles such as random forests), and even deep neural networks!  \u003c/p\u003e\n\n\u003cp\u003eFor a much more in-depth explanation of what model stacking looks like and why it is effective, take a look at this great \u003ca href=\"http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/\"\u003earticle from Kaggle's blog, No Free Hunch!\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eBootstrap aggregation\u003c/h2\u003e\n\n\u003cp\u003eThe main concept that makes ensembling possible is \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e. Bootstrap aggregation is itself a combination of two ideas -- bootstrap resampling and aggregation. You're already familiar with bootstrap resampling from our section on the Central Limit theorem. It refers to the subsets of your dataset by sampling with replacement, much as we did to calculate our sample means when working with the Central Limit theorem. Aggregation is exactly as it sounds -- the practice of combining all the different estimates to arrive at a single estimate -- although the specifics for how we combine them are up to us. A common approach is to treat each classifier in the ensemble's prediction as a \"vote\" and let our overall prediction be the majority vote.  It's also common to see ensembles that take the arithmetic mean of all predictions, or compute a weighted average. \u003c/p\u003e\n\n\u003cp\u003eThe process for training an ensemble through bootstrap aggregation is as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eGrab a sizable sample from your dataset, with replacement \u003c/li\u003e\n\u003cli\u003eTrain a classifier on this sample\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eRepeat until all classifiers have been trained on their own sample from the dataset\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eWhen making a prediction, have each classifier in the ensemble make a prediction \u003c/li\u003e\n\u003cli\u003eAggregate all predictions from all classifiers into a single prediction, using the method of your choice\u003cbr\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-ensemble-methods/master/images/new_bagging.png\" alt=\"flowchart of input sample being split into several bootstrap samples, then building several decision trees, then aggregation\"\u003e\u003c/p\u003e\n\n\u003cp\u003eDecision trees are often used because they are very sensitive to variance. On their own, this is a weakness. However, when aggregated together into an ensemble, this actually becomes a good thing!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what constitutes an \u003cstrong\u003eEnsemble\u003c/strong\u003e, and how \u003cstrong\u003eBagging\u003c/strong\u003e plays a central role in this. In the next lesson, we'll see how bagging is combined with another important technique to create one of the most effective ensemble algorithms available today -- \u003cstrong\u003e\u003cem\u003eRandom forests\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e","frontPage":false},{"exportId":"short-video-matrix-multiplication","title":"Short Video: Matrix Multiplication","type":"WikiPage","content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802388?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"ensembles-recap","title":"Ensembles - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ensemble-methods-section-recap\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMultiple independent estimates are consistently more accurate than any single estimate, so ensemble techniques are a powerful way for improving the quality of your models\u003c/li\u003e\n\u003cli\u003eSometimes you'll use model stacking or meta-ensembles where you use a combination of different types of models for your ensemble\u003c/li\u003e\n\u003cli\u003eIt's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\u003c/li\u003e\n\u003cli\u003eBagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\u003c/li\u003e\n\u003cli\u003eBootstrap resampling uses multiple smaller samples from the test dataset to create independent estimates, and aggregate these estimates to make predictions\u003c/li\u003e\n\u003cli\u003eA random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling method to create variance among the trees\u003c/li\u003e\n\u003cli\u003eWith a random forest, for each tree, we sample two-thirds of the training data and the remaining third is used to calculate the out-of-bag error\u003c/li\u003e\n\u003cli\u003eIn addition, the Subspace Sampling method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eGridsearchCV\u003c/code\u003e is an exhaustive search technique for finding optimal combinations of hyperparameters\u003c/li\u003e\n\u003cli\u003eBoosting leverages an ensemble of weak learners (weak models) to create a strong combined model\u003c/li\u003e\n\u003cli\u003eBoosting (when compared to random forests) is an iterative rather than independent process, using each iteration to strengthen the weaknesses of the previous iterations\u003c/li\u003e\n\u003cli\u003eTwo of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\u003c/li\u003e\n\u003cli\u003eAdaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\u003c/li\u003e\n\u003cli\u003eGradient Boosting is a more advanced boosting algorithm that makes use of Gradient Descent\u003c/li\u003e\n\u003cli\u003eXGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"classification-metrics-recap","title":"Classification Metrics - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classification-metrics-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you learned about the different ways to evaluate classification models such as logistic regression.\u003c/p\u003e\n\n\u003ch2\u003eClassification Metrics\u003c/h2\u003e\n\n\u003cp\u003eWhile precision, recall, and accuracy are useful metrics for evaluating classifiers, determining an appropriate balance between false positives and false negatives will depend on the particular problem application and the relative costs of each. For example, in the context of medical screening, a false negative could be devastating, eliminating the possibility for early intervention of the given disease. On the other hand, in another context, such as finding spam email, the cost of false positives might be much higher than false negatives -- after all, having a spam email sneak its way into your inbox is probably preferable then missing an important time-sensitive email because it was marked as spam. Due to these contextual considerations, you as the practitioner are responsible for selecting appropriate trade-offs.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eYou can evaluate logistic regression models using some combination of precision, recall, and accuracy\u003c/li\u003e\n\u003cli\u003eA confusion matrix is another common way to visualize the performance of a classification model\u003c/li\u003e\n\u003cli\u003eReceiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) can be used to help determine the best precision-recall trade-off for a given classifier\u003c/li\u003e\n\u003cli\u003eClass weights, under/oversampling, and SMOTE can be used to deal with class imbalance problems\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"linear-algebra-and-calculus-introduction","title":"Linear Algebra and Calculus - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-algebra-and-calculus-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to take a step back to learn some of the basics of the math that powers the most popular machine learning models. You may not need deep knowledge of linear algebra and calculus just to build a scikit-learn model, but this introduction should give you a better understanding of how your models are working \"under the hood\".\u003c/p\u003e\n\n\u003ch2\u003eLinear Algebra\u003c/h2\u003e\n\n\u003cp\u003eLinear Algebra is so foundational to machine learning that you're going to see it referenced many times as the course progresses. In this section, the goal is to give you both a theoretical introduction and some computational practice, solving a real-life problem by writing the code required to solve a linear regression using OLS.\u003c/p\u003e\n\n\u003cp\u003eWe're going to kick this section off by looking at some of the many places that linear algebra is used in machine learning - from deep learning to natural language processing (NLP) to dimensionality reduction techniques such as principal component analysis (PCA).\u003c/p\u003e\n\n\u003ch3\u003eSystems of Linear Equations\u003c/h3\u003e\n\n\u003cp\u003eWe then start to dig into the math! We look at the idea of linear simultaneous equations - a set of two or more equations each of which is linear (can be plotted on a graph as a straight line). We then see how such equations can be represented as vectors or matrices to represent such systems efficiently.\u003c/p\u003e\n\n\u003ch3\u003eScalars, Vectors, Matrices, and Tensors\u003c/h3\u003e\n\n\u003cp\u003eIn a code along, we'll introduce the concepts and concrete representations (in NumPy) of scalars, vectors, matrices, and tensors - why they are important and how to create them. \u003c/p\u003e\n\n\u003ch3\u003eVector/Matrix Operations\u003c/h3\u003e\n\n\u003cp\u003eWe then start to build up the basic skills required to perform matrix operations such as addition and multiplication.  You will also cover key techniques used by many machine learning models to perform their calculations covering both the Hadamard product and the (more common) dot product. \u003c/p\u003e\n\n\u003ch3\u003eSolving Systems of Linear Equations Using NumPy\u003c/h3\u003e\n\n\u003cp\u003eWe then bring the previous work together to look at how to use NumPy to solve systems of linear equations, introducing the identity and inverse matrices along the way.\u003c/p\u003e\n\n\u003ch3\u003eRegression Analysis Using Linear Algebra and NumPy\u003c/h3\u003e\n\n\u003cp\u003eHaving built up a basic mathematical and computational foundation for linear algebra, you will solve a real data problem - looking at how to use NumPy to solve a linear regression using the ordinary least squares (OLS) method.\u003c/p\u003e\n\n\u003ch3\u003eComputational Complexity\u003c/h3\u003e\n\n\u003cp\u003eIn the last linear algebra lesson, we look at the idea of computational complexity and Big O notation, showing why OLS is computationally inefficient, and that a gradient descent algorithm can instead be used to solve a linear regression much more efficiently.\u003c/p\u003e\n\n\u003ch2\u003eCalculus and Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eNext, you'll learn about the mechanism behind many machine learning optimization algorithms: gradient descent! Along the way, we'll also look at cost functions and will provide a foundation in calculus that will be valuable to you throughout your career as a data scientist.\u003c/p\u003e\n\n\u003cp\u003eJust as we used solving a linear regression using OLS as an excuse to introduce you to linear algebra, we're now using the idea of gradient descent to introduce enough calculus to both understand and have good intuitions about many of the machine learning models that you're going to learn throughout the rest of the course.\u003c/p\u003e\n\n\u003ch3\u003eAn Introduction to Calculus and Derivatives\u003c/h3\u003e\n\n\u003cp\u003eWe're going to start off by introducing derivatives - the \"instantaneous rate of change of a function\" or (more graphically) the \"slope of a curve\". We'll start off by looking at how to calculate the slope of a curve for a straight line, and then we'll explore how to calculate the rate of change for more complex (non-linear) functions.\u003c/p\u003e\n\n\u003ch3\u003eGradient Descent\u003c/h3\u003e\n\n\u003cp\u003eNow that we know how to calculate the slope of a curve - and, by extension, to find a local minimum (low point) or maximum (high point) where the curve is flat (the slope of the curve is zero), we'll look at the idea of a gradient descent to step from some random point on a cost curve to find the local optima to solve for a given linear equation. We'll also look at how best to select the step sizes for descending the cost function, and how to use partial derivatives to optimize both slope and offset to more effectively solve a linear regression using gradient descent.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIt is possible to use machine learning models such as scikit-learn without understanding the underlying math, but a basic understanding of this math will help you to make the most informed choices about which models to use based on your particular project context.\u003c/p\u003e","frontPage":false},{"exportId":"decision-trees-introduction","title":"Decision Trees - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we're going to introduce another kind of model for predicting values that can be used for both continuous and categorical predictions - decision trees. Decision trees are used to classify (or estimate continuous values) by partitioning the sample space as efficiently as possible into sets with similar data points until you get to (or close to) a homogenous set and can reasonably predict the value for new data points. \u003c/p\u003e\n\n\u003cp\u003eDespite the fact that they've been around for decades, they are still (in conjunction with ensemble methods that we'll learn about in the next section) one of the most powerful modeling tools available in the field of machine learning. They are also highly interpretable when compared to more complex models (they're simple to explain and it's easy to understand how they make their decisions).\u003c/p\u003e\n\n\u003ch3\u003eEntropy and Information Gain\u003c/h3\u003e\n\n\u003cp\u003eDue to the nature of decision trees, you can get very different predictions depending on what questions you ask and in what order. The question then is how to come up with the right questions to ask in the right order. In this section, we also introduce the idea of entropy and information gain as mechanisms for selecting the most promising questions to ask in a decision tree.\u003c/p\u003e\n\n\u003ch3\u003eID3 Classification Trees\u003c/h3\u003e\n\n\u003cp\u003eWe also talk about Ross Quinlan's ID3 (Iterative Dichotomiser 3) algorithm for generating a decision tree from a dataset. \u003c/p\u003e\n\n\u003ch3\u003eBuilding Trees using Scikit-learn\u003c/h3\u003e\n\n\u003cp\u003eNext up, we look at how to build a decision tree using the built-in functions available in scikit-learn, and how to test the accuracy of the predictions using a simple accuracy measure, AUC, and a confusion matrix. We also show how to use the \u003ccode\u003egraph_viz\u003c/code\u003e library to generate a visualization of the resulting decision tree.\u003c/p\u003e\n\n\u003ch3\u003eHyperparameter Tuning and Pruning\u003c/h3\u003e\n\n\u003cp\u003eWe then look at some of the hyperparameters available when optimizing a decision tree. For example, if you're not careful, generated decision trees can lead to overfitting of data (wherein a model is a perfect match for training data, but horrible for test data). There are a number of hyperparameters you can use when generating a tree to minimize overfitting such as maximum depth or minimum leaf sample size. We look at these various \"pruning\" strategies to avoid overfitting of the data and to create a better model. \u003c/p\u003e\n\n\u003ch3\u003eRegression with CART Trees\u003c/h3\u003e\n\n\u003cp\u003eIn addition to building decision tree classifiers, you will also build decision trees for regression problems. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eDecision trees are highly effective and interpretable. This section will provide you with the skills to create both classifiers and to perform regression using decision trees and to use hyperparameter tuning to optimize your model.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"logistic-regression-introduction","title":"Logistic Regression - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-intro-v2-4\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro-v2-4\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-intro-v2-4/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this you'll be introduced to a new type of machine learning technique: classification! You'll learn about an algorithm called logistic regression and how it can be seen through a statistical point of view with maximum likelihood estimation (MLE). This should provide you some additional time to wrangle with statistical concepts and improve your overall coding abilities.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eYou're familiar with linear regression to predict continuous values. You're now going to return to regression to look at how it can be used as a classifier instead to determine the likelihood of a given data point being associated with one of two categories.\u003c/p\u003e\n\n\u003cp\u003eWe'll start by introducing the sigmoid function and showing how it can be used to fit a curve that matches a binary classifier (e.g. does someone make over or under $40k a year or are they a good or bad credit risk).\u003c/p\u003e\n\n\u003ch2\u003eMaximum Likelihood Estimation (MLE)\u003c/h2\u003e\n\n\u003cp\u003eMaximum likelihood estimation is a statistical procedure for determining underlying parameter distributions. As the name implies, the underlying motivation is to find parameters that maximize the theoretical chances of observing the actual observations.\u003c/p\u003e\n\n\u003ch2\u003eMLE and Logistic Regression\u003c/h2\u003e\n\n\u003cp\u003eLogistic regression, despite its name, is a classification algorithm. An interesting nuance is that it provides confidence values with its predictions since the raw output is a probability of a class between 0 and 1. The general process for this is similar to linear regression, where coefficients for various feature weights are altered in order to optimize the accuracy of subsequent predictions from the model.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIt's important to be aware of logistic regression as one of the most basic classifiers that you can use. In this section you'll learn how it works and how to use it.\u003c/p\u003e","frontPage":false},{"exportId":"classifiers-with-bayes","title":"Classifiers with Bayes","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classifiers-with-bayes\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classifiers-with-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you're familiar with Bayes' theorem and foundational concepts of Bayesian statistics, you'll take a look at how to implement some of these ideas for machine learning. Classification tasks can be a natural application of Bayes' theorem since you are looking to predict some label given other information, which can be conceptualized through conditional probability.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the assumption that leads to Naive Bayes being \"naive\"\u003c/li\u003e\n\u003cli\u003eExplain how to use the probabilities generated by Naive Bayes to make a classification \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eNaive Bayes\u003c/h2\u003e\n\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features. \u003c/p\u003e\n\n\u003cp\u003eFor example, extending the previous medical examples of Bayes' theorem, a researcher might examine multiple patient measurements to better predict whether or not an individual has a given disease. Provided that these measurements are independent (and uncorrelated from one another), one can then examine the conditional probability of each of these metrics and apply Bayes' theorem to determine a relative probability of having the disease or not. Combining these probabilities can then give an overall confidence of a patient having the disease given all the information. From this, one can then make a prediction for whether or not you believe an individual has the disease or not based on which probability is higher.\u003c/p\u003e\n\n\u003cp\u003eMathematically, if \u003cimg class=\"equation_image\" title=\"Y\" src=\"https://learning.flatironschool.com/equation_images/Y\" alt=\"{\" data-equation-content=\"Y\"\u003e is a class you wish to predict (such as having a disease) and \u003cimg class=\"equation_image\" title=\"X_1, X_2, ..., X_n\" src=\"https://learning.flatironschool.com/equation_images/X_1,%20X_2,%20...,%20X_n\" alt=\"{\" data-equation-content=\"X_1, X_2, ..., X_n\"\u003e are the various measurements for the given individual or case, then the probability of class \u003cimg class=\"equation_image\" title=\"Y\" src=\"https://learning.flatironschool.com/equation_images/Y\" alt=\"{\" data-equation-content=\"Y\"\u003e can be written as:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\large P(Y|X_1, X_2, ..., X_n) = \\dfrac{P(X_1|Y) \\cdot P(X_2|Y) \\cdot ... \\cdot P(X_n|Y)}{P(X_1, X_2, ..., X_n)}P(Y)\" src=\"/equation_images/%20%255Clarge%20P(Y|X_1,%20X_2,%20...,%20X_n)%20=%20%255Cdfrac{P(X_1|Y)%20%255Ccdot%20P(X_2|Y)%20%255Ccdot%20...%20%255Ccdot%20P(X_n|Y)}{P(X_1,%20X_2,%20...,%20X_n)}P(Y)\" alt=\"{\" data-equation-content=\" \\large P(Y|X_1, X_2, ..., X_n) = \\dfrac{P(X_1|Y) \\cdot P(X_2|Y) \\cdot ... \\cdot P(X_n|Y)}{P(X_1, X_2, ..., X_n)}P(Y)\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eAgain, note that multiplying the conditional probabilities is based on the assumption that these probabilities (and their underlying features) are independent -- and it is this assumption that the Naive Bayes algorithm is considered naive, or simple, because this is almost never true. However, Naives Bayes can prove to be quite efficient given the right circumstances, as you will see in the upcoming lessons. \u003c/p\u003e\n\n\u003cp\u003eIn practice, calculating the denominator, \u003cimg class=\"equation_image\" title=\"P(X_1, X_2, ..., X_n)\" src=\"https://learning.flatironschool.com/equation_images/P(X_1,%20X_2,%20...,%20X_n)\" alt=\"{\" data-equation-content=\"P(X_1, X_2, ..., X_n)\"\u003e is often impractical or impossible as this exact combination of features may not have been previously observed. However, doing so is often not required. This is because when implementing a classifier, the exact probabilities themselves are not required to generate a prediction. Instead, you must simply answer which option is the most probable. To do this, you would calculate \u003cimg class=\"equation_image\" title=\"P(Y_0)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_0)\" alt=\"{\" data-equation-content=\"P(Y_0)\"\u003e, the probability of not having the disease as well as \u003cimg class=\"equation_image\" title=\"P(Y_1)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_1)\" alt=\"{\" data-equation-content=\"P(Y_1)\"\u003e, the probability of having the disease. Furthermore, since the denominator, \u003cimg class=\"equation_image\" title=\"P(X_1, X_2, ..., X_n)\" src=\"https://learning.flatironschool.com/equation_images/P(X_1,%20X_2,%20...,%20X_n)\" alt=\"{\" data-equation-content=\"P(X_1, X_2, ..., X_n)\"\u003e, is equal for both \u003cimg class=\"equation_image\" title=\"P(Y_0)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_0)\" alt=\"{\" data-equation-content=\"P(Y_0)\"\u003e and \u003cimg class=\"equation_image\" title=\"P(Y_1)\" src=\"https://learning.flatironschool.com/equation_images/P(Y_1)\" alt=\"{\" data-equation-content=\"P(Y_1)\"\u003e, you can compare the numerators, as these will be proportional to the overall probability. You'll investigate this further as you code some Naive Bayes classification algorithms yourself in the upcoming lessons.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly explored how Bayes' theorem can be used to build classification algorithms. In the upcoming lessons and labs you'll investigate particular implementations of Naive Bayes classifiers which differ in how the individual conditional probabilities themselves are constructed. As you will see, Naive Bayes can be extremely effective or trivially useful depending on the context and implementation.\u003c/p\u003e","frontPage":false},{"exportId":"mle-and-logistic-regression","title":"MLE and Logistic Regression","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-logistic-regression\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll further investigate the connections between maximum likelihood estimation and logistic regression. This is a common perspective for logistic regression and will be the underlying intuition for upcoming lessons where you'll code the algorithm from the ground up using NumPy.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDetermine how MLE is tied into logistic regression \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eMLE formulation\u003c/h2\u003e\n\n\u003cp\u003eAs discussed, maximum likelihood estimation finds the underlying parameters of an assumed distribution to maximize the likelihood of the observations. Logistic regression expands upon this by investigating the conditional probabilities associated with the various features, treating them as independent probabilities and calculating the respective total probability.  \u003c/p\u003e\n\n\u003cp\u003eFor example, when predicting an individual's risk for heart disease, you might consider various factors such as their family history, weight, diet, exercise routines, blood pressure, and cholesterol. When looked at individually, each of these has an associated conditional probability that the individual has heart disease based on each of these factors. Mathematically, you can write each of these probabilities for each factor \u003cimg class=\"equation_image\" title=\"X\" src=\"https://learning.flatironschool.com/equation_images/X\" alt=\"{\" data-equation-content=\"X\"\u003e as:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\pi_i=Pr(Y_i=1|X_i=x_i)=\\dfrac{\\text{exp}(\\beta_0+\\beta_1 x_i)}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\" src=\"/equation_images/%255Cpi_i=Pr(Y_i=1|X_i=x_i)=%255Cdfrac{%255Ctext{exp}(%255Cbeta_0+%255Cbeta_1%20x_i)}{1+%255Ctext{exp}(%255Cbeta_0+%255Cbeta_1%20x_i)}\" alt=\"{\" data-equation-content=\"\\pi_i=Pr(Y_i=1|X_i=x_i)=\\dfrac{\\text{exp}(\\beta_0+\\beta_1 x_i)}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is the standard linear regression model (\u003cimg class=\"equation_image\" title=\"\\beta_0+\\beta_1 x_i\" src=\"https://learning.flatironschool.com/equation_images/%255Cbeta_0+%255Cbeta_1%20x_i\" alt=\"{\" data-equation-content=\"\\beta_0+\\beta_1 x_i\"\u003e) you have seen previously, modified to have a range of 0 to 1. The range is modified and constrained by applying the sigmoid function since you're predicting probabilities.\u003c/p\u003e\n\n\u003cp\u003eThen, combining these conditional probabilities from multiple features, you maximize the likelihood function of each of those independent conditional probabilities, giving you:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\" src=\"/equation_images/%20L(%255Cbeta_0,%255Cbeta_1)=%255Cprod%255Climits_{i=1}^N%20%255Cpi_i^{y_i}(1-%255Cpi_i)^{n_i-y_i}=%255Cprod%255Climits_{i=1}^N%20%255Cdfrac{%255Ctext{exp}{y_i(%255Cbeta_0+%255Cbeta_1%20x_i)}}{1+%255Ctext{exp}(%255Cbeta_0+%255Cbeta_1%20x_i)}\" alt=\"{\" data-equation-content=\" L(\\beta_0,\\beta_1)=\\prod\\limits_{i=1}^N \\pi_i^{y_i}(1-\\pi_i)^{n_i-y_i}=\\prod\\limits_{i=1}^N \\dfrac{\\text{exp}{y_i(\\beta_0+\\beta_1 x_i)}}{1+\\text{exp}(\\beta_0+\\beta_1 x_i)}\"\u003e   \u003c/p\u003e\n\n\u003ch2\u003eNotes on mathematical symbols\u003c/h2\u003e\n\n\u003cp\u003eRecall that the \u003cimg class=\"equation_image\" title=\"\\prod\" src=\"https://learning.flatironschool.com/equation_images/%255Cprod\" alt=\"{\" data-equation-content=\"\\prod\"\u003e sign stands for a product of each of these individual probabilities. (Similar to how \u003cimg class=\"equation_image\" title=\"\\sum\" src=\"https://learning.flatironschool.com/equation_images/%255Csum\" alt=\"{\" data-equation-content=\"\\sum\"\u003e stands for the sum of a series.) Since this is a monotonically increasing function, its maximum will be the same as the logarithm of the function, which is typically used in practice in order to decompose this product of probabilities into a sum of log probabilities for easier calculation of the derivative. In future sections, you'll investigate the derivative of this function and then use that in order to code up our own function for logistic regression.  \u003c/p\u003e\n\n\u003ch2\u003eAlgorithm bias and ethical concerns\u003c/h2\u003e\n\n\u003cp\u003eIt should also be noted that while this is mathematically sound and a powerful tool, the model will simply reflect the data that is fed in. For example, logistic regression and other algorithms are used to inform a wide range of decisions including whether to provide someone with a loan, the degree of criminal sentencing, or whether to hire an individual for a job. In all of these scenarios, it is again important to remember that the algorithm is simply reflective of the underlying data itself. If an algorithm is trained on a dataset where African Americans have had disproportionate criminal prosecution, the algorithm will continue to perpetuate these racial injustices. Similarly, algorithms trained on data that reflect a gender pay-gap will also continue to promote this bias unless adequately accounted for through careful preprocessing and normalization. With this, substantial thought and analysis regarding problem set up and the resulting model is incredibly important. While future lessons and labs in this section return to underlying mathematical theory and how to implement logistic regression on your own, it is worthwhile to investigate some of the current problems regarding some of these algorithms, and how naive implementations can perpetuate unjust biases.\u003c/p\u003e\n\n\u003ch2\u003eAdditional resources\u003c/h2\u003e\n\n\u003cp\u003eBelow are a handful of resources providing further information regarding some of the topics discussed here. Be sure to check out some of the news articles describing how poor safeguards and problem formulation surrounding algorithms such as logistic regression can lead to unjust biases: \u003c/p\u003e\n\n\u003ch3\u003eAlgorithm bias and ethical concerns\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\"\u003eMachine Bias\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.bloomberg.com/opinion/articles/2018-10-16/amazon-s-gender-biased-algorithm-is-not-alone\"\u003eAmazon‚Äôs Gender-Biased Algorithm Is Not Alone\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/business/2017/12/21/the-software-that-runs-our-lives-can-bigoted-and-unfair-but-can-fix/RK4xG4gYxcVNVTIubeC1JI/story.html\"\u003eThe software that runs our lives can be bigoted and unfair. But we can fix it\u003c/a\u003e  \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.bostonglobe.com/ideas/2017/07/07/why-artificial-intelligence-far-too-human/jvG77QR5xPbpwBL2ApAFAN/story.html\"\u003eWhy artificial intelligence is far too human\u003c/a\u003e   \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.npr.org/2016/03/14/470427605/can-computers-be-racist-the-human-like-bias-of-algorithms\"\u003eCan Computers Be Racist? The Human-Like Bias Of Algorithms\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eAdditional mathematical resources\u003c/h3\u003e\n\n\u003cp\u003eIf you want to really go down the math rabbit-hole, check out section 4.4 on Logistic Regression from the Elements of Statistical Learning which can be found here: \u003ca href=\"https://web.stanford.edu/%7Ehastie/ElemStatLearn//\"\u003ehttps://web.stanford.edu/~hastie/ElemStatLearn//\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you further analyzed logistic regression from the perspective of maximum likelihood estimation. Additionally, there was a brief pause to consider the setup and interpretation of algorithms such as logistic regression. In particular, remember that issues regarding racial and gender bias that can be perpetuated by these algorithms. Always try to ensure your models are ethically sound. In the proceeding labs and lessons, you will continue to formalize your knowledge of logistic regression, implementing gradient descent and then a full logistic regression algorithm using Python packages in order to give you a deeper understanding of how logistic regression works.\u003c/p\u003e","frontPage":false},{"exportId":"introduction-to-supervised-learning","title":"Introduction to Supervised Learning","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll examine what exactly the term \"Supervised Learning\" means, and where it fits in Data Science. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the components of what makes something a supervised learning task \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is Supervised Learning?\u003c/h2\u003e\n\n\u003cp\u003eThe term \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e refers to a class of machine learning algorithms that can \"learn\" a task through \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. We'll explore this definition more fully in a bit -- but first, it's worth taking some time to understand where supervised learning fits in the overall picture in regards to Data Science. By now, you've probably noticed that many of the things we've learned in Data Science and Computer Science are very hierarchical. This is especially true when it comes to AI and Machine Learning. Let's break down the hierarchy a bit, and see where \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e fits.  \u003c/p\u003e\n\n\u003ch2\u003eArtificial Intelligence\u003c/h2\u003e\n\n\u003cp\u003eAt the top of the hierarchy is \u003cstrong\u003e\u003cem\u003eArtificial Intelligence\u003c/em\u003e\u003c/strong\u003e.  AI is a catch-all term for various kinds of algorithms that can complete tasks that normally require human intelligence to complete. AI is made up of several subcategories, and is also a subcategory itself in the greater hierarchy of Computer Science. When data scientists talk about AI, we're almost focused on a single branch of AI, \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e. Machine Learning is responsible for the boom in AI technologies and abilities in the last few decades, but it's worth noting that there are other areas of AI that do not fall under the umbrella of 'Machine Learning'. Other branches of AI include things like \u003cem\u003eGenetic Algorithms\u003c/em\u003e for optimization, or rules-based AI for things like building a bot for players to play against in a video game. While these are still active areas of research, they have little to no application in Data Science, so they're beyond the scope of this lesson. In general, when you see the phrase 'Artificial Intelligence', it's generally safe to assume that that the speaker is probably referring to the subfield of AI known as \u003cstrong\u003e\u003cem\u003eMachine Learning\u003c/em\u003e\u003c/strong\u003e (which is also sometimes referred to by it's older, more traditional name -- \u003cstrong\u003e\u003cem\u003eStatistical Learning\u003c/em\u003e\u003c/strong\u003e).\u003c/p\u003e\n\n\u003cp\u003eThe following graphic shows the breakdown of the 'Machine Learning' branch of AI:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/master/images/new_ml-hierarchy.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eMachine Learning\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eMachine Learning\u003c/em\u003e can be further divided into two overall categories:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe main difference between these two areas of machine learning is the need for \u003cstrong\u003e\u003cem\u003elabeled training data\u003c/em\u003e\u003c/strong\u003e. In \u003cstrong\u003e\u003cem\u003eSupervised Learning\u003c/em\u003e\u003c/strong\u003e, any data used must have a \u003cstrong\u003e\u003cem\u003elabel\u003c/em\u003e\u003c/strong\u003e. These labels are the \u003cem\u003eground truth\u003c/em\u003e , which allows our supervised learning algorithms to 'check their work'. By comparing its predictions against the actual labels, our algorithm can learn to make less incorrect predictions and improve the overall performance of the task its learning to do. It helps to think of Supervised Learning as close to the type of learning we do as students in grade school. Imagine using practice exams to study for the SAT or ACT test. We can go through all the practice questions we want, but in order to learn from our performance on those practice questions, we need to know what the correct answers are! Without them, we would have no way of knowing which questions we got right and which ones we got wrong, so we wouldn't be able to learn what changes we would need to make to improve our overall performance! \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\"A computer program is said to \u003cstrong\u003elearn\u003c/strong\u003e from experience \u003cem\u003eE\u003c/em\u003e with respect to some class of tasks \u003cem\u003eT\u003c/em\u003e and performance measure \u003cem\u003eP\u003c/em\u003e, if its performance at tasks in \u003cem\u003eT\u003c/em\u003e, as measured by \u003cem\u003eP\u003c/em\u003e, improves with experience \u003cem\u003eE\u003c/em\u003e.\"  -- \u003ca href=\"http://www.cs.cmu.edu/%7Etom/\"\u003eTom Mitchell\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eLet's pretend we've built and trained a model to detect if a picture contains a cat or not. Using the language from the definition above:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eTask (T)\u003c/strong\u003e: predict if a picture contains a cat or not\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePerformance Measure (P)\u003c/strong\u003e: The objective function used to score the predictions made by our model for each image\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eExperience (E)\u003c/strong\u003e: All of our labeled training data. The more training data we provide, the more 'experience' our model gets!\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe'll spend some time learning about \u003cstrong\u003e\u003cem\u003eUnsupervised Learning\u003c/em\u003e\u003c/strong\u003e in the next module, so don't worry about it for now!\u003c/p\u003e\n\n\u003ch2\u003eClassification and Regression\u003c/h2\u003e\n\n\u003cp\u003eThe field of \u003cem\u003eSupervised Learning\u003c/em\u003e can be further broken down into two categories -- \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRegression\u003c/em\u003e\u003c/strong\u003e. At this point in your studies, you already have significant experience with regression -- specifically \u003cstrong\u003eLinear Regression\u003c/strong\u003e , probably the most foundational (and important) machine learning model. Recall that regression allows us to answer questions like \"how much?\" or \"how many?\". If our label is a real-valued number, then the supervised learning problem you're trying to solve is a \u003cem\u003eregression\u003c/em\u003e problem. \u003c/p\u003e\n\n\u003cp\u003eThe other main kind of supervised learning problem is \u003cstrong\u003e\u003cem\u003eClassification\u003c/em\u003e\u003c/strong\u003e. Classification allows us to tell if something belongs to one class or the other. In the case of the \u003ca href=\"https://www.kaggle.com/c/titanic\"\u003etitanic\u003c/a\u003e dataset, this may be something like survival. For example, given various characteristics of a passenger, predict whether they will survive or not. Questions that can be answered in a True/False format (in the titanic example, \"Survived\" or \"Not survived\") are a type of \u003cstrong\u003e\u003cem\u003eBinary Classification\u003c/em\u003e\u003c/strong\u003e. To perform binary classification, you will be introduced to \u003cstrong\u003eLogistic Regression\u003c/strong\u003e. Don't let the name confuse you, although the name contains the word \"regression,\" this important foundational technique is very important in understanding classification problems. There are several other classification techniques you will be learning in this module, but in order to gain a sound understanding of \u003cstrong\u003eClassification\u003c/strong\u003e tasks, this section will be focused exclusively on building and evaluating logistic regression models. \u003c/p\u003e\n\n\u003cp\u003eHowever, we are not limited to only two classes when working with classification algorithms -- we can have as many classes as we see fit. When a supervised learning problem has more than two classes, we refer to it as a \u003cstrong\u003e\u003cem\u003eMulticlass Classification\u003c/em\u003e\u003c/strong\u003e problem. \u003c/p\u003e\n\n\u003ch2\u003eObjective Functions\u003c/h2\u003e\n\n\u003cp\u003eWhenever we're dealing with supervised learning, we have an \u003cstrong\u003e\u003cem\u003eObjective Function\u003c/em\u003e\u003c/strong\u003e (also commonly called a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e) that we're trying to optimize against. Regardless of the supervised learning model we're working with, we can be sure that we have some sort of function under the hood that we're using to grade the predictions made by our model against the actual ground-truth labels for each prediction. In the quote from Tom Mitchell listed above, objective functions are \u003cem\u003eP\u003c/em\u003e. While classification and regression models use different kinds of objective functions to evaluate their performance, the concept is the same -- these functions allow the model to evaluate exactly how right or wrong a prediction is, which the algorithm can then \"learn\" from. These objective functions serve an important purpose, because they act as the ground-truth for determining if our model is getting better or not. \u003c/p\u003e\n\n\u003ch3\u003eThe Limitations of Labeled Data\u003c/h3\u003e\n\n\u003cp\u003eBecause supervised learning requires \u003cstrong\u003e\u003cem\u003eLabels\u003c/em\u003e\u003c/strong\u003e for any data used, this severely limits the amount of available data we have for use with supervised learning algorithms.  Of all the data in the world, only a very, very small percentage is labeled. Why? Because labeling data is a purposeful activity that can only be done by humans, and is therefore time-consuming and expensive. In supervised learning, labels are not universal -- they are unique to the problem we're trying to solve. If we're trying to train a model to predict if someone survived the titanic disaster, we need to know the survival results of every passenger in our dataset -- there's no way around it. However, if we're trying to predict how much a person paid for a ticket on the titanic, survival data now no longer works as a label -- instead, we need to know how much each passenger paid for a ticket. In a more generalized sense, this means that for whatever problem we're trying to train a supervised learning model to solve, we need to have a large enough dataset containing examples where humans have already done the things we're trying to get our model to learn how to do.  \u003c/p\u003e\n\n\u003cp\u003eAlthough labeled data is still expensive and time-consuming to get, the internet has made the overall process of getting labeled data a bit easier than it used to be. Nowadays, when companies need to construct a dataset of labeled training data to solve a problem, they typically make use of services like Amazon's \u003ca href=\"https://docs.aws.amazon.com/mturk/index.html\"\u003eAWS Mechanical Turk\u003c/a\u003e, or 'MTurk' for short. Services like this obtain labels by paying people for each label they generate. In this way, a company can crowdsource the work to label the training data needed. The company simply uploads unlabeled training data like an image, and a \"turker\" will then provide a label for that image according to the instructions from the company. Depending on the problem the company is trying to solve, the label for the image might be something as simple as the word \"cat\", or as complex as as boxes drawn around all the cats in the image. \u003c/p\u003e\n\n\u003ch3\u003eNegative Examples\u003c/h3\u003e\n\n\u003cp\u003eWhen creating a labeled dataset for a classification problem, it is worth noting that negative examples are just as important to be included in the dataset as positive examples. If our training data in the titanic dataset only contained data on passengers that all survived, no supervised learning algorithm would be able to learn how to predict if a passenger survived or died with any sort of accuracy. \u003cstrong\u003e\u003cem\u003ePositive Examples\u003c/em\u003e\u003c/strong\u003e are data points that belong to the class we're training our model to recognize. For instance, let's pretend we're building a model to tell if a picture is of a cat or not. All the pictures of cats in our dataset would be positive examples. However, in order to build a good cat classifier, our dataset would also need to contain many different kinds of pictures that don't include cats. Intuitively, this makes sense -- if every picture that our model ever saw had a cat in it, then the only thing that model will learn is that everything is a cat. To truly learn what we need it to learn, this model will also need to learn what a cat \u003cem\u003eisn't\u003c/em\u003e, by looking at pictures that don't include cats -- our \u003cstrong\u003e\u003cem\u003eNegative Examples\u003c/em\u003e\u003c/strong\u003e. In this way, with a complex enough model and enough labeled training data, our classifier will eventually learn that the differentiating factor between images with positive labels and images with negative labels are the shapes and patterns common to cats, but not dogs (or other animals). In this way, supervised learning can be a bit tricky. For instance, if all of the negative examples in our cat classifier dataset are of cars and houses, then the model will almost certainly get a picture of a dog incorrect by predicting that the picture is of a cat. Why does this happen? Because the model hasn't seen a dog before, and therefore has no idea whether this fits. In this particular example, we can guess that any picture of a dog will look more like a cat than it would a house or car, which from the model's perspective means that this is probably a picture of a cat. \u003c/p\u003e\n\n\u003cp\u003eIn summary, this part of supervised learning can often be more art than science -- when creating a dataset, make sure that your dataset contains enough negative examples, and that you are very thoughtful about what those negative examples actually contain! \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about \u003cem\u003eSupervised Learning\u003c/em\u003e, and where it fits in relation to Machine Learning and Artificial Intelligence. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-intro-to-supervised-learning-v2-1\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-intro-to-supervised-learning-v2-1/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"topic-30-lesson-priorities-live","title":"Topic 30 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.3523%; height: 153px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eBagging\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Ensemble Methods - Introduction\" href=\"pages/ensemble-methods-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/ensemble-methods-introduction\" data-api-returntype=\"Page\"\u003eEnsemble Methods - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Ensemble Methods\" href=\"pages/ensemble-methods\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/ensemble-methods\" data-api-returntype=\"Page\"\u003eEnsemble Methods\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Random Forests\" href=\"pages/random-forests\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/random-forests\" data-api-returntype=\"Page\"\u003eRandom Forests\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Tree Ensembles and Random Forests - Lab\" href=\"assignments/gcbdd65aa18e5b390121d84ab623eec90\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12102\" data-api-returntype=\"Assignment\"\u003eTree Ensembles and Random Forests - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"GridSearchCV - Lab\" href=\"assignments/g24774d5f3764ee7c2cc27699b8e85222\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12104\" data-api-returntype=\"Assignment\"\u003eGridSearchCV - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.5396%; height: 155px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eBagging\u003c/em\u003e Lecture, Before \u003cem\u003eBoosting\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Bagging Exit Ticket\" href=\"quizzes/gbcb18a41f12d7478e63f428f7e764a2a\"\u003eBagging Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Gradient Boosting and Weak Learners\" href=\"pages/gradient-boosting-and-weak-learners\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/gradient-boosting-and-weak-learners\" data-api-returntype=\"Page\"\u003eGradient Boosting and Weak Learners\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Gradient Boosting - Lab\" href=\"assignments/g59ccab92e9380e16e1a668e43a6307ec\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12105\" data-api-returntype=\"Assignment\"\u003eGradient Boosting - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"XGBoost\" href=\"pages/xgboost\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/xgboost\" data-api-returntype=\"Page\"\u003eXGBoost\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"XGBoost - Lab\" href=\"assignments/ge3ed4d45a85151cc7115a93f81746615\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12106\" data-api-returntype=\"Assignment\"\u003eXGBoost - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Ensemble Methods\" href=\"quizzes/g01b43860f608fdfa4d009588e4178c7e\"\u003eQuiz: Ensemble Methods\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6335%; height: 56px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eBoosting\u003c/em\u003e Lecture, Before \u003cem\u003eClassification Workflow 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Boosting Exit Ticket\" href=\"quizzes/g918c8032ae84254b92f323de5e44d759\"\u003eBoosting Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.446%; height: 84px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eClassification Workflow 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 37.6964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.46745%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Classification Workflow 2 Exit Ticket\" href=\"quizzes/gbd72b2305e9ba694fe616a4ad180890c\"\u003e\u003cstrong\u003eClassification Workflow 2 Exit Ticket\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 37.6964%; height: 27px;\"\u003e\u003ca title=\"Ensembles - Recap\" href=\"pages/ensembles-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/ensembles-recap\" data-api-returntype=\"Page\"\u003eEnsembles - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.46745%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","frontPage":false},{"exportId":"topic-22-lesson-priorities-live","title":"Topic 22 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u003cspan\u003eIn the Live program, there is no scheduled lecture on Calculus and Cost Functions; instead there is a recorded video lecture. Take the time to watch the lecture recording if you would like a deeper dive into or refresher on the calculus content.\u003c/span\u003e\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.719%; height: 305px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eLinear Algebra\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Linear Algebra and Calculus - Introduction\" href=\"pages/linear-algebra-and-calculus-introduction\"\u003eLinear Algebra and Calculus - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Motivation for Linear Algebra in Data Science\" href=\"pages/motivation-for-linear-algebra-in-data-science\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/motivation-for-linear-algebra-in-data-science\" data-api-returntype=\"Page\"\u003eMotivation for Linear Algebra in Data Science\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Systems of Linear Equations\" href=\"pages/systems-of-linear-equations\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/systems-of-linear-equations\" data-api-returntype=\"Page\"\u003eSystems of Linear Equations\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Systems of Linear Equations - Lab\" href=\"assignments/g6cafbb465925915959d5223bf0942753\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187049\" data-api-returntype=\"Assignment\"\u003eSystems of Linear Equations - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Scalars, Vectors, Matrices, and Tensors - Code Along\" href=\"assignments/gfea549a153a77fb9593814c7ea567a9d\"\u003eScalars, Vectors, Matrices, and Tensors - Code Along\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\"\u003e\u003cspan style=\"color: #000000;\"\u003e\u003cstrong\u003e 1st\u003c/strong\u003e\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Matrix Multiplication - Code Along\" href=\"assignments/g1f9f8e83538aa1ccd15d2d9b0de0394c\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187012\" data-api-returntype=\"Assignment\"\u003eMatrix Multiplication - Code Along\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\"\u003e\u003cspan data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\" data-sheets-userformat=\"{\u0026quot;2\u0026quot;:4224,\u0026quot;10\u0026quot;:2,\u0026quot;15\u0026quot;:\u0026quot;Arial\u0026quot;}\"\u003e 2nd\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003ca title=\"Short Video: Matrix Multiplication\" href=\"pages/short-video-matrix-multiplication\"\u003eShort Video: Matrix Multiplication\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cspan data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\" data-sheets-userformat=\"{\u0026quot;2\u0026quot;:4224,\u0026quot;10\u0026quot;:2,\u0026quot;15\u0026quot;:\u0026quot;Arial\u0026quot;}\"\u003e2nd\u003c/span\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Solving Systems of Linear Equations with NumPy - Code Along\" href=\"assignments/g8345898d9a24b82069d1f41709ee0fdd\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187047\" data-api-returntype=\"Assignment\"\u003eSolving Systems of Linear Equations with NumPy - Code Along\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Solving Systems of Linear Equations with NumPy - Lab\" href=\"assignments/g601c542580c65fb6de1d04b8bb0a30ef\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187048\" data-api-returntype=\"Assignment\"\u003eSolving Systems of Linear Equations with NumPy - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Regression Analysis using Linear Algebra and NumPy - Code Along\" href=\"assignments/gf03dc6212b7f20a3571de75b90fb5ccc\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187035\" data-api-returntype=\"Assignment\"\u003eRegression Analysis using Linear Algebra and NumPy - Code Along\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Regression with Linear Algebra - Lab\" href=\"assignments/gce6931adc68361b10a894c60cafbf6f7\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187039\" data-api-returntype=\"Assignment\"\u003eRegression with Linear Algebra - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Computational Complexity: From OLS to Gradient Descent\" href=\"pages/computational-complexity-from-ols-to-gradient-descent\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/computational-complexity-from-ols-to-gradient-descent\" data-api-returntype=\"Page\"\u003eComputational Complexity: From OLS to Gradient Descent\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003csup\u003e1\u003c/sup\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Linear Algebra\" href=\"quizzes/gc405785850874cf6a53b7923e4458e4d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30646\" data-api-returntype=\"Quiz\"\u003eQuiz: Linear Algebra\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003csup\u003e1\u003c/sup\u003eThis is rated as 2nd priority for progress within the DS program, but we encourage you to study this in preparation for technical interviews\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6253%; height: 71px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eLinear Algebra\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eCalculus and Cost Functions\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Linear Algebra Exit Ticket\" href=\"quizzes/g62b3e5e4cbc8970111617a88cdab7801\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30657\" data-api-returntype=\"Quiz\"\u003eLinear Algebra Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Derivatives\" href=\"assignments/g9777bfbacb7532d96452a0bc04a904b5\"\u003eIntroduction to Derivatives\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Introduction To Derivatives - Lab\" href=\"assignments/ga3c10c23d3d9e1151dccb8cb9437a1f4\"\u003eIntroduction To Derivatives - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003cstrong\u003e\u003ca title=\"Derivatives of Non-Linear Functions\" href=\"assignments/g3096b7ec24bf7a02135c7a720a776e2d\"\u003eDerivatives of Non-Linear Functions\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003ca title=\"Rules for Derivatives\" href=\"assignments/g240536dee629c9b857d210808dac71ec\"\u003eRules for Derivatives\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 49.4339%;\"\u003e\u003cstrong\u003e\u003ca title=\"Derivatives: Conclusion\" href=\"assignments/gc0e76e1a6e29d946d6f18e9aacad1c3d\"\u003eDerivatives: Conclusion\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6253%; height: 290px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eCalculus and Cost Functions\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eGradient Descent\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Calculus and Cost Functions Exit Ticket\" href=\"quizzes/gb3db5cbf0bcdb11aabfa93891bfac5b0\"\u003eCalculus and Cost Functions Exit Ticket\u003c/a\u003e\u003ca title=\"Introduction to Gradient Descent\" href=\"assignments/g0eaaa285661d545beda76b6a1d97b302\"\u003e\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Gradient Descent\" href=\"assignments/g0eaaa285661d545beda76b6a1d97b302\"\u003eIntroduction to Gradient Descent\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient Descent: Step Sizes\" href=\"assignments/g508c94e5f4410b774066661cb6893554\"\u003eGradient Descent: Step Sizes\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient Descent: Step Sizes - Lab\" href=\"assignments/gafebbe6171e387ed7e205bba80badcc0\"\u003eGradient Descent: Step Sizes - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient Descent in 3D\" href=\"assignments/g3929ccf412b073ec731206e2dce84453\"\u003eGradient Descent in 3D\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\" The Gradient in Gradient Descent\" href=\"pages/the-gradient-in-gradient-descent\"\u003e The Gradient in Gradient Descent\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Short Video: The Gradient\" href=\"pages/short-video-the-gradient\"\u003eShort Video: The Gradient\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Gradient to Cost Function\" href=\"assignments/gc5264c4c0c07fc228d3a316350dc8504\"\u003eGradient to Cost Function\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Applying Gradient Descent - Lab\" href=\"assignments/gf1d5c6a7ab52e0d588b4b7db4b395574\"\u003eApplying Gradient Descent - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; text-align: center; height: 29px;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6253%; height: 71px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eGradient Descent\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 49.4339%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.28315%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Gradient Descent Exit Ticket\" href=\"quizzes/gc53fdbad8c7fa83c61a7b59363bb80bf\"\u003eGradient Descent Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 49.4339%; height: 29px;\"\u003e\u003ca title=\"Linear Algebra and Calculus - Recap\" href=\"pages/linear-algebra-and-calculus-recap\"\u003eLinear Algebra and Calculus - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.28315%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","frontPage":false},{"exportId":"machine-learning-fundamentals-introduction","title":"Machine Learning Fundamentals - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ml-fundamentals-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eLinear regression serves as the bridge between traditional statistics and machine learning in this curriculum. In this section you'll learn how to apply machine learning techniques using the familiar linear regression algorithm.\u003c/p\u003e\n\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\n\u003cp\u003eStatistical learning goes beyond the statistical modeling we have covered already. We will continue to build models that model the relationships between independent and dependent variables, but the emphasis will shift from \u003cstrong\u003e\u003cem\u003einference\u003c/em\u003e\u003c/strong\u003e to \u003cstrong\u003e\u003cem\u003eprediction\u003c/em\u003e\u003c/strong\u003e. In the predictive context we also need to consider model \u003cstrong\u003e\u003cem\u003egeneralization\u003c/em\u003e\u003c/strong\u003e and not just how well the model is fit to the training data.\u003c/p\u003e\n\n\u003ch3\u003eModel Validation\u003c/h3\u003e\n\n\u003cp\u003eIn order to assess how well a predictive model will generalize to unseen data, we need model \u003cstrong\u003e\u003cem\u003evalidation\u003c/em\u003e\u003c/strong\u003e techniques. These include a \u003cstrong\u003e\u003cem\u003etrain-test split\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003ecross-validation\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eBias-Variance Trade-Off\u003c/h3\u003e\n\n\u003cp\u003ePredictive modeling often involves striking the right balance between \u003cstrong\u003e\u003cem\u003ebias\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003evariance\u003c/em\u003e\u003c/strong\u003e, also referred to as the balance between \u003cstrong\u003e\u003cem\u003eunderfitting\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e. Especially as you learn models beyond linear regression, you'll be able to tune the model performance to strike the right balance.\u003c/p\u003e\n\n\u003ch2\u003eRegularization\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eRegularization\u003c/em\u003e\u003c/strong\u003e is a technique to help prevent overfitting in predictive modeling. We'll specifically discuss \u003cstrong\u003e\u003cem\u003eridge\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003elasso\u003c/em\u003e\u003c/strong\u003e regression, which are extensions to linear regression that include penalty terms to help prevent overfitting.\u003c/p\u003e\n\n\u003cp\u003eLasso regression in particular does not have a closed form solution and therefore must be solved using an alternative approach such as gradient descent. It also can be helpful for feature selection purposes.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eLinear regression can be used for prediction as well as inference. This has implications for the modeling techniques required, because the emphasis is not solely on the fit to the training data. In this section we'll go over the theoretical concerns as well as the practical approaches to tackling them.\u003c/p\u003e","frontPage":false},{"exportId":"motivation-for-linear-algebra-in-data-science","title":"Motivation for Linear Algebra in Data Science","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll learn about algebra as a foundational step for data science, and later on statistics. Linear algebra is also very important when moving on to machine learning models, where a solid understanding of linear equations plays a major role. This lesson will attempt to present some motivational examples of how and why a solid foundation of linear algebra is valuable for data scientists.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eState the importance of linear algebra in the fields of data science and machine learning \u003c/li\u003e\n\u003cli\u003eDescribe the areas in AI and machine learning where linear algebra might be used for advanced analytics \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eWhile having a deep understanding of linear algebra may not be mandatory, some basic knowledge is undoubtedly extremely helpful in your journey towards becoming a data scientist.\u003c/p\u003e\n\n\u003cp\u003eYou may already know a number of linear algebraic concepts without even knowing it. Examples are: matrix multiplication and dot-products. Later on, you'll learn more complex algebraic concepts like the calculation of matrix determinants, cross-products, and eigenvalues/eigenvectors. As a data scientist, it is important to know some of the theories as well as having a practical understanding of these concepts in a real-world setting.\u003c/p\u003e\n\n\u003ch2\u003eAn analogy\u003c/h2\u003e\n\n\u003cp\u003eThink of a simple example where you first learn about a sine function as an infinite polynomial while learning trigonometry. Students usually practice this function by passing different values to this function and getting the expected results and then manage to relate this to triangles and vertices. When learning advanced physics, students get to learn more applications of sine and other similar functions in the area of sound and light. In the domain of Signal Processing for unidimensional data, these functions pop up again to help you solve filtering, time-series related problems. An introduction to numeric computation around sine functions can not alone help you understand its wider application areas. In fact, sine functions are everywhere in the universe from music to light/sound/radio waves, from pendulum oscillations to alternating current.\u003c/p\u003e\n\n\u003ch2\u003e\u0026nbsp;Why Linear Algebra?\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eLinear algebra is the branch of mathematics concerning vector spaces and linear relationships between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAnalogous to the example we saw above, it's important that a data scientist understands how data structures are built with vectors and matrices following the geometric intuitions from linear algebra, in addition to the numeric calculations. A data-focused understanding of linear algebra can help machine learning practitioners decide what tools can be applied to a given problem and how to interpret the results of experiments. You'll see that a good understanding of linear algebra is particularly useful in many ML/AI algorithms, especially in deep learning, where a lot of the operations happen under the hood.\u003c/p\u003e\n\n\u003cp\u003eFollowing are some of the areas where linear algebra is commonly practiced in the domain of data science and machine learning:  \u003c/p\u003e\n\n\u003ch3\u003eComputer Vision / Image Processing\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/rgb.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eComputers are designed to process binary information only (only 0s and 1s). How can an image such as the dog shown here, with multiple attributes like color, be stored in a computer? This is achieved by storing the pixel intensities for red, blue and green colors in a matrix format. Color intensities can be coded into this matrix and can be processed further for analysis and other tasks. Any operation performed on this image would likely use some form of linear algebra with matrices as the back end.\u003c/p\u003e\n\n\u003ch3\u003eDeep Learning - Tensors\u003c/h3\u003e\n\n\u003cp\u003eDeep Learning is a sub-domain of machine learning, concerned with algorithms that can imitate the functions and structure of a biological brain as a computational algorithm. These are called artificial neural networks (ANNs). \u003c/p\u003e\n\n\u003cp\u003eThe algorithms usually store and process data in the form of mathematical entities called tensors. A tensor is often thought of as a generalized matrix. That is, it could be a 1-D matrix (a vector is actually such a tensor), a 2-D matrix (like a data frame), a 3-D matrix (something like a cube of numbers), even a 0-D matrix (a single number), or a higher dimensional structure that is harder to visualize.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/tensor.png\" width=\"850\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs shown in the image above where different input features are being extracted and stored as spatial locations inside a tensor which appears as a cube. A tensor encapsulates the scalar, vector, and the matrix characteristics. For deep learning, creating and processing tensors and operations that are performed on these also require knowledge of linear algebra. Don't worry if you don't fully understand this right now, you'll learn more about tensors later!\u003c/p\u003e\n\n\u003ch3\u003eNatural Language Processing\u003c/h3\u003e\n\n\u003cp\u003eNatural Language Processing (NLP) is another (very popular) area in Machine Learning dealing with text data. The most common techniques employed in NLP include BoW (Bag of Words) representation, Term Document Matrix etc. As shown in the image below, the idea is that words are being encoded as numbers and stored in a matrix format. Here, we just use 3 sentences to illustrate this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/NLPmatrix.png\" width=\"650\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is just a short example, but you can store long documents in (giant) matrices like this. Using these counts in a matrix form can help perform tasks like semantic analysis, language translation, language generation etc.\u003c/p\u003e\n\n\u003ch3\u003eDimensionality Reduction\u003c/h3\u003e\n\n\u003cp\u003eDimensionality reduction techniques, which are heavily used when dealing with big datasets, use matrices to process data in order to reduce its dimensions. Principle Component Analysis (PCA) is a widely used dimensionality reduction technique that relies solely on calculating eigenvectors and eigenvalues to identify principal components as a set of highly reduced dimensions. The picture below is an example of a three-dimensional data being mapped into two dimensions using matrix manipulations. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-motivation/master/images/pca.png\" width=\"900\"\u003e\u003c/p\u003e\n\n\u003cp\u003eGreat, you now know about some key areas where linear algebra is used! In the following lessons, you'll go through an introductory series of lessons and labs that will cover basic ideas of linear algebra: an understanding of vectors and matrices with some basic operations that can be performed on these mathematical entities. We will implement these ideas in Python, in an attempt to give you the foundational knowledge to deal with these algebraic entities and their properties. These skills will be applied in advanced machine learning sections later in the course. \u003c/p\u003e\n\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=_MxCXGF9N-8\"\u003eYoutube: Why Linear Algebra\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://towardsdatascience.com/boost-your-data-sciences-skills-learn-linear-algebra-2c30fdd008cf\"\u003eBoost your data science skills. Learn linear algebra.\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.quora.com/What-are-the-applications-of-linear-algebra-in-machine-learning\"\u003eQuora: Applications of Linear Algebra in Deep Learning\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about some Data Science examples that heavily rely on linear algebra principles. You looked at some use cases in practical machine learning problems where linear algebra and matrix manipulation might come in handy. In the following lessons, you'll take a deeper dive into specific concepts in linear algebra, working your way towards solving a regression problem using linear algebraic operations only. \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-lingalg-motivation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-lingalg-motivation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"table-of-contents-live","title":"Table of Contents (Live)","type":"WikiPage","content":"\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 21: Object-Oriented Programming and Scikit-Learn\" href=\"modules/g8aab1c17ecda1520340c32e7b2ecd632\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/45903\" data-api-returntype=\"Module\"\u003eTopic 21: Object-Oriented Programming and Scikit-Learn\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 21 Lesson Priorities (Live)\" href=\"pages/topic-21-lesson-priorities-live\"\u003eTopic 21 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 22: Linear Algebra and Calculus\" href=\"modules/ga1ec22255791208102870b00c9597631\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43417\" data-api-returntype=\"Module\"\u003eTopic 22: Linear Algebra and Calculus\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 22 Lesson Priorities (Live)\" href=\"pages/topic-22-lesson-priorities-live\"\u003eTopic 22 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 23: Machine Learning Fundamentals\" href=\"modules/g1853751b3adb03f437e97cf14f6a7a8b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/45904\" data-api-returntype=\"Module\"\u003eTopic 23: Machine Learning Fundamentals\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 23 Lesson Priorities (Live)\" href=\"pages/topic-23-lesson-priorities-live\"\u003eTopic 23 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 24: Logistic Regression\" href=\"modules/g3ca6a0af38c0b8fea8e33b3bcd24d8e1\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43420\" data-api-returntype=\"Module\"\u003eTopic 24: Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 24 Lesson Priorities (Live)\" href=\"pages/topic-24-lesson-priorities-live\"\u003eTopic 24 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 25: Classification Metrics\" href=\"modules/gec1b618791ced2ade978d083978119b0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43421\" data-api-returntype=\"Module\"\u003eTopic 25: Classification Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 25 Lesson Priorities (Live)\" href=\"pages/topic-25-lesson-priorities-live\"\u003eTopic 25 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 26: Decision Trees\" href=\"modules/g82789eaec1c2f4bc30fcb1651aaa6e3f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43422\" data-api-returntype=\"Module\"\u003eTopic 26: Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 26 Lesson Priorities (Live)\" href=\"pages/topic-26-lesson-priorities-live\"\u003eTopic 26 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 27: K Nearest Neighbors\" href=\"modules/g2a0d32eb8567428df5a74a26b5cbba7d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43423\" data-api-returntype=\"Module\"\u003eTopic 27: K Nearest Neighbors\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 27 Lesson Priorities (Live)\" href=\"pages/topic-27-lesson-priorities-live\"\u003eTopic 27 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 28: Bayes Classification\" href=\"modules/gccd76d19614efb1b6f9d4bc63d404deb\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43424\" data-api-returntype=\"Module\"\u003eTopic 28: Bayes Classification\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 28 Lesson Priorities (Live)\" href=\"pages/topic-28-lesson-priorities-live\"\u003eTopic 28 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 29: Model Tuning and Pipelines\" href=\"modules/gaab62e319239fc8a2aa6ed300350f65e\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43425\" data-api-returntype=\"Module\"\u003eTopic 29: Model Tuning and Pipelines\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 29 Lesson Priorities (Live)\" href=\"pages/topic-29-lesson-priorities-live\"\u003eTopic 29 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca title=\"Topic 30: Ensemble Methods\" href=\"modules/g1296f971fd4327def9ab6c54b8f6e0a4\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43426\" data-api-returntype=\"Module\"\u003eTopic 30: Ensemble Methods\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp style=\"padding-left: 40px;\"\u003e\u003cspan style=\"font-size: 10pt;\"\u003e\u003ca title=\"Topic 30 Lesson Priorities (Live)\" href=\"pages/topic-30-lesson-priorities-live\"\u003eTopic 30 Lesson Priorities\u003c/a\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca title=\"üèÜ Milestones\" href=\"modules/gbcbec1317916c833fa0f56e4889aaaaa\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43414\" data-api-returntype=\"Module\"\u003eüèÜ Milestones\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca title=\"APPENDIX: More OOP\" href=\"modules/gfd45e1b463d698d320c9d9f01d71f34b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/modules/43427\" data-api-returntype=\"Module\"\u003eAPPENDIX\u003c/a\u003e\u003c/p\u003e","frontPage":true},{"exportId":"decision-trees-recap","title":"Decision Trees - Recap","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDecision trees can be used for both categorization and regression tasks\u003c/li\u003e\n\u003cli\u003eThey are a powerful and interpretable technique for many machine learning problems (especially when combined with ensemble methods)\u003c/li\u003e\n\u003cli\u003eDecision trees are a form of Directed Acyclic Graphs (DAGs) - you traverse them in a specified direction, and there are no \"loops\" in the graphs to go backward\u003c/li\u003e\n\u003cli\u003eAlgorithms for generating decision trees are designed to maximize the information gain from each split\u003c/li\u003e\n\u003cli\u003eA popular algorithm for generating decision trees is ID3 - the Iterative Dichotomiser 3 algorithm\u003c/li\u003e\n\u003cli\u003eThere are several hyperparameters for decision trees to reduce overfitting - including maximum depth, minimum samples to split a node that is currently a leaf, minimum leaf sample size, maximum leaf nodes, and maximum features \u003c/li\u003e\n\u003c/ul\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-decision-trees-section-recap\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-section-recap/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"random-forests","title":"Random Forests","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll learn about a powerful and popular ensemble method that makes use of decision trees -- a random forest!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how the random forest algorithm works \u003c/li\u003e\n\u003cli\u003eDescribe the subspace sampling method that makes random forests \"random\" \u003c/li\u003e\n\u003cli\u003eExplain the benefits and drawbacks of random forest models \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eUnderstanding the Random forest algorithm\u003c/h2\u003e\n\n\u003cp\u003eThe \u003cstrong\u003e\u003cem\u003eRandom Forest\u003c/em\u003e\u003c/strong\u003e algorithm is a supervised learning algorithm that can be used both for classification and regression tasks. Decision trees are the cornerstone of random forests -- if you don't remember much about decision trees, now may be a good time to go back and review that section until you feel comfortable with the topic. \u003c/p\u003e\n\n\u003cp\u003ePut simply, the random forest algorithm is an ensemble of decision trees. However, you may recall that decision trees use a \u003cstrong\u003e\u003cem\u003egreedy algorithm\u003c/em\u003e\u003c/strong\u003e, meaning that given the same data, the algorithm will make a choice that maximizes information gain at every step. By itself, this presents a problem -- it doesn't matter how many trees we add to our forest if they're all the same tree! Trees trained on the same dataset will come out the exact same way every time -- there is no randomness to this algorithm. It doesn't matter if our forest has a million decision trees; if they are all exactly the same, then our performance will be no better than if we just had a single tree.\u003c/p\u003e\n\n\u003cp\u003eThink about this from a business perspective -- would you rather have a team at your disposal where everyone has exactly the same training and skills, or a team where each member has their own individual strengths and weaknesses? The second team will almost always do much better!\u003c/p\u003e\n\n\u003cp\u003eAs we learned when reading up on ensemble methods, variance is a good thing in any ensemble. So how do we create high variance among all the trees in our random forest? The answer lies in two clever techniques that the algorithm uses to make sure that each tree focuses on different things -- \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e and the \u003cstrong\u003e\u003cem\u003eSubspace Sampling Method\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2\u003eBagging\u003c/h2\u003e\n\n\u003cp\u003eThe first way to encourage differences among the trees in our forest is to train them on different samples of data. Although more data is generally better, if we gave every tree the entire dataset, we would end up with each tree being exactly the same. Because of this, we instead use \u003cstrong\u003e\u003cem\u003eBootstrap Aggregation\u003c/em\u003e\u003c/strong\u003e (AKA \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e) to obtain a portion of our data by sampling with replacement. For each tree, we sample two-thirds of our training data with replacement -- this is the data that will be used to build our tree. The remaining data is used as an internal test set to test each tree -- this remaining one-third is referred to as \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Data\u003c/em\u003e\u003c/strong\u003e, or \u003cstrong\u003e\u003cem\u003eOOB\u003c/em\u003e\u003c/strong\u003e. For each new tree created, the algorithm then uses the remaining one-third of data that wasn't sampled to calculate the \u003cstrong\u003e\u003cem\u003eOut-Of-Bag Error\u003c/em\u003e\u003c/strong\u003e, in order to get a running, unbiased estimate of overall tree performance for each tree in the forest. \u003c/p\u003e\n\n\u003cp\u003eTraining each tree on its own individual \"bag\" of data is a great start for getting us some variability between the decision trees in our forest. However, with just bagging, all the trees are still focusing on all the same predictors. This allows for a potential weakness to affect all the trees at once -- if a predictor that usually provides strong signal provides bad information for a given observation, then it's likely that all the trees will fall for this false signal and make the wrong prediction. This is where the second major part of the Random forest algorithm comes in!\u003c/p\u003e\n\n\u003ch2\u003eSubspace sampling method\u003c/h2\u003e\n\n\u003cp\u003eAfter bagging the data, the random forest uses the \u003cstrong\u003e\u003cem\u003eSubspace sampling method\u003c/em\u003e\u003c/strong\u003e to further increase variability between the trees. Although it has a fancy mathematical-sounding name, all this method does is randomly select a subset of features to use as predictors for each node when training a decision tree, instead of using all predictors available at each node. \u003c/p\u003e\n\n\u003cp\u003eLet's pretend we're training our random forest on a dataset with 3000 rows and 10 columns. For each given tree, we would randomly \"bag\" 2000 rows with replacement. Next, we perform a subspace sample by randomly selecting a number of predictors at each node of a decision tree. Exactly how many predictors are used is a tunable parameter for this algorithm -- for simplicity's sake, let's assume we pick 6 predictors in this example. \u003c/p\u003e\n\n\u003cp\u003eThis brings us to the following pseudocode so far:\u003c/p\u003e\n\n\u003cp\u003eFor each tree in the dataset:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eBag 2/3 of the overall data -- in our example, 2000 rows \u003c/li\u003e\n\u003cli\u003eRandomly select a set number of features to use for training each node within this -- in this example, 6 features\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eTrain the tree on the modified dataset, which is now a DataFrame consisting of 2000 rows and 6 columns\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eDrop the unused columns from step 3 from the out-of-bag rows that weren't bagged in step 1, and then use this as an internal testing set to calculate the out-of-bag error for this particular tree \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-random-forests/master/images/new_rf-diagram.png\" width=\"750\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eResiliency to overfitting\u003c/h3\u003e\n\n\u003cp\u003eOnce we've created our target number of trees, we'll be left with a random forest filled with a diverse set of decision trees that are trained on different sets of data, and also look at different subsets of features to make predictions. This amount of diversity among the trees in our forest will make for a model that is extremely resilient to noisy data, thus reducing the chance of overfitting.\u003c/p\u003e\n\n\u003cp\u003eTo understand why this is the case, let's put it in practical terms. Let's assume that of the 10 columns that we mentioned in our hypothetical dataset, column 2 correlates heavily with our target. However, there is still some noise in this dataset, and this column doesn't correlate \u003cem\u003eperfectly\u003c/em\u003e with our target -- there will be times where it suggests one class or another, but this isn't actually the case -- let's call these rows \"false signals\". In the case of a single decision tree, or even a forest where all trees focus on all the same predictors, we can expect to get the model to almost always get these false signal examples wrong. Why? Because the model will have learned to treat column 2 as a \"star player\" of sorts. When column 2 provides a false signal, our model will fall for it and get the prediction wrong. \u003c/p\u003e\n\n\u003cp\u003eNow, let's assume that we have a random forest complete with subspace sampling. If we randomly use 6 out of 10 predictors when creating each node of each tree, then this means that ~40% of the nodes of the trees in our forest won't even know column 2 exists! In the cases where column 2 provides a \"false signal\", the nodes of trees that use column 2 will likely make an incorrect prediction -- but that only matters to the ~60% that look at column 2. Our forest still contains another 40% of nodes within trees that are essentially \"immune\" to the false signal in column 2, because they don't use that predictor. In this way, the \"wisdom of the crowd\" buffers the performance of every constituent in that crowd. Although for any given example, some trees may draw the wrong conclusion from a particular predictor, the odds that \u003cem\u003eevery tree\u003c/em\u003e makes the same mistake because they looked at the same predictor is infinitesimally small!\u003c/p\u003e\n\n\u003ch3\u003eMaking predictions with random forests\u003c/h3\u003e\n\n\u003cp\u003eOnce we have trained all the trees in our random forest, we can effectively use it to make predictions! When given data to make predictions on, the algorithm provides only the appropriate features to each tree in the forest, gets that tree's individual prediction, and then aggregates all predictions together to determine the overall prediction that the algorithm will make for said data. In essence, each tree \"votes\" for the prediction that the forest will make, with the majority winning. \u003c/p\u003e\n\n\u003ch2\u003eBenefits and drawbacks\u003c/h2\u003e\n\n\u003cp\u003eLike any algorithm, the random forest comes with its own benefits and drawbacks. \u003c/p\u003e\n\n\u003ch3\u003eBenefits\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eStrong performance\u003c/em\u003e\u003c/strong\u003e: The random forest algorithm usually has very strong performance on most problems, when compared with other classification algorithms. Because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInterpretability\u003c/em\u003e\u003c/strong\u003e:  Conveniently, since each tree in the random forest is a \u003cstrong\u003e\u003cem\u003eGlass-Box Model\u003c/em\u003e\u003c/strong\u003e (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well! You'll demonstrate this yourself in the upcoming lab, by inspecting feature importances for both individual trees and the entire random forest. \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eDrawbacks\u003c/h3\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eComputational complexity\u003c/em\u003e\u003c/strong\u003e: Like any ensemble method, training multiple models means paying the computational cost of training each model. On large datasets, the runtime can be quite slow compared to other algorithms.\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e\u003cem\u003eMemory usage\u003c/em\u003e\u003c/strong\u003e: Another side effect of the ensembled nature of this algorithm, having multiple models means storing each in memory. Random forests tend to have a larger memory footprint than other models. Whereas a parametric model like a logistic regression just needs to store each of the coefficients, a random forest has to remember every aspect of every tree! It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens or even hundreds of MB. For data scientists working on modern computers, this isn't typically a problem -- however, there are special cases where the memory footprint can make this an untenable choice -- for instance, an app on a smartphone that uses machine learning may not be able to afford to spend that much disk space on a random forest model!\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e(Optional) Random forests White paper\u003c/h2\u003e\n\n\u003cp\u003eThis algorithm was not invented all at once -- there were several iterations by different researchers that built upon each previous idea. However, the version used today is the one created by Leo Breiman and Adele Cutler, who also own the trademark for the name \"random forest\". \u003c/p\u003e\n\n\u003cp\u003eAlthough not strictly necessary for understanding how to use random forests, we highly recommend taking a look at the following resources from Breiman and Cutler if you're interested in really digging into how random forests work:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/randomforest2001.pdf\"\u003eRandom forests paper\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.stat.berkeley.edu/%7Ebreiman/RandomForests/cc_home.htm\"\u003eRandom forests website\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about a random forest, which is a powerful and popular ensemble method that uses decision trees!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-random-forests\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-random-forests\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-random-forests/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"short-video-bias-variance-tradeoff","title":"Short Video: Bias-Variance Tradeoff","type":"WikiPage","content":"\u003cdiv style=\"padding: 62.5% 0 0 0; position: relative;\"\u003e\u003ciframe style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\" title=\"one-hot_encoding_phase2_gd\" src=\"https://player.vimeo.com/video/713478315?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" allowfullscreen=\"allowfullscreen\" allow=\"autoplay; fullscreen; picture-in-picture\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"short-video-regression-trees","title":"Short Video: Regression Trees","type":"WikiPage","content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802712?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"machine-learning-fundamentals-recap","title":"Machine Learning Fundamentals - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ml-fundamentals-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you used a familiar model, linear regression, to learn about machine learning fundamentals.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eInference\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eprediction\u003c/em\u003e\u003c/strong\u003e are two different use cases for statistical modeling\n\n\u003cul\u003e\n\u003cli\u003eLinear regression can be used for both kinds of modeling\u003c/li\u003e\n\u003cli\u003eWhile inference is most interested in understanding relationships in the data, prediction is most interested in \u003cstrong\u003e\u003cem\u003egeneralization\u003c/em\u003e\u003c/strong\u003e, i.e. making good predictions on unseen data\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eModel validation\u003c/em\u003e\u003c/strong\u003e techniques allow you to measure how well your model generalizes\n\n\u003cul\u003e\n\u003cli\u003eThe two most popular validation techniques are \u003cstrong\u003e\u003cem\u003etrain-test split\u003c/em\u003e\u003c/strong\u003e (which randomly shuffles the data into a training set and a test set) and \u003cstrong\u003e\u003cem\u003ecross-validation\u003c/em\u003e\u003c/strong\u003e (which splits the data into folds and uses one fold at a time as the test set)\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003ePredictive modeling involves balancing between \u003cstrong\u003e\u003cem\u003ebias\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003evariance\u003c/em\u003e\u003c/strong\u003e\n\n\u003cul\u003e\n\u003cli\u003eBias is associated with \u003cstrong\u003e\u003cem\u003eunderfitting\u003c/em\u003e\u003c/strong\u003e and less-complex models\u003c/li\u003e\n\u003cli\u003eAn example of a less-complex model would be linear regression\u003c/li\u003e\n\u003cli\u003eA high-bias model will perform poorly on both the training and the test data\u003c/li\u003e\n\u003cli\u003eVariance is associated with \u003cstrong\u003e\u003cem\u003eoverfitting\u003c/em\u003e\u003c/strong\u003e and more-complex models\u003c/li\u003e\n\u003cli\u003eAn example of a more-complex model would be polynomial regression\u003c/li\u003e\n\u003cli\u003eA high-variance model will perform well on the training data and poorly on the test data\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eRegularization\u003c/em\u003e\u003c/strong\u003e is a technique to reduce overfitting\n\n\u003cul\u003e\n\u003cli\u003eTwo popular penalized coefficient regularization techniques are \u003cstrong\u003e\u003cem\u003eridge\u003c/em\u003e\u003c/strong\u003e regression and \u003cstrong\u003e\u003cem\u003elasso\u003c/em\u003e\u003c/strong\u003e regression\u003c/li\u003e\n\u003cli\u003eLasso regression penalizes coefficients to 0, which means that it can be used for \u003cstrong\u003e\u003cem\u003efeature selection\u003c/em\u003e\u003c/strong\u003e in addition to other feature selection techniques such as recursive feature elimination\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"gridsearchcv","title":"GridSearchCV","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gridsearchcv\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we'll explore the concept of parameter tuning to maximize our model performance using a combinatorial grid search!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDesign a parameter grid for use with scikit-learn's \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e to increase model performance through parameter tuning\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eParameter tuning\u003c/h2\u003e\n\n\u003cp\u003eBy now, you've seen that the process of building and training a supervised learning model is an iterative one. Your first model rarely performs the best! There are multiple ways we can potentially improve model performance. Thus far, most of the techniques we've used have been focused on our data. We can get better data, or more data, or both. We can engineer certain features, or clean up the data by removing rows/variables that hurt model performance, like multicollinearity. \u003c/p\u003e\n\n\u003cp\u003eThe other major way to potentially improve model performance is to find good parameters to set when creating the model. For example, if we allow a decision tree to have too many leaves, the model will almost certainly overfit the data. Too few, and the model will underfit. However, each modeling problem is unique -- the same parameters could cause either of those situations, depending on the data, the task at hand, and the complexity of the model needed to best fit the data. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll learn how we can use a \u003cstrong\u003e\u003cem\u003ecombinatorial grid search\u003c/em\u003e\u003c/strong\u003e to find the best combination of parameters for a given model. \u003c/p\u003e\n\n\u003ch2\u003eGrid search\u003c/h2\u003e\n\n\u003cp\u003eWhen we set parameters in a model, the parameters are not independent of one another -- the value set for one parameter can have significant effects on other parameters, thereby affecting overall model performance. Consider the following grid.\u003c/p\u003e\n\n\u003ctable\u003e\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center\"\u003eParameter\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e1\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e2\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e3\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003e4\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003ecriterion\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\"gini\"\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\"entropy\"\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003emax_depth\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e2\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003emin_samples_split\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e1\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e5\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e10\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003e20\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eAll the parameters above work together to create the framework of the decision tree that will be trained. For a given problem, it may be the case that increasing the value of the parameter for \u003ccode\u003emin_samples_split\u003c/code\u003e generally improves model performance up to a certain point, by reducing overfitting. However, if the value for \u003ccode\u003emax_depth\u003c/code\u003e is too low or too high, this may doom the model to overfitting or underfitting, by having a tree with too many arbitrary levels and splits that overfit on noise, or limiting the model to nothing more than a \"stump\" by only allowing it to grow to one or two levels. \u003c/p\u003e\n\n\u003cp\u003eSo how do we know which combination of parameters is best? The only way we can really know for sure is to try \u003cstrong\u003e\u003cem\u003eevery single combination!\u003c/em\u003e\u003c/strong\u003e For this reason, grid search is sometimes referred to as an \u003cstrong\u003e\u003cem\u003eexhaustive search\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003ch2\u003eUse \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library provides an easy way to tune model parameters through an exhaustive search by using its \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\"\u003e\u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/a\u003e class, which can be found inside the \u003ccode\u003emodel_selection\u003c/code\u003e module. \u003ccode\u003eGridsearchCV\u003c/code\u003e combines \u003cstrong\u003e\u003cem\u003eK-Fold Cross-Validation\u003c/em\u003e\u003c/strong\u003e with a grid search of parameters. In order to do this, we must first create a \u003cstrong\u003e\u003cem\u003eparameter grid\u003c/em\u003e\u003c/strong\u003e that tells \u003ccode\u003esklearn\u003c/code\u003e which parameters to tune, and which values to try for each of those parameters. \u003c/p\u003e\n\n\u003cp\u003eThe following code snippet demonstrates how to use \u003ccode\u003eGridSearchCV\u003c/code\u003e to perform a parameter grid search using a sample parameter grid, \u003ccode\u003eparam_grid\u003c/code\u003e. Our parameter grid should be a dictionary, where the keys are the parameter names, and the values are the different parameter values we want to use in our grid search for each given key. After creating the dictionary, all you need to do is pass it to \u003ccode\u003eGridSearchCV()\u003c/code\u003e along with the classifier. You can also use K-fold cross-validation during this process, by specifying the \u003ccode\u003ecv\u003c/code\u003e parameter. In this case, we choose to use 3-fold cross-validation for each model created inside our grid search. \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eclf\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eDecisionTreeClassifier\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eparam_grid\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'criterion'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e'gini'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'entropy'\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'max_depth'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'min_samples_split'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e20\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003egs_tree\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eGridSearchCV\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclf\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eparam_grid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003egs_tree\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003etrain_data\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etrain_labels\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003egs_tree\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebest_params_\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis code will run all combinations of the parameters above. The first model to be trained would be \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=1)\u003c/code\u003e using a 3-fold cross-validation, and recording the average score. Then, it will change one parameter, and repeat the process (e.g., \u003ccode\u003eDecisionTreeClassifier(criterion='gini', max_depth=1, min_samples_split=5)\u003c/code\u003e, and so on), keeping track of the overall performance of each model. Once it has tried every combination, the \u003ccode\u003eGridSearchCV\u003c/code\u003e object we created will automatically default the model that had the best score. We can even access the best combination of parameters by checking the \u003ccode\u003ebest_params_\u003c/code\u003e attribute! \u003c/p\u003e\n\n\u003ch2\u003eDrawbacks of \u003ccode\u003eGridSearchCV\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eGridSearchCV is a great tool for finding the best combination of parameters. However, it is only as good as the parameters we put in our parameter grid -- so we need to be very thoughtful during this step! \u003c/p\u003e\n\n\u003cp\u003eThe main drawback of an exhaustive search such as \u003ccode\u003eGridsearchCV\u003c/code\u003e is that there is no way of telling what's best until we've exhausted all possibilities! This means training many versions of the same machine learning model, which can be very time consuming and computationally expensive. Consider the example code above -- we have three different parameters, with 2, 4, and 4 variations to try, respectively. We also set the model to use cross-validation with a value of 3, meaning that each model will be built 3 times, and their performances averaged together. If we do some simple math, we can see that this simple grid search we see above actually results in \u003ccode\u003e2 * 4 * 4 * 3 =\u003c/code\u003e \u003cstrong\u003e\u003cem\u003e96 different models trained!\u003c/em\u003e\u003c/strong\u003e For projects that involve complex models and/or very large datasets, the time needed to run a grid search can often be prohibitive. For this reason, be very thoughtful about the parameters you set -- sometimes the extra runtime isn't worth it -- especially when there's no guarantee that the model performance will improve!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about grid search, how to perform grid search, and the drawbacks associated with the method!\u003c/p\u003e","frontPage":false},{"exportId":"logistic-regression-recap","title":"Logistic Regression - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eWell done! In this section you learned about a different supervised learning technique: classification.\u003c/p\u003e\n\n\u003cp\u003eYou also reviewed maximum likelihood estimation and saw how it applies to logistic regression. This included writing some challenging code, including gradient descent, which pushed you to think critically regarding algorithm implementation.\u003c/p\u003e\n\n\u003ch2\u003eLogistic Regression\u003c/h2\u003e\n\n\u003cp\u003eSpecifically, you practiced building a very basic classification model from scratch - a logistic regression model. Logistic regression uses a sigmoid function which helps to plot an \"s\"-like curve that enables a linear function to act as a binary classifier.\u003c/p\u003e\n\n\u003ch2\u003eLog-likelihoods in Maximum Likelihood Estimation\u003c/h2\u003e\n\n\u003cp\u003eOne of the nuances you saw in maximum likelihood estimation was that of log-likelihoods. Recall that the purpose of taking log-likelihoods as opposed to likelihoods themselves is that it allows us to decompose the product of probabilities as sums of log probabilities. Analytically, this is essential to calculating subsequent gradients in order to find the next steps for our optimization algorithm.\u003c/p\u003e\n\n\u003ch2\u003eLocal Minima in Gradient Descent\u003c/h2\u003e\n\n\u003cp\u003eOne of the most important notes from this section is that \u003cstrong\u003egradient descent does not guarantee an optimal solution\u003c/strong\u003e. Gradient descent is meant to find optimal solutions, but it only guarantees a local minimum. For this reason, gradient descent is frequently run multiple times, and the parameters with the lowest loss function then being selected for the final model.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section was designed to give you additional practice coding algorithms in Python, and a deeper understanding of how iterative algorithms such as logistic regression converge to produce underlying model parameters.\u003c/p\u003e","frontPage":false},{"exportId":"systems-of-linear-equations","title":"Systems of Linear Equations","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lingalg-linear-equations\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eLinear algebra is a sub-field of mathematics concerned with vectors, matrices, and linear transforms between them. \nThe first step towards developing a good understanding of linear algebra is to get a good sense of \u003cem\u003ewhat linear mappings and linear equations\u003c/em\u003e are, \u003cem\u003ehow these relate to vectors and matrices\u003c/em\u003e and \u003cem\u003ewhat this has to do with data analysis\u003c/em\u003e. Let's try to develop a basic intuition around these ideas by first understanding what linear equations are. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe a system of linear equations for solving analytical problems \u003c/li\u003e\n\u003cli\u003eDescribe how matrices and vectors can be used to solve linear equations \u003c/li\u003e\n\u003cli\u003eSolve a system of equations using elimination and substitution \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat are linear equations?\u003c/h2\u003e\n\n\u003cp\u003eIn mathematics, a system of linear equations (or linear system) is a collection of two or more linear equations involving the same set of variables. For example, look at the following equations: \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" 3x + 2y - z = 0 \" src=\"https://learning.flatironschool.com/equation_images/%203x%20+%202y%20-%20z%20=%200\" alt=\"{\" data-equation-content=\" 3x + 2y - z = 0 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" 2x- 2y + 4z = -2 \" src=\"https://learning.flatironschool.com/equation_images/%202x-%202y%20+%204z%20=%20-2\" alt=\"{\" data-equation-content=\" 2x- 2y + 4z = -2 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" -x + 0.5y - z = 0 \" src=\"https://learning.flatironschool.com/equation_images/%20-x%20+%200.5y%20-%20z%20=%200\" alt=\"{\" data-equation-content=\" -x + 0.5y - z = 0 \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is a system of three equations in the three variables \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e, \u003cimg class=\"equation_image\" title=\"y\" src=\"https://learning.flatironschool.com/equation_images/y\" alt=\"{\" data-equation-content=\"y\"\u003e, and \u003cimg class=\"equation_image\" title=\"z\" src=\"https://learning.flatironschool.com/equation_images/z\" alt=\"{\" data-equation-content=\"z\"\u003e. A solution to a linear system is an assignment of values to the variables in a way that \u003cem\u003eall the equations are simultaneously satisfied\u003c/em\u003e. A solution to the system above is given by:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" x = 1 \" src=\"https://learning.flatironschool.com/equation_images/%20x%20=%201\" alt=\"{\" data-equation-content=\" x = 1 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" y = -8/3 \" src=\"https://learning.flatironschool.com/equation_images/%20y%20=%20-8/3\" alt=\"{\" data-equation-content=\" y = -8/3 \"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" z = -7/3 \" src=\"https://learning.flatironschool.com/equation_images/%20z%20=%20-7/3\" alt=\"{\" data-equation-content=\" z = -7/3 \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eThese values make all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\u003c/p\u003e\n\n\u003ch2\u003eSolving linear equations\u003c/h2\u003e\n\n\u003cp\u003eA system of linear equations can always be expressed in a matrix form. Algebraically, both of these express the same thing. Let's work with an example to see how this works: \u003c/p\u003e\n\n\u003ch3\u003eExample\u003c/h3\u003e\n\n\u003cp\u003eLet's say you go to a market and buy 2 apples and 1 banana. For this, you end up paying 35 pence. If you denote apples by \u003cimg class=\"equation_image\" title=\"a\" src=\"https://learning.flatironschool.com/equation_images/a\" alt=\"{\" data-equation-content=\"a\"\u003e and bananas by \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e, the relationship between items bought and the price paid can be written down as an equation - let's call it Eq. A: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"2a + b = 35\" src=\"https://learning.flatironschool.com/equation_images/2a%20+%20b%20=%2035\" alt=\"{\" data-equation-content=\"2a + b = 35\"\u003e  - (Eq. A)\u003c/p\u003e\n\n\u003cp\u003eOn your next trip to the market, you buy 3 apples and 4 bananas, and the cost is 65 pence. Just like above, this can be written as Eq. B:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"3a + 4b = 65\" src=\"https://learning.flatironschool.com/equation_images/3a%20+%204b%20=%2065\" alt=\"{\" data-equation-content=\"3a + 4b = 65\"\u003e - (Eq. B)\u003c/p\u003e\n\n\u003cp\u003eThese two equations (known as a simultaneous equations) form a system that can be solved by hand for values of \u003cimg class=\"equation_image\" title=\"a\" src=\"https://learning.flatironschool.com/equation_images/a\" alt=\"{\" data-equation-content=\"a\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e i.e., price of a single apple and banana.\u003c/p\u003e\n\n\u003cp\u003eLet's solve this system for individual prices using a series of eliminations and substitutions:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStep 1:\u003c/strong\u003e Multiply Eq. A by 4\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"8a + 4b = 140\" src=\"https://learning.flatironschool.com/equation_images/8a%20+%204b%20=%20140\" alt=\"{\" data-equation-content=\"8a + 4b = 140\"\u003e - (Eq. C)\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStep 2 :\u003c/strong\u003e Subtract Eq. B from Eq. C\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"5a = 75\" src=\"https://learning.flatironschool.com/equation_images/5a%20=%2075\" alt=\"{\" data-equation-content=\"5a = 75\"\u003e which leads to \u003cimg class=\"equation_image\" title=\"a = 15\" src=\"https://learning.flatironschool.com/equation_images/a%20=%2015\" alt=\"{\" data-equation-content=\"a = 15\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eStep 3:\u003c/strong\u003e Substitute the value of \u003cimg class=\"equation_image\" title=\"a\" src=\"https://learning.flatironschool.com/equation_images/a\" alt=\"{\" data-equation-content=\"a\"\u003e in Eq. A\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"30 + b = 35\" src=\"https://learning.flatironschool.com/equation_images/30%20+%20b%20=%2035\" alt=\"{\" data-equation-content=\"30 + b = 35\"\u003e which leads to \u003cimg class=\"equation_image\" title=\"b = 5\" src=\"https://learning.flatironschool.com/equation_images/b%20=%205\" alt=\"{\" data-equation-content=\"b = 5\"\u003e\u003c/p\u003e\n\n\u003cp\u003eSo the price of an apple is 15 pence and the price of the banana is 5 pence. \u003c/p\u003e\n\n\u003ch2\u003eFrom equations to vectors and matrices\u003c/h2\u003e\n\n\u003cp\u003eNow, as your number of shopping trips increase along with the number of items you buy at each trip, the system of equations will become more complex and solving a system for individual price may become very expensive in terms of time and effort. In these cases, you can use a computer to find the solution.\u003c/p\u003e\n\n\u003cp\u003eThe above example is a classic linear algebra problem. The numbers 2 and 1 from Eq. A and 3 and 4 from Eq. B are linear coefficients that relate input variables a and b to the known output 15 and 5.\u003c/p\u003e\n\n\u003cp\u003eUsing linear algebra, we can write this system of equations as shown below: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-lingalg-linear-equations/master/images/ss.png\" width=\"320\"\u003e\u003c/p\u003e\n\n\u003cp\u003eYou see that in order for a computational algorithm to solve this (and other similar) problems, we need to first convert the data we have into a set of matrix and vector objects. Machine learning involves building up these objects from the given data, understanding their relationships and how to process them for a particular problem. \u003c/p\u003e\n\n\u003cp\u003eSolving these equations requires knowledge of defining these vectors and matrices in a computational environment and of operations that can be performed on these entities to solve for unknown variables as we saw above. We'll look into how to do this in upcoming lessons. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned how a system of linear (simultaneous) equations can be solved using elimination and substitution, and also, how to covert these problems into matrices and vectors to be processed by computational algorithms. In the next couple of lessons, we'll look at how to describe these entities in Python and NumPy and also how to perform arithmetic operations to solve these types of equations.\u003c/p\u003e","frontPage":false},{"exportId":"gradient-to-cost-function-appendix","title":"Gradient to Cost Function - Appendix","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-to-cost-function-appendix\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-appendix/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll find the details on how to compute the partial derivatives in the \"Gradient to cost function\" lesson.\u003c/p\u003e\n\u003ch2\u003eComputing the First Partial Derivative\u003c/h2\u003e\n\u003cp\u003eLet's start with taking the \u003cstrong\u003epartial derivative\u003c/strong\u003e with respect to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=m\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=%20%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow this is a tricky function to take the derivative of. So we can use functional composition followed by the chain rule to make it easier. Using functional composition, we can rewrite our function \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e as two functions:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)%5Cmspace%7B5ex%7D\"\u003e -- set \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e equal to \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y-%5Chat%7By%7D\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2%5Cmspace%7B4ex%7D\"\u003e -- now \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J\"\u003e is a function of \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J=g%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eNow using the chain rule to find the partial derivative with respect to a change in the slope, gives us:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdm%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))*%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eBecause \u003cstrong\u003eg\u003c/strong\u003e is a function of \u003cstrong\u003em\u003c/strong\u003e we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e and\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJ\u003c/strong\u003e is a function of \u003cstrong\u003eg (which is a function of m\u003c/strong\u003e) we get \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e .\u003c/p\u003e\n\u003cp\u003eOur next step is to solve these derivatives individually.\u003c/p\u003e\n\u003cp\u003eFirst:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7BdJ%7D%7Bdg%7D%7D(J)\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,%20b))%20=%202*g(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20(mx%20%2bb))\"\u003e\u003c/p\u003e\n\u003cp\u003eSolve \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cboldsymbol%7B%5Cfrac%7Bdg%7D%7Bdm%7D%7D(g)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdm%7D%20(y%20-%20mx%20-%20b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%5Cfrac%7Bdg%7D%7Bdm%7Dy%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Dmx%20-%20%5Cfrac%7Bdg%7D%7Bdm%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=%200-x-0\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B9ex%7D=-x\"\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eEach of the terms are treated as constants, except for the middle term.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eNow plugging these back into our chain rule [1] we have:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdm%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%5Ccolor%7Bblue%7D%7B(2*g(m,b))%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B-x%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B21.75ex%7D=%202*(y%20-%20(mx%20%2b%20b))*-x\"\u003e\u003c/p\u003e\n\u003cp\u003eSo\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B1%5D%5Cmspace%7B5ex%7D%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20m%7DJ(m,%20b)%20=2*(y%20-%20(mx%20%2b%20b))*-x\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B15.75ex%7D=%20-2x*(y%20-%20(mx%20%2b%20b%20))\"\u003e\u003c/p\u003e\n\u003ch2\u003eComputing the Second Partial Derivative\u003c/h2\u003e\n\u003cp\u003eOk, now let's calculate the partial derivative with respect to a change in the y-intercept. We express this mathematically with the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7B%5Cdelta%20J%7D%7B%5Cdelta%20b%7DJ(m,%20b)%20=%20%5Cfrac%7BdJ%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eThen once again, we use functional composition following by the chain rule. So we view our cost function as the same two functions \u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))\"\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=g(m,b)%20=%20y%20-%20(mx%20%2b%20b)\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=J(g(m,b))%20=%20(g(m,b))%5E2\"\u003e\u003c/p\u003e\n\u003cp\u003eSo applying the chain rule, to this same function composition, we get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5B2%5D%5Cmspace%7B5ex%7D%5Cfrac%7BdJ%7D%7Bdb%7DJ(g)%20=%20%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)*%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e\u003c/p\u003e\n\u003cp\u003eNow, our next step is to calculate these partial derivatives individually.\u003c/p\u003e\n\u003cp\u003eFrom our earlier calculation of the partial derivative, we know that \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7BdJ%7D%7Bdg%7DJ(g(m,b))%20=%20%5Cfrac%7BdJ%7D%7Bdg%7Dg(m,b)%5E2%20=%202*g(m,b)\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe only thing left to calculate is \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)\"\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%20=%5Cfrac%7Bdg%7D%7Bdb%7D(y%20-%20(mx%20%2b%20b)%20)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdg%7D%7Bdb%7D(y-mx-b)\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%5Cfrac%7Bdb%7D%7Bdb%7Dy-%5Cfrac%7Bdb%7D%7Bdb%7Dmx-%5Cfrac%7Bdg%7D%7Bdb%7Db\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=0-0-1\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B8.5ex%7D=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eNow we plug our terms into our chain rule [2] and get:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Ccolor%7Bblue%7D%7B%5Cfrac%7BdJ%7D%7Bdg%7DJ(g)%7D%5Ccolor%7Bblack%7D%7B*%7D%5Ccolor%7Bred%7D%7B%5Cfrac%7Bdg%7D%7Bdb%7Dg(m,b)%7D%20%5Ccolor%7Bblack%7D%7B=%7D%20%5Ccolor%7Bblue%7D%7B2*g(m,b)%7D*%5Ccolor%7Bred%7D%7B-1%7D\"\u003e \u003cbr\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cmspace%7B16ex%7D=-2*(y%20-%20(mx%20%2b%20b))\"\u003e\u003c/p\u003e","frontPage":false},{"exportId":"building-an-object-oriented-simulation","title":"Building an Object-Oriented Simulation","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn a bit more about best practices for running simulations in the real world using object-oriented programming!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse inheritance to write nonredundant code\u003c/li\u003e\n\u003cli\u003eCreate methods that calculate statistics of the attributes of an object\u003c/li\u003e\n\u003cli\u003eCreate object-oriented data models that describe the real world with multiple classes and subclasses and interaction between classes\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eCreating Stochastic Simulations\u003c/h2\u003e\n\n\u003cp\u003eAs a capstone for everything you've learned about object-oriented programming, you'll be creating a \u003cstrong\u003e\u003cem\u003eHerd Immunity Simulation\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/master/images/herd_immunity.gif\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/dataisbeautiful/comments/5v72fw/how_herd_immunity_works_oc/\"\u003egif created by u/theotheredmund\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThis simulation is meant to model the effects that vaccinations have on the way a communicable disease spreads through a population. The simulation you're building depends on just a few statistics from the CDC (Center for Disease Control):\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003er0\u003c/code\u003e, the average number of people a contagious person infects before they are no longer contagious (because they got better or they died)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emortality_rate\u003c/code\u003e, the percentage chance a person infected with a disease will die from it \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe main workflow of this simulation is as follows:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003ePerson()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003ealive\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evaccinated\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehas_been_infected\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enewly_infected\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a \u003ccode\u003eSimulation()\u003c/code\u003e class with the following attributes:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epopulation\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003evirus_name\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enum_time_steps\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003er0\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epercent_pop_vaccinated\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eCreate methods for our \u003ccode\u003eSimulation()\u003c/code\u003e class that will cover each step of the simulation. \u003c/p\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIn order for our simulation to work, you'll need to define some rules for it:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eEach infected person will \"interact\" with 100 random people from the \u003ccode\u003epopulation\u003c/code\u003e. If the person the infected individual interacts with is sick, vaccinated, or has had the disease before, nothing happens. However, if the person the infected individual interacts with is healthy, unvaccinated, and has not been infected yet, then that person becomes infected. \u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAt the end of each round, the following things happen:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eAll currently infected people either get better from the disease or die, with the chance of death corresponding to the mortality rate of the disease \u003c/li\u003e\n\u003cli\u003eAll people that were newly infected during this round become the new infected for the next round\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe simulation continues for the set number of time steps.  Any time someone dies or gets infected, log it in a text file called \u003ccode\u003e'simulation_logs.txt'\u003c/code\u003e.  Once the simulation is over, write some code to quickly parse the text logs into data and visualize the results, so that you can run multiple simulations and answer questions like: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIf vaccination rates for {disease x} dropped by 5%, how many more people become infected in an epidemic? How many more die?\u003c/li\u003e\n\u003cli\u003eWhat does the spread of {disease x} through a population look like?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf this all seems a bit daunting, don't worry! You'll be provided with much more detail as you build this step-by-step during the lab. \u003c/p\u003e\n\n\u003cp\u003eWith that, go ahead and take a look at this cool simulation lab!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you reviewed some best practices for simulating things in the real world using object-oriented programming!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-building-an-object-oriented-simulation\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"short-video-the-gradient","title":"Short Video: The Gradient","type":"WikiPage","content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802605?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"topic-29-lesson-priorities-live","title":"Topic 29 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.9064%; height: 106px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eModel Tuning\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Model Tuning and Pipelines - Introduction\" href=\"pages/model-tuning-and-pipelines-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/model-tuning-and-pipelines-introduction\" data-api-returntype=\"Page\"\u003eModel Tuning and Pipelines - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003cstrong\u003e\u003ca title=\"GridSearchCV\" href=\"pages/gridsearchcv\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/gridsearchcv\" data-api-returntype=\"Page\"\u003eGridSearchCV\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Introduction to Pipelines\" href=\"pages/introduction-to-pipelines\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/introduction-to-pipelines\" data-api-returntype=\"Page\"\u003e\u003cstrong\u003eIntroduction to Pipelines\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Pipelines in scikit-learn - Lab\" href=\"assignments/g9460c3b0812bc3c864b5800d801a5245\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187031\" data-api-returntype=\"Assignment\"\u003ePipelines in scikit-learn - Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003ca class=\"ig-title title item_link\" title=\"Refactoring Your Code to Use Pipelines\" href=\"modules/items/g6e46afb5a0281f261c0a9a477ea29717\"\u003eRefactoring Your Code to Use Pipelines\u003c/a\u003e\u003cstrong\u003e\u003ca class=\"ig-title title item_link\" title=\"Refactoring Your Code to Use Pipelines\" href=\"modules/items/g6e46afb5a0281f261c0a9a477ea29717\"\u003e\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003ca title=\"Short Video: Pipelines\" href=\"pages/short-video-pipelines\"\u003eShort Video: Pipelines\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.9064%; height: 106px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eModel Tuning\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003cstrong\u003e\u003ca title=\"Model Tuning Exit Ticket\" href=\"quizzes/g5436ad5ecb4614660bbf52f63218ae19\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30638\" data-api-returntype=\"Quiz\"\u003eModel Tuning Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Pickle\" href=\"assignments/g0576788fe1f7929f3f8b2444c8156ef3\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187029\" data-api-returntype=\"Assignment\"\u003ePickle\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003ca title=\"Pickling and Deploying Pipelines\" href=\"assignments/g8492efd41e732bf37959b70989a9dcb6\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187030\" data-api-returntype=\"Assignment\"\u003ePickling and Deploying Pipelines\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Model Tuning and Pipelines - Recap\" href=\"pages/model-tuning-and-pipelines-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/model-tuning-and-pipelines-recap\" data-api-returntype=\"Page\"\u003eModel Tuning and Pipelines - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","frontPage":false},{"exportId":"knn-with-scikit-learn","title":"KNN with scikit-learn","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll explore how to use scikit-learn's implementation of the K-Nearest Neighbors algorithm. In addition, you'll also learn about best practices for using the algorithm. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eList the considerations when fitting a KNN model using scikit-learn\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy use scikit-learn?\u003c/h2\u003e\n\n\u003cp\u003eWhile you've written your own implementation of the KNN algorithm, scikit-learn adds many backend optimizations which can make the algorithm perform faster and more efficiently. Building your own implementation of any machine learning algorithm is a valuable experience, providing great insight into how said algorithm works. However, in general, you should always use professional toolsets such as scikit-learn whenever possible; since their implementations will always be best-in-class, in a way a single developer or data scientist simply can't hope to rival on their own. In the case of KNN, you'll find scikit-learn's implementation to be much more robust and fast, because of optimizations such as caching distances in clever ways under the hood. \u003c/p\u003e\n\n\u003ch2\u003eRead the \u003ccode\u003esklearn\u003c/code\u003e docs\u003c/h2\u003e\n\n\u003cp\u003eAs a rule of thumb, you should familiarize yourself with any documentation available for any libraries or frameworks you use. scikit-learn provides high-quality documentation. For every algorithm, you'll find a general \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\"\u003edocumentation page\u003c/a\u003e which tells you inputs, parameters, outputs, and caveats of any algorithm. In addition, you'll also find very informative \u003ca href=\"https://scikit-learn.org/stable/modules/neighbors.html#classification\"\u003eUser Guides\u003c/a\u003e that explain both how the algorithm works, and how to best use it, complete with sample code! \u003c/p\u003e\n\n\u003cp\u003eFor example, the following image can be found in the scikit-learn user guide for K-Nearest Neighbors, along with an explanation of how different parameters can affect the overall performance of the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-knn-with-scikit-learn/master/images/knn_docs.png\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eBest practices\u003c/h2\u003e\n\n\u003cp\u003eYou'll also find that scikit-learn provides robust implementations for additional components of the algorithm implementation process such as evaluation metrics. With that, you can easily evaluate models using precision, accuracy, or recall scores on the fly using built-in functions!\u003c/p\u003e\n\n\u003cp\u003eWith that, it's important to focus on practical questions when completing the upcoming lab. In particular, try to focus on the following questions:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eWhat decisions do I need to make regarding my data? How might these decisions affect overall performance?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhich predictors do I need? How can I confirm that I have the right predictors?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat parameter values (if any) should I choose for my model? How can I find the optimal value for a given parameter?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eWhat metrics will I use to evaluate the performance of my model? Why?\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eHow do I know if there's room left for improvement with my model? Are the potential performance gains worth the time needed to reach them?\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eA final note\u003c/h2\u003e\n\n\u003cp\u003eAfter cleaning, preprocessing, and modeling the data in the next lab, you'll be given the opportunity to iterate on your model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you got a brief overview of some of the advantages of using scikit-learn's built-in KNN implementation. While you haven't seen specific code examples, you now have an indispensable resource: the official documentation to guide you. Since it's an incredibly important skill to know where to seek out information and how to digest that into actionable processes, it'll be up to you to piece through the necessary documentation to complete the upcoming lab. Good luck!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-with-scikit-learn\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"computational-complexity-from-ols-to-gradient-descent","title":"Computational Complexity: From OLS to Gradient Descent","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-computational-complexity\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-computational-complexity/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll be introduced to computational complexity. You'll learn about this idea in relationship with OLS regression and see how this may not be the most efficient algorithm to calculate the regression parameters when performing regression with large datasets. You'll set the stage for an optimization algorithm called \"Gradient Descent\" which will be covered in detail later. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe computational complexity and how it is related to Big O notation \u003c/li\u003e\n\u003cli\u003eDescribe why OLS with matrix algebra would become problematic for large/complex data \u003c/li\u003e\n\u003cli\u003eExplain how optimizing techniques such as gradient descent can solve complexity issues\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eComplexities in OLS\u003c/h2\u003e\n\n\u003cp\u003eRecall the OLS formula for calculating the beta vector:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\beta =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\" src=\"/equation_images/%20%255Cbeta%20=(%255Cboldsymbol{X}^T%255Cboldsymbol{X})^{-1}%255Cboldsymbol{X}^T%20y\" alt=\"{\" data-equation-content=\" \\beta =(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis formula looks very simple, elegant, and intuitive. It works perfectly fine for the case of simple linear regression due to a limited number of computed dimensions, but with datasets that are very large or \u003cstrong\u003ebig data\u003c/strong\u003e sets, it becomes computationally very expensive as it can potentially involve a huge number of complex mathematical operations.\u003c/p\u003e\n\n\u003cp\u003eFor this formula, we need to find \u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})\"\u003e, and invert it as well, which makes it very expensive. Imagine the matrix \u003cimg class=\"equation_image\" title=\"X_{(N \\times M+1)}\" src=\"/equation_images/X_{(N%20%255Ctimes%20M+1)}\" alt=\"{\" data-equation-content=\"X_{(N \\times M+1)}\"\u003e has \u003cimg class=\"equation_image\" title=\"(M+1)\" src=\"https://learning.flatironschool.com/equation_images/(M+1)\" alt=\"{\" data-equation-content=\"(M+1)\"\u003e columns where \u003cimg class=\"equation_image\" title=\"M\" src=\"https://learning.flatironschool.com/equation_images/M\" alt=\"{\" data-equation-content=\"M\"\u003e is the number of predictors and \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e is the number of rows of observations. In machine learning, you will often find datasets with \u003cimg class=\"equation_image\" title=\"M \u003e1000\" src=\"/equation_images/M%20\u003e1000\" alt=\"{\" data-equation-content=\"M \u003e1000\"\u003e and \u003cimg class=\"equation_image\" title=\"N \u003e 1,000,000\" src=\"/equation_images/N%20\u003e%201,000,000\" alt=\"{\" data-equation-content=\"N \u003e 1,000,000\"\u003e. The \u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})\"\u003e matrix itself takes a while to calculate, then you have to invert an \u003cimg class=\"equation_image\" title=\"M \\times M\" src=\"https://learning.flatironschool.com/equation_images/M%20%255Ctimes%20M\" alt=\"{\" data-equation-content=\"M \\times M\"\u003e matrix which adds more to the complexity - making it very expensive. You'll also come across situations where the input matrix grows so large that it cannot fit into your computer's memory. \u003c/p\u003e\n\n\u003ch2\u003eThe Big O notation\u003c/h2\u003e\n\n\u003cp\u003eIn computer science, Big O notation is used to describe how \"fast\" an algorithm grows, by comparing the number of operations within the algorithm. Big O notation helps you see the worst-case scenario for an algorithm. Typically, we are most concerned with the Big O time because we are interested in how slowly a given algorithm will possibly run at worst.\u003c/p\u003e\n\n\u003ch4\u003eExample\u003c/h4\u003e\n\n\u003cp\u003eImagine you need to find a person you only know the name of. What's the most straightforward way of finding this person? Well, you could go through every single name in the phone book until you find your target. This is known as a simple search. If the phone book is not very long, with say, only 10 names, this is a fairly fast process. But what if there are 10,000 names in the phone book?\u003c/p\u003e\n\n\u003cp\u003eAt best, your target's name is at the front of the list and you only need to need to check the first item. At worst, your target's name is at the very end of the phone book and you will need to have searched all 10,000 names. As the \"dataset\" (or the phone book) increases in size, the maximum time it takes to run a simple search also linearly increases.\u003c/p\u003e\n\n\u003cp\u003eBig O notation allows you to describe what the worst case is. The worst case is that you will have to search through all elements (\u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e) in the phone book. You can describe the run-time as:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e where \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e is the number of operations\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eBecause the maximum number of operations is equal to the maximum number of elements in our phone book, we say the Big \u003cimg class=\"equation_image\" title=\"O\" src=\"https://learning.flatironschool.com/equation_images/O\" alt=\"{\" data-equation-content=\"O\"\u003e of a simple search is \u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e. \u003cstrong\u003eA simple search will never be slower than \u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e time.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eDifferent algorithms have different run-times. That is, algorithms grow at different rates. The most common Big O run-times, from fastest to slowest, are:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(\\log n)\" src=\"https://learning.flatironschool.com/equation_images/O(%255Clog%20n)\" alt=\"{\" data-equation-content=\"O(\\log n)\"\u003e: aka \u003cimg class=\"equation_image\" title=\"\\log\" src=\"https://learning.flatironschool.com/equation_images/%255Clog\" alt=\"{\" data-equation-content=\"\\log\"\u003e time\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(n)\" src=\"https://learning.flatironschool.com/equation_images/O(n)\" alt=\"{\" data-equation-content=\"O(n)\"\u003e: aka linear time\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(n^2)\" src=\"/equation_images/O(n^2)\" alt=\"{\" data-equation-content=\"O(n^2)\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"O(n^3)\" src=\"/equation_images/O(n^3)\" alt=\"{\" data-equation-content=\"O(n^3)\"\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese rates, as well as some other rates, can be visualized in the following figure:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/big_o.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003ch3\u003eOLS and Big O notation\u003c/h3\u003e\n\n\u003cp\u003eInverting a matrix costs \u003cimg class=\"equation_image\" title=\"O(n^3)\" src=\"/equation_images/O(n^3)\" alt=\"{\" data-equation-content=\"O(n^3)\"\u003e for computation where n is the number of rows in \u003cimg class=\"equation_image\" title=\"X\" src=\"https://learning.flatironschool.com/equation_images/X\" alt=\"{\" data-equation-content=\"X\"\u003e matrix, i.e., the observations. Here is an explanation of how to calculate Big O for OLS.\u003c/p\u003e\n\n\u003cp\u003eOLS linear regression is computed as \u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})^{-1}%255Cboldsymbol{X}^T%20y\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T y\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eIf \u003cimg class=\"equation_image\" title=\"\\boldsymbol{X}\" src=\"/equation_images/%255Cboldsymbol{X}\" alt=\"{\" data-equation-content=\"\\boldsymbol{X}\"\u003e is an \u003cimg class=\"equation_image\" title=\"(n \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(n%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(n \\times k)\"\u003e matrix:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{X})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{X})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{X})\"\u003e takes \u003cimg class=\"equation_image\" title=\"O(n*k^2)\" src=\"/equation_images/O(n*k^2)\" alt=\"{\" data-equation-content=\"O(n*k^2)\"\u003e time and produces a \u003cimg class=\"equation_image\" title=\"(k \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(k%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(k \\times k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe matrix inversion of a (k x k) matrix takes \u003cimg class=\"equation_image\" title=\"O(k^3)\" src=\"/equation_images/O(k^3)\" alt=\"{\" data-equation-content=\"O(k^3)\"\u003e time\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"(\\boldsymbol{X}^T\\boldsymbol{Y})\" src=\"/equation_images/(%255Cboldsymbol{X}^T%255Cboldsymbol{Y})\" alt=\"{\" data-equation-content=\"(\\boldsymbol{X}^T\\boldsymbol{Y})\"\u003e takes \u003cimg class=\"equation_image\" title=\"O(n*k^2)\" src=\"/equation_images/O(n*k^2)\" alt=\"{\" data-equation-content=\"O(n*k^2)\"\u003e time and produces a \u003cimg class=\"equation_image\" title=\"(k \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(k%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(k \\times k)\"\u003e matrix\u003c/li\u003e\n\u003cli\u003eThe final matrix multiplication of two \u003cimg class=\"equation_image\" title=\"(k \\times k)\" src=\"https://learning.flatironschool.com/equation_images/(k%20%255Ctimes%20k)\" alt=\"{\" data-equation-content=\"(k \\times k)\"\u003e matrices takes \u003cimg class=\"equation_image\" title=\"O(k^3)\" src=\"/equation_images/O(k^3)\" alt=\"{\" data-equation-content=\"O(k^3)\"\u003e time\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e** So the Big O running time for OLS is \u003cimg class=\"equation_image\" title=\"O(k^{2*(n + k)})\" src=\"/equation_images/O(k^{2*(n%20+%20k)})\" alt=\"{\" data-equation-content=\"O(k^{2*(n + k)})\"\u003e - which is pretty expensive **\u003c/p\u003e\n\n\u003cp\u003eMoreover, if  \u003cimg class=\"equation_image\" title=\"X\" src=\"https://learning.flatironschool.com/equation_images/X\" alt=\"{\" data-equation-content=\"X\"\u003e is ill-conditioned  (i.e. it isn't a square matrix), there will be computational errors in the estimation. \nAnother common problem is overfitting and underfitting in estimation of regression coefficients.\u003c/p\u003e\n\n\u003cp\u003eSo, this leads us to the gradient descent kind of optimization algorithm which can save us from this type of problem. The main reason why gradient descent is used for linear regression is the computational complexity: it's computationally cheaper (faster) to find the solution using the gradient descent in most cases. \u003c/p\u003e\n\n\u003ch2\u003eGradient Descent\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-computational-complexity/master/images/gradient_descent.png\" width=\"850\"\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eGradient Descent is an iterative approach to minimize the model loss (error), used while training a machine learning model like linear regression. It is an optimization algorithm based on a convex function as shown in the figure above, that tweaks its parameters iteratively to minimize a given function to its local minimum.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn regression, it is used to find the values of model parameters (coefficients, or the \u003cimg class=\"equation_image\" title=\"\\beta\" src=\"https://learning.flatironschool.com/equation_images/%255Cbeta\" alt=\"{\" data-equation-content=\"\\beta\"\u003e matrix) that minimize a cost function (like RMSE) as far as possible.\u003c/p\u003e\n\n\u003cp\u003eIn order to fully understand how this works, you need to know what a gradient is and how is it calculated. And for this, you would need some Calculus. It may sound a bit intimidating at this stage, but don't worry. The next few sections will introduce you to the basics of calculus with gradients and derivatives. \u003c/p\u003e\n\n\u003ch2\u003eFurther Reading\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations\"\u003eWiki: Computational complexity of mathematical operations\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://medium.com/karuna-sehgal/a-simplified-explanation-of-the-big-o-notation-82523585e835\"\u003eSimplified Big O notation\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you learned about the shortcomings and limitations of OLS and matrix inverses. You looked at the Big O notation to explain how calculating inverses and transposes for large matrix might make our analysis unstable and computationally very expensive. This lesson sets a stage for your next section on calculus and gradient descent. You will have a much better understanding of the gradient descent diagram shown above and how it all works by the end of next section. \u003c/p\u003e","frontPage":false},{"exportId":"xgboost","title":"XGBoost","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you are familiar with gradient boosting, you'll learn about the top gradient boosting algorithm currently in use -- XGBoost!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare XGBoost to other boosting algorithms \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhat is XGBoost?\u003c/h2\u003e\n\n\u003cp\u003eGradient boosting is one of the most powerful concepts in machine learning right now. As you've seen, the term \u003cem\u003egradient boosting\u003c/em\u003e refers to a class of algorithms, rather than any single one. The version with the highest performance right now is \u003cstrong\u003e\u003cem\u003eXGBoost\u003c/em\u003e\u003c/strong\u003e, which is short for \u003cstrong\u003e\u003cem\u003eeXtreme Gradient Boosting\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible. There are many under-the-hood optimizations that allow XGBoost to train more quickly than any other library implementations of gradient boosting algorithms. For instance, XGBoost is configured in such a way that it parallelizes the construction of trees across all your computer's CPU cores during the training phase. It also allows for more advanced use cases, such as distributing training across a cluster of computers, which is often a technique used to speed up computation. The algorithm even automatically handles missing values!\u003c/p\u003e\n\n\u003ch2\u003eInstalling \u003ccode\u003eXGBoost\u003c/code\u003e\n\u003c/h2\u003e\n\n\u003cp\u003e\u003ccode\u003eXGBoost\u003c/code\u003e is an independent library that provides implementations in C++, Python, and other languages. Luckily, the open-source community has had the good sense to make the Python API for \u003ccode\u003eXGBoost\u003c/code\u003e mirror that of \u003ccode\u003esklearn\u003c/code\u003e, so using \u003ccode\u003eXGBoost\u003c/code\u003e feels no different than using any other supervised learning algorithm from \u003ccode\u003esklearn\u003c/code\u003e. The only downside is that it does not come packaged with \u003ccode\u003esklearn\u003c/code\u003e, so we must install it ourselves. \u003cstrong\u003econda\u003c/strong\u003e makes this quite easy. \u003c/p\u003e\n\n\u003cp\u003eAll you need to do is run the command \u003ccode\u003econda install py-xgboost\u003c/code\u003e in your terminal, and \u003ccode\u003econda\u003c/code\u003e will take care of the rest!\u003c/p\u003e\n\n\u003ch2\u003eUse cases\u003c/h2\u003e\n\n\u003cp\u003eXGBoost has risen to prominence by being the go-to algorithm for winning competitions on \u003ca href=\"https://www.kaggle.com/\"\u003eKaggle\u003c/a\u003e, a competitive data science platform. It is so common to see XGBoost cited as an algorithm used by the winners of Kaggle competitions that it has become a bit of a running joke in the community. \u003ca href=\"https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions\"\u003eThis page\u003c/a\u003e contains an (incomplete) list of all the recent competitions with place winners that used XGBoost for their solution!\u003c/p\u003e\n\n\u003cp\u003eXGBoost is a great choice for classification tasks. It provides best-in-class performance compared to other classification algorithms (with the exception of Deep Learning, which we'll learn more about soon).\u003c/p\u003e\n\n\u003ch2\u003eTakeaways\u003c/h2\u003e\n\n\u003cp\u003eWhen approaching a supervised learning problem, you should always use multiple algorithms, and compare the performances of the various models. There will always be use cases where some classes of models tend to outperform others. However, there are some models that generally outperform all the others -- XGBoost is at the top of this list! Make sure that this is an algorithm you're familiar with, as there are many situations where you'll find it quite useful!\u003c/p\u003e\n\n\u003cp\u003eYou can find the full documentation for XGBoost \u003ca href=\"https://xgboost.readthedocs.io/en/latest/\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we learned about what XGBoost is, and why it is so powerful and useful to Data Scientists!\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-xgboost\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-xgboost\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-xgboost/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"introduction-to-pipelines","title":"Introduction to Pipelines","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pipelines-v2-1\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've learned a substantial number of different supervised learning algorithms. Now, it's time to learn about a handy tool used to integrate these algorithms into a single manageable pipeline.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain how pipelines can be used to combine various parts of a machine learning workflow\u003cbr\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eWhy Use Pipelines?\u003c/h2\u003e\n\n\u003cp\u003ePipelines are extremely useful tools to write clean and manageable code for machine learning. Recall how we start preparing our dataset: we want to clean our data, transform it, potentially use feature selection, and then run a machine learning algorithm. Using pipelines, you can do all these steps in one go!\u003c/p\u003e\n\n\u003cp\u003ePipeline functionality can be found in scikit-learn's \u003ccode\u003ePipeline\u003c/code\u003e module. Pipelines can be coded in a very simple way:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003esklearn.pipeline\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Create the pipeline\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\u003cspan class=\"p\"\u003e([(\u003c/span\u003e\u003cspan class=\"s\"\u003e'mms'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eMinMaxScaler\u003c/span\u003e\u003cspan class=\"p\"\u003e()),\u003c/span\u003e\n                 \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'tree'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eDecisionTreeClassifier\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erandom_state\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e123\u003c/span\u003e\u003cspan class=\"p\"\u003e))])\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis pipeline will ensure that first we'll apply a Min-Max scaler on our data before fitting a decision tree. However, the \u003ccode\u003ePipeline()\u003c/code\u003e function above is only defining the sequence of actions to perform. In order to actually fit the model, you need to call the \u003ccode\u003e.fit()\u003c/code\u003e method like so: \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e# Fit to the training data\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_train\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_train\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen, to score the model on test data, you can call the \u003ccode\u003e.score()\u003c/code\u003e method like so: \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e# Calculate the score on test data\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escore\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_test\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA really good blog post on the basic ideas of pipelines can be found \u003ca href=\"https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eIntegrating Grid Search in Pipelines\u003c/h2\u003e\n\n\u003cp\u003eNote that the above pipeline simply creates one pipeline for a training set, and evaluates on a test set. Is it possible to create a pipeline that performs grid search? And cross-validation? Yes, it is!\u003c/p\u003e\n\n\u003cp\u003eFirst, you define the pipeline in the same way as above. Next, you create a parameter grid. When this is all done, you use the function \u003ccode\u003eGridSearchCV()\u003c/code\u003e, which you've seen before, and specify the pipeline as the estimator and the parameter grid. You also have to define how many folds you'll use in your cross-validation. \u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e# Create the pipeline\n\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\u003cspan class=\"p\"\u003e([(\u003c/span\u003e\u003cspan class=\"s\"\u003e'mms'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eMinMaxScaler\u003c/span\u003e\u003cspan class=\"p\"\u003e()),\u003c/span\u003e\n                 \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'tree'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eDecisionTreeClassifier\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erandom_state\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e123\u003c/span\u003e\u003cspan class=\"p\"\u003e))])\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Create the grid parameter\n\u003c/span\u003e\u003cspan class=\"n\"\u003egrid\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[{\u003c/span\u003e\u003cspan class=\"s\"\u003e'tree__max_depth'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \n         \u003cspan class=\"s\"\u003e'tree__min_samples_split'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e10\u003c/span\u003e\u003cspan class=\"p\"\u003e]}]\u003c/span\u003e\n\n\n\u003cspan class=\"c1\"\u003e# Create the grid, with \"pipe\" as the estimator\n\u003c/span\u003e\u003cspan class=\"n\"\u003egridsearch\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eGridSearchCV\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eestimator\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003epipe\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                          \u003cspan class=\"n\"\u003eparam_grid\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003egrid\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                          \u003cspan class=\"n\"\u003escoring\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'accuracy'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                          \u003cspan class=\"n\"\u003ecv\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Fit using grid search\n\u003c/span\u003e\u003cspan class=\"n\"\u003egridsearch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_train\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_train\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# Calculate the test score\n\u003c/span\u003e\u003cspan class=\"n\"\u003egridsearch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003escore\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eX_test\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn article with a detailed workflow can be found \u003ca href=\"https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eGreat, this wasn't too difficult! The proof of all this is in the pudding. In the next lab, you'll use this workflow to build pipelines applying classification algorithms you have learned so far in this module. \u003c/p\u003e","frontPage":false},{"exportId":"phase-3-project-description","title":"Phase 3 Project Description","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-phase-3-project-v2-3\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project-v2-3\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-phase-3-project-v2-3/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eCongratulations! You've made it through another \u003cem\u003eintense\u003c/em\u003e module, and now you're ready to show off your newfound Machine Learning skills!\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-phase-3-project-v2-3/main/images/smart.gif\" alt=\"awesome\"\u003e\u003c/p\u003e\n\u003cp\u003eAll that remains in Phase 3 is to put your new skills to use with another large project!\u003c/p\u003e\n\u003cp\u003eIn this project description, we will cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProject Overview\u003c/li\u003e\n\u003cli\u003eDeliverables\u003c/li\u003e\n\u003cli\u003eGrading\u003c/li\u003e\n\u003cli\u003eGetting Started\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eFor this project, you will engage in the full data science process from start to finish, solving a \u003cstrong\u003eclassification\u003c/strong\u003e problem using a \u003cstrong\u003edataset of your choice\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eBusiness Problem and Data\u003c/h3\u003e\n\u003cp\u003eSimilar to the Phase 2 project, it is up to you to define a stakeholder and business problem. Unlike the Phase 2 project, you are also responsible for choosing a dataset.\u003c/p\u003e\n\u003cp\u003eFor complete details, see \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset\"\u003ePhase 3 Project - Choosing a Dataset\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003eKey Points\u003c/h3\u003e\n\u003ch4\u003eClassification\u003c/h4\u003e\n\u003cp\u003eRecall the distinction between \u003cem\u003eclassification\u003c/em\u003e and \u003cem\u003eregression\u003c/em\u003e models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClassification is used when the target variable is a \u003cem\u003ecategory\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eRegression is used when the target variable is a \u003cem\u003enumeric value\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(Categorical data may be represented in the data as numbers, e.g. 0 and 1, but they are not truly numeric values. If you're unsure, ask yourself \"is a target value of 1 \u003cem\u003eone more than\u003c/em\u003e a target value of 0\"; if it is one more, that is a regression target, if not, that is a classification target.)\u003c/p\u003e\n\u003cp\u003eYou already practiced performing a regression analysis in Phase 2, and you will have additional opportunities to work on regression problems in later phases, but \u003cstrong\u003efor this project, you must be modeling a classification problem\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003eFindings and Recommendations\u003c/h4\u003e\n\u003cp\u003eIn the previous two projects, the framing was primarily \u003cem\u003edescriptive\u003c/em\u003e and \u003cem\u003einferential\u003c/em\u003e, meaning that you were trying to understand the distributions of variables and the relationship between them. For this project you can still use these techniques, but make sure you are also using a \u003cstrong\u003e\u003cem\u003epredictive\u003c/em\u003e\u003c/strong\u003e approach.\u003c/p\u003e\n\u003cp\u003eA predictive \u003cem\u003efinding\u003c/em\u003e might include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHow well your model is able to predict the target\u003c/li\u003e\n\u003cli\u003eWhat features are most important to your model\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA predictive \u003cem\u003erecommendation\u003c/em\u003e might include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe contexts/situations where the predictions made by your model would and would not be useful for your stakeholder and business problem\u003c/li\u003e\n\u003cli\u003eSuggestions for how the business might modify certain input variables to achieve certain target results\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eIterative Approach to Modeling\u003c/h4\u003e\n\u003cp\u003eThe expectations from the Phase 2 project still stand:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eYou should demonstrate an iterative approach to modeling. This means that you must build multiple models. Begin with a basic model, evaluate it, and then provide justification for and proceed to a new model. After you finish refining your models, you should provide 1-3 paragraphs in the notebook discussing your final model.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWith the additional techniques you have learned in Phase 3, be sure to explore:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eModel features and preprocessing approaches\u003c/li\u003e\n\u003cli\u003eDifferent kinds of models (logistic regression, k-nearest neighbors, decision trees, etc.)\u003c/li\u003e\n\u003cli\u003eDifferent model hyperparameters\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAt minimum you must build three models:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA simple, interpretable baseline model (logistic regression or single decision tree)\u003c/li\u003e\n\u003cli\u003eA more-complex model (e.g. random forest)\u003c/li\u003e\n\u003cli\u003eA version of either the simple model or more-complex model with tuned hyperparameters\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eClassification Metrics\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eYou must choose appropriate classification metrics and use them to evaluate your models.\u003c/strong\u003e Choosing the right classification metrics is a key data science skill, and should be informed by data exploration and the business problem itself. You must then use this metric to evaluate your model performance using both training and testing data.\u003c/p\u003e\n\u003ch2\u003eDeliverables\u003c/h2\u003e\n\u003cp\u003eThere are three deliverables for this project:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003enon-technical presentation\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eJupyter Notebook\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003eGitHub repository\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe deliverables requirements are almost the same as in the Phase 1 and Phase 2 projects. \u003cstrong\u003e\u003cem\u003eThe only difference between the Phase 2 and Phase 3 project checklist is that the \"Regression Results\" element has been replaced with an \"Evaluation\" element.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003eNon-Technical Presentation\u003c/h3\u003e\n\u003cp\u003eRecall that the non-technical presentation is a slide deck presenting your analysis to \u003cstrong\u003e\u003cem\u003ebusiness stakeholders\u003c/em\u003e\u003c/strong\u003e, and should be presented live as well as submitted in PDF form on Canvas.\u003c/p\u003e\n\u003cp\u003eWe recommend that you follow this structure, although the slide titles should be specific to your project:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBeginning\n\u003cul\u003e\n\u003cli\u003eOverview\u003c/li\u003e\n\u003cli\u003eBusiness and Data Understanding\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMiddle\n\u003cul\u003e\n\u003cli\u003eModeling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEnd\n\u003cul\u003e\n\u003cli\u003eRecommendations\u003c/li\u003e\n\u003cli\u003eNext Steps\u003c/li\u003e\n\u003cli\u003eThank you\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMake sure that your discussion of classification modeling is geared towards a non-technical audience! Assume that their prior knowledge of machine learning is minimal. You don't need to explain the details of your model implementations, but you should explain why classification is useful for the problem context. Make sure you translate any metrics or feature importances into their plain language implications.\u003c/p\u003e\n\u003cp\u003eThe graded elements for the non-technical presentation are the same as in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-1-project-v2-3#deliverables\"\u003ePhase 1\u003c/a\u003e and Phase 2.\u003c/p\u003e\n\u003ch3\u003eJupyter Notebook\u003c/h3\u003e\n\u003cp\u003eRecall that the Jupyter Notebook is a notebook that uses Python and Markdown to present your analysis to a \u003cstrong\u003e\u003cem\u003edata science audience\u003c/em\u003e\u003c/strong\u003e. You will submit the notebook in PDF format on Canvas as well as in \u003ccode\u003e.ipynb\u003c/code\u003e format in your GitHub repository.\u003c/p\u003e\n\u003cp\u003eThe graded elements for the Jupyter Notebook are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBusiness Understanding\u003c/li\u003e\n\u003cli\u003eData Understanding\u003c/li\u003e\n\u003cli\u003eData Preparation\u003c/li\u003e\n\u003cli\u003eModeling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eCode Quality\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eGitHub Repository\u003c/h3\u003e\n\u003cp\u003eRecall that the GitHub repository is the cloud-hosted directory containing all of your project files as well as their version history.\u003c/p\u003e\n\u003cp\u003eThe requirements are the same as in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-1-project-v2-3#github-repository\"\u003ePhase 1\u003c/a\u003e and Phase 2, except for the required sections in the \u003ccode\u003eREADME.md\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor this project, the \u003ccode\u003eREADME.md\u003c/code\u003e file should contain:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOverview\u003c/li\u003e\n\u003cli\u003eBusiness and Data Understanding\n\u003cul\u003e\n\u003cli\u003eExplain your stakeholder audience and dataset choice here\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eModeling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEvaluation\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eConclusion\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eJust like in Phase 1 and 2, the \u003ccode\u003eREADME.md\u003c/code\u003e file should be the bridge between your non technical presentation and the Jupyter Notebook. It should not contain the code used to develop your analysis, but should provide a more in-depth explanation of your methodology and analysis than what is described in your presentation slides.\u003c/p\u003e\n\u003ch2\u003eGrading\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eTo pass this project, you must pass each project rubric objective.\u003c/em\u003e\u003c/strong\u003e The project rubric objectives for Phase 3 are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAttention to Detail\u003c/li\u003e\n\u003cli\u003eML Communication\u003c/li\u003e\n\u003cli\u003eData Preparation for Machine Learning\u003c/li\u003e\n\u003cli\u003eNonparametric and Ensemble Modeling\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eAttention to Detail\u003c/h3\u003e\n\u003cp\u003eJust like in Phase 1 and 2, this rubric objective is based on your completion of checklist items. \u003cstrong\u003e\u003cem\u003eIn Phase 3, you need to complete 80% (8 out of 10) or more of the checklist elements in order to pass the Attention to Detail objective.\u003c/em\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE THAT THE PASSING BAR IS HIGHER IN PHASE 3 THAN IT WAS IN PHASE 2!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe standard will increase with each Phase, until you will be required to complete all elements to pass Phase 5 (Capstone).\u003c/p\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003e90% or more of the project checklist items are complete\u003c/p\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003e80% of the project checklist items are complete\u003c/p\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003e70% of the project checklist items are complete\u003c/p\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003e60% or fewer of the project checklist items are complete\u003c/p\u003e\n\u003ch3\u003eML Communication\u003c/h3\u003e\n\u003cp\u003eRecall that communication is one of the key data science \"soft skills\". In Phase 3, we are specifically focusing on ML Communication. We define ML Communication as:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCommunicate the \u003cstrong\u003eperformance\u003c/strong\u003e of and \u003cstrong\u003einsights\u003c/strong\u003e generated by machine learning models to diverse audiences via writing, live presentation, and visualization\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHigh-quality ML Communication includes rationale, results, limitations, and recommendations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRationale:\u003c/strong\u003e Explaining why you are using machine learning rather than a simpler form of data analysis\n\u003cul\u003e\n\u003cli\u003eWhat about the problem or data is suitable for this form of analysis?\u003c/li\u003e\n\u003cli\u003eFor a data science audience, this includes your reasoning for the changes you applied while iterating between models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResults:\u003c/strong\u003e Describing the classification metrics\n\u003cul\u003e\n\u003cli\u003eYou can report multiple metrics for a single model, but make sure that indicate a reason for which metrics you are using (and don't try to use all of them at once)\u003c/li\u003e\n\u003cli\u003eFor a business audience, make sure you connect any metrics to real-world implications. You do not need to get into the details of how the model works.\u003c/li\u003e\n\u003cli\u003eFor a data science audience, you don't need to explain what a metric is, but make sure you explain why you chose that particular one.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLimitations:\u003c/strong\u003e Identifying the limitations and/or uncertainty present in your analysis\n\u003cul\u003e\n\u003cli\u003eAre there certain kinds of records where model performance is worse? If you used this model in production, what kinds of problems might that cause?\u003c/li\u003e\n\u003cli\u003eIn general, this should be more in-depth for a data science audience and more surface-level for a business audience.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRecommendations:\u003c/strong\u003e Interpreting the model results and limitations in the context of the business problem\n\u003cul\u003e\n\u003cli\u003eWhat should stakeholders \u003cem\u003edo\u003c/em\u003e with this information?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003eCommunicates the rationale, results, limitations, and specific recommendations generated by a classification model\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee above for an extended explanation of these terms.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003eSuccessfully communicates model metrics without any major errors\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe minimum requirement is to communicate the \u003cem\u003eresults\u003c/em\u003e, meaning at least one overall model metric for your final model. See the Approaching Objective section for an explanation of what a \"major error\" means.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003eCommunicates model metrics with at least one major error\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA major error means that some aspect of your explanation is fundamentally incorrect. For example, if you report a regression metric for a classification model, that would be a major error. Another example would be if you report the model's performance on the training data, rather than the model's performance on the test data.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003eDoes not communicate model metrics\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIt is not sufficient just to display the \u003ccode\u003eclassification_report\u003c/code\u003e or confusion matrix for a given model. You need to focus on one or more specific metrics that are important for your business case.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eData Preparation for Machine Learning\u003c/h3\u003e\n\u003cp\u003eWe define this objective as:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eApplying appropriate preprocessing and feature engineering steps to tabular data in preparation for predictive modeling\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis builds on the data preparation requirement from the Phase 2 project; you still need to ensure that you have a strategy for dealing with missing and non-numeric data.\u003c/p\u003e\n\u003cp\u003eFor the Phase 3 project, make sure you also consider:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePreventing Data Leakage:\u003c/strong\u003e As you prepare data for modeling, make sure that you are correctly applying data preparation techniques so that your model's performance on test data realistically represents how it would perform on unseen data. For scikit-learn transformers specifically, \u003cstrong\u003e\u003cem\u003emake sure that you do not fit the transformer on the test data\u003c/em\u003e\u003c/strong\u003e. Instead, fit the transformer on the training data and use it to transform both the train and test data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScaling:\u003c/strong\u003e If you are using a distance-based model algorithm (e.g. kNN or logistic regression with regularization), make sure you scale your data prior to fitting the model.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFeature engineering is encouraged but not required for this project.\u003c/p\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003eGoes above and beyond with data preparation, such as feature engineering or using pipelines\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eRelevant examples of feature engineering will depend on your choice of dataset and business problem.\u003c/p\u003e\n\u003cp\u003ePipelines are the best-practice approach to data preparation that avoids leakage, but they can get complicated very quickly. We therefore do not recommend that you use pipelines in your initial modeling approach, but rather that you refactor to use pipelines if you have time.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003eSuccessfully prepares data for modeling, using a final holdout dataset that is transformed by (but not fitted on) transformers used to prepare training data AND scaling data when appropriate\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee the descriptions above for explanations of how to use transformers and scaling.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003ePrepares some data successfully, but has at least one major error\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA major error means that some aspect of your data preparation is fundamentally incorrect. Some examples of major errors include: (1) fitting transformers on test data, (2) not performing a train-test split, (3) not scaling data that is used in a distance-based model.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003eDoes not prepare data for modeling\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis includes projects where data is partially prepared, but the model is unable to run.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003eNonparametric and Ensemble Modeling\u003c/h3\u003e\n\u003cp\u003eThis builds on the linear modeling requirement from the Phase 2 project. Your project should consider the different types of models that have been covered in the course so far and whether they are appropriate or inappropriate for the dataset and business case you are working with.\u003c/p\u003e\n\u003cp\u003eYour final model can still be a linear model (e.g. logistic regression) but you should explore at least one nonparametric model (e.g. decision tree) as well and articulate why one or the other is a better approach.\u003c/p\u003e\n\u003ch4\u003eExceeds Objective\u003c/h4\u003e\n\u003cp\u003eGoes above and beyond in the modeling process, such as articulating why a given model type is best suited to the problem or correctly using scikit-learn models not covered in the curriculum\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAnother way you might go above and beyond would be to create custom Python classes, possibly inheriting from scikit-learn classes.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eMeets Objective (Passing Bar)\u003c/h4\u003e\n\u003cp\u003eUses at least two types of scikit-learn model and tunes at least one hyperparameter in a justifiable way without any major errors\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSee the \"Iterative Approach to Modeling\" section above for a more-lengthy explanation.\u003c/p\u003e\n\u003cp\u003eOnce again, ideally you would include written justifications for each model iteration, but at minimum the iterations must be \u003cem\u003ejustifiable\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eFor an explanation of \"major errors\", see the description under \"Approaching Objective\".\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eApproaching Objective\u003c/h4\u003e\n\u003cp\u003eBuilds multiple classification models with at least one major error\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eA major error means that some aspect of your modeling approach is fundamentally incorrect.\u003c/p\u003e\n\u003cp\u003eOnce again, the number one major error to avoid is including the target as one of your features. If you are getting metrics that are \"too good to be true\", make sure that you removed the target (\u003ccode\u003ey\u003c/code\u003e) from your data before fitting the model.\u003c/p\u003e\n\u003cp\u003eOther examples of major errors include: using a numeric target value (since this is a classification project), not starting with a baseline model (e.g. proceeding directly to a Random Forest model), or not tuning hyperparameters in a justifiable way (e.g. reducing regularization on a model that is overfitting)\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4\u003eDoes Not Meet Objective\u003c/h4\u003e\n\u003cp\u003eDoes not build multiple classification models\u003c/p\u003e\n\u003ch2\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003ePlease start by reviewing the contents of this project description. If you have any questions, please ask your instructor ASAP.\u003c/p\u003e\n\u003cp\u003eOnce you are ready to begin the project, you will need to complete the \u003cem\u003e\u003cstrong\u003e\u003ca title=\"Phase 3 Project Proposal\" href=\"quizzes/gda45aa7fe7bca1a5bf280d1c988cc36c\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5539/quizzes/34866\" data-api-returntype=\"Quiz\"\u003eProject Proposal\u003c/a\u003e\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eRecall that more information is available in \u003ca href=\"https://github.com/learn-co-curriculum/dsc-phase-3-choosing-a-dataset\"\u003ePhase 3 Project - Choosing a Dataset\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo get started with project development, create a new repository on GitHub. For this project, we recommend that you do not fork the template repository, but rather that you make a new repository from scratch, starting by going to \u003ca href=\"https://github.com/new\"\u003egithub.com/new\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis project is an opportunity to expand your data science toolkit by evaluating, choosing, and working with new datasets. Spending time up front making sure you have a good dataset for a solvable problem will help avoid the major problems that can sometimes derail data science projects. You've got this!\u003c/p\u003e","frontPage":false},{"exportId":"topic-23-lesson-priorities-live","title":"Topic 23 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.6634%; height: 127px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eInference vs. Prediction\u003c/em\u003e\u0026nbsp;Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Machine Learning Fundamentals - Introduction\" href=\"pages/machine-learning-fundamentals-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/machine-learning-fundamentals-introduction\" data-api-returntype=\"Page\"\u003eMachine Learning Fundamentals - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 1st\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Data Science Processes\" href=\"pages/data-science-processes\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/data-science-processes\" data-api-returntype=\"Page\"\u003eData Science Processes\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Statistical Learning Theory\" href=\"pages/statistical-learning-theory\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/statistical-learning-theory\" data-api-returntype=\"Page\"\u003e\u003cstrong\u003eStatistical Learning Theory\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Regression Model Validation\" href=\"assignments/g7481fbdcbe748b61b8266678416afd43\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197106\" data-api-returntype=\"Assignment\"\u003eRegression Model Validation\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e\u0026nbsp;1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Regression Model Validation - Lab\" href=\"assignments/g8878f371f80e2be164db9ca56270ac6f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197107\" data-api-returntype=\"Assignment\"\u003eRegression Model Validation - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Cross-Validation\" href=\"assignments/gf75f69ba85f77765f91d254d7d93222e\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197099\" data-api-returntype=\"Assignment\"\u003eIntroduction to Cross-Validation\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Introduction to Cross-Validation - Lab\" href=\"assignments/g5c9102ed5e9c336f43723273d0ebd59b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197100\" data-api-returntype=\"Assignment\"\u003eIntroduction to Cross-Validation - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Short Video: Cross-Validation\" href=\"pages/short-video-cross-validation\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/short-video-cross-validation\" data-api-returntype=\"Page\"\u003eShort Video: Cross-Validation\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Bias-Variance Tradeoff\" href=\"assignments/g24fef19b98005baa41a82dd41eea0b54\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197089\" data-api-returntype=\"Assignment\"\u003eBias-Variance Tradeoff\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e\u0026nbsp;1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Bias-Variance Tradeoff - Lab\" href=\"assignments/g3b15a2f839ce8b5455258fa436cec3b0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197090\" data-api-returntype=\"Assignment\"\u003eBias-Variance Tradeoff - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Short Video: Bias-Variance Tradeoff\" href=\"pages/short-video-bias-variance-tradeoff\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/short-video-bias-variance-tradeoff\" data-api-returntype=\"Page\"\u003eShort Video: Bias-Variance Tradeoff\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u0026nbsp;2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Quiz: Machine Learning Fundamentals\" href=\"quizzes/gde2a7ea9ada6166586c9cf787822bfba\"\u003e\u003cstrong\u003eQuiz: Machine Learning Fundamentals\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e\u0026nbsp;1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.6649%; height: 53px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eInference vs. Prediction\u003c/em\u003e Lecture, Before \u003cem\u003eModel Validation and Data Leakage \u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\" Inference vs. Prediction Exit Ticket\" href=\"quizzes/g63baf40c53d2206ad7b65a1501c08cbf\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33587\" data-api-returntype=\"Quiz\"\u003e Inference vs. Prediction Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.3789%; height: 42px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eModel Validation and Data Leakage\u003c/em\u003e\u0026nbsp;Lecture, Before \u003cem\u003eRegularization \u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Model Validation and Data Leakage Exit Ticket\" href=\"quizzes/g343df895fe21388a003ee4f3f256d3e0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33591\" data-api-returntype=\"Quiz\"\u003eModel Validation and Data Leakage Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Ridge and Lasso Regression\" href=\"assignments/g80f8d50fa5bf6e890812cf951e88626b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197108\" data-api-returntype=\"Assignment\"\u003eRidge and Lasso Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 1st\u0026quot;}\"\u003e\u003cstrong\u003e 1st \u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Ridge and Lasso Regression - Lab\" href=\"assignments/g7ff4049e3a4478aad77ba3a1b80487cc\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197109\" data-api-returntype=\"Assignment\"\u003eRidge and Lasso Regression - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Feature Selection Methods\" href=\"assignments/gf5dc93ee053ea7f248ee15a7b680c8fe\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197094\" data-api-returntype=\"Assignment\"\u003eFeature Selection Methods\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Extensions to Linear Models - Lab\" href=\"assignments/gf751cd1332473f2e094bab2de0075b78\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197093\" data-api-returntype=\"Assignment\"\u003eExtensions to Linear Models - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot; 2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003ca title=\"Quiz: Regularization\" href=\"quizzes/gb7d6a7cdf1300cc8cb899aa98c8bc385\"\u003e\u003cstrong\u003eQuiz: Regularization\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.6634%; height: 53px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eRegularization \u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003cth style=\"width: 28.5105%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 5.71692%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"Regularization Exit Ticket\" href=\"quizzes/g15774678425387b9fbeab8a27d51575f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33585\" data-api-returntype=\"Quiz\"\u003eRegularization Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 28.5105%;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\" href=\"quizzes/g9d5b89d09608ee0da642cc68cf19da0e\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33594\" data-api-returntype=\"Quiz\"\u003e‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; text-align: center;\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 28.5105%; height: 29px;\"\u003e\u003ca title=\"Machine Learning Fundamentals - Recap\" href=\"pages/machine-learning-fundamentals-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/machine-learning-fundamentals-recap\" data-api-returntype=\"Page\"\u003eMachine Learning Fundamentals - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 5.71692%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","frontPage":false},{"exportId":"entropy-and-information-gain","title":"Entropy and Information Gain","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-entropy-and-information-gain\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-entropy-and-information-gain/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eInformation gain\u003c/em\u003e\u003c/strong\u003e is calculated using a statistical measure called \u003cstrong\u003e\u003cem\u003eEntropy\u003c/em\u003e\u003c/strong\u003e. Entropy is a widely used concept in the fields of Physics, Mathematics, Computer Science (information theory), and more. You may have come across the idea of entropy in thermodynamics, societal dynamics, and a number of other domains. In electronics and computer science, the idea of entropy is usually derived from \u003cstrong\u003eShannon's\u003c/strong\u003e description of entropy to measure the information gain against some cost incurred in the process. In this lesson, we shall look at how this works with the simple example we introduced in the previous lesson. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the process for selecting the best attribute for a split \u003c/li\u003e\n\u003cli\u003eCalculate entropy and information gain by hand for a simple dataset \u003c/li\u003e\n\u003cli\u003eCompare and contrast entropy and information gain \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eShannon's Entropy\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eEntropy is a measure of disorder or uncertainty.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe measure is named after \u003cem\u003eClaude Shannon\u003c/em\u003e, who is known as the \"father of information theory\". Information theory provides measures of uncertainty associated with random variables. These measures help calculate the average information content one is missing when one does not know the value of the random variable. This uncertainty is measured in bits, i.e., the amount of information (in bits) contained per average instance in a stream of instances.\u003c/p\u003e\n\n\u003cp\u003eConceptually, information can be thought of as being stored or transmitted as variables that can take on different values. A variable can be thought of as a unit of storage that can take on, at different times, one of several different specified values, following some process for taking on those values. Informally, we get information from a variable by looking at its value, just as we get information from an email by reading its contents. In the case of the variable, the information is about the process behind the variable.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eThe entropy of a variable is the \"amount of information\" contained in the variable. \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis amount is not only determined by the number of different values the variable can take, just as the information in an email is not quantified just by the number of words in the email or the different possible words in the language of the email. Informally, the amount of information in an email is proportional to the amount of ‚Äúsurprise‚Äù its reading causes. \u003c/p\u003e\n\n\u003cp\u003eFor example, if an email is simply a repeat of an earlier email, then it is not informative at all. On the other hand, if, for example, the email reveals the outcome of an election, then it is highly informative. Similarly, the information in a variable is tied to the amount of surprise the value of that variable causes when revealed.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eShannon‚Äôs entropy quantifies the amount of information in a variable, thus providing the foundation for a theory around the notion of information.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn terms of data, we can informally describe entropy as an indicator of how messy your data is.  A high degree of entropy always reflects \"messed-up\" data with low/no information content. The uncertainty about the content of the data, before viewing the data remains the same (or almost the same) as that before the data was available. \u003c/p\u003e\n\n\u003cp\u003eIn a nutshell, higher entropy means less predictive power when it comes to doing data science with that data. \u003c/p\u003e\n\n\u003ch2\u003eEntropy and decision trees\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eDecision trees aim to tidy the data by separating the samples and re-grouping them in the classes they belong to.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBecause decision trees use a supervised learning approach, we know the target variable of our data. So, we maximize the \u003cstrong\u003epurity\u003c/strong\u003e of the classes \u003cstrong\u003eas much as possible\u003c/strong\u003e while making splits, aiming to have \u003cstrong\u003eclarity\u003c/strong\u003e in the leaf nodes. Remember, it may not be possible to remove the uncertainty totally, i.e., to fully clean up the data. Have a look at the image below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/split_fs.png\" alt=\"initial dataset with a decision split that reduces entropy\" width=\"300\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWe can see that the split has not \u003cstrong\u003eFULLY\u003c/strong\u003e classified the data above, but the resulting data is \u003cstrong\u003etidier\u003c/strong\u003e than it was before the split. By using a series of such splits that focus on different features, we try to clean up the data as much as possible in the leaf nodes. At each step, we want to decrease the entropy, so \u003cstrong\u003eentropy is computed before and after the split\u003c/strong\u003e. If it decreases, the split is retained and we can proceed to the next step, otherwise, we must try to split with another feature or stop this branch (or quit, in which case we claim that the current tree is the best solution).\u003c/p\u003e\n\n\u003ch3\u003eCalculating entropy\u003c/h3\u003e\n\n\u003cp\u003eLet's pretend we have a sample, \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e. This sample contains \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e total items falling into two different categories, \u003ccode\u003eTrue\u003c/code\u003e and \u003ccode\u003eFalse\u003c/code\u003e. Of the \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e total items we have, \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e observations have a target value equal to \u003cimg class=\"equation_image\" title=\"True\" src=\"https://learning.flatironschool.com/equation_images/True\" alt=\"{\" data-equation-content=\"True\"\u003e, and \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e observations have a target value equal to \u003cimg class=\"equation_image\" title=\"False\" src=\"https://learning.flatironschool.com/equation_images/False\" alt=\"{\" data-equation-content=\"False\"\u003e. Note that if we know \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e and \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e, we can easily calculate \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e to be \u003cimg class=\"equation_image\" title=\"m = N - n\" src=\"https://learning.flatironschool.com/equation_images/m%20=%20N%20-%20n\" alt=\"{\" data-equation-content=\"m = N - n\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eLet's assume our boss brings us the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e, and asks us to group each observation in \u003cimg class=\"equation_image\" title=\"N\" src=\"https://learning.flatironschool.com/equation_images/N\" alt=\"{\" data-equation-content=\"N\"\u003e according to whether their target value is True or False. They also want to know the ratio of Trues to Falses in our dataset. We can calculate this as follows: \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"p = n/N - (class 1)\" src=\"https://learning.flatironschool.com/equation_images/p%20=%20n/N%20-%20(class%201)\" alt=\"{\" data-equation-content=\"p = n/N - (class 1)\"\u003e\u003c/p\u003e \u003cp\u003e\u003cimg class=\"equation_image\" title=\"q = m/N = 1-p - (class 2)\" src=\"https://learning.flatironschool.com/equation_images/q%20=%20m/N%20=%201-p%20-%20(class%202)\" alt=\"{\" data-equation-content=\"q = m/N = 1-p - (class 2)\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eIf we know these ratios, we can calculate the \u003cem\u003eentropy\u003c/em\u003e of the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e. This will provide us with an easy way to see how organized or disorganized our dataset is. For instance, let's assume that our boss believes that the dataset should mostly be full of \"True\"'s, with some occasional \"False\"'s slipping through. The more Falses in with the Trues (or Trues in with the Falses!), the more disorganized our dataset is. We can calculate entropy using the following equation:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"E = -p . log_2(p) - q . log_2(q)\" src=\"https://learning.flatironschool.com/equation_images/E%20=%20-p%20.%20log_2(p)%20-%20q%20.%20log_2(q)\" alt=\"{\" data-equation-content=\"E = -p . log_2(p) - q . log_2(q)\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eDon't worry too much about this equation yet -- we'll dig deeper into what it means in a minute. \u003c/p\u003e\n\n\u003cp\u003eThe equation above tells us that a dataset is considered tidy if it only contains one class (i.e. no uncertainty or confusion). If the dataset contains a mix of classes for our target variable, the entropy increases. This is easier to understand when we visualize it. Consider the following graph of entropy in a dataset that has two classes for our target variable:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-entropy-and-information-gain/master/images/new_entropy_fs.png\" alt=\"parabola that shows the proportion of class p on the x-axis and entropy on the y-axis. the parabola peaks at 0.5\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see, when the classes are split equally, \u003cimg class=\"equation_image\" title=\"p = 0.5\" src=\"https://learning.flatironschool.com/equation_images/p%20=%200.5\" alt=\"{\" data-equation-content=\"p = 0.5\"\u003e and \u003cimg class=\"equation_image\" title=\"q = 1 - p = 0.5\" src=\"https://learning.flatironschool.com/equation_images/q%20=%201%20-%20p%20=%200.5\" alt=\"{\" data-equation-content=\"q = 1 - p = 0.5\"\u003e, the entropy value is at its maximum, 1. Conversely, when the proportion of the split is at 0 (all of one target class) or at 1 (all of the other target class), the entropy value is 0! This means that we can easily think of entropy as follows: the more one-sided the proportion of target classes, the less entropy. Think of a sock drawer that may or may not have some underwear mixed in. If the sock drawer contains only socks (or only underwear), then entropy is 0. If you reach in and pull out an article of clothing, you know exactly what you're going to get. However, if 10% of the items in that sock drawer are actually underwear, you are less certain what that random draw will give you. That uncertainty increases as more and more underwear gets mixed into that sock drawer, right up until there is the exact same amount of socks and underwear in the drawer. When the proportion is exactly equal, you have no way of knowing item of clothing a random draw might give you -- maximum entropy, and perfect chaos!\u003c/p\u003e\n\n\u003cp\u003eThis is where the logic behind decision trees comes in -- what if we could split the contents of our sock drawer into different subsets, which might divide the drawer into more organized subsets? For instance, let's assume that we've built a laundry robot that can separate items of clothing by color. If a majority of our socks are white, and a majority of our underwear is some other color, then we can safely assume that the two subsets will have a better separation between socks and underwear, even if the original chaotic drawer had a 50/50 mix of the two!\u003c/p\u003e\n\n\u003ch3\u003eGeneralization of entropy\u003c/h3\u003e\n\n\u003cp\u003eNow that we have a good real-world example to cling to, let's get back to thinking about the mathematical definition of entropy. \u003c/p\u003e\n\n\u003cp\u003eEntropy \u003cimg class=\"equation_image\" title=\"H(S)\" src=\"https://learning.flatironschool.com/equation_images/H(S)\" alt=\"{\" data-equation-content=\"H(S)\"\u003e is a measure of the amount of uncertainty in the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e. We can see this is a measurement or characterization of the amount of information contained within the dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eWe saw how to calculate entropy for a two-class variable before. However, in the real world we deal with multiclass problems very often, so it would be a good idea to see a general representation of the formula we saw before. The general representation is: \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large H(S) = -\\sum (P_i . log_2(P_i))\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20H(S)%20=%20-%255Csum%20(P_i%20.%20log_2(P_i))\" alt=\"{\" data-equation-content=\"\\large H(S) = -\\sum (P_i . log_2(P_i))\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen  \u003cimg class=\"equation_image\" title=\"H(S) = 0\" src=\"https://learning.flatironschool.com/equation_images/H(S)%20=%200\" alt=\"{\" data-equation-content=\"H(S) = 0\"\u003e, this means that the set \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e is perfectly classified, meaning that there is no disorganization in our data because all of our data in S is the exact same class. If we know how much entropy exists in a subset (and remember, we can subset our data by just splitting it into 2 or more groups according to whatever metric we choose), then we can easily calculate how much \u003cstrong\u003e\u003cem\u003einformation gain\u003c/em\u003e\u003c/strong\u003e each potential split would give us!\u003c/p\u003e\n\n\u003ch2\u003eInformation gain\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eInformation gain is an impurity/uncertainty based criterion that uses the entropy as the measure of impurity.\u003c/strong\u003e \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThere are several different algorithms out there for creating decision trees. Of those, the ID3 algorithm is one of the most popular. Information gain is the key criterion that is used by the ID3 classification tree algorithm to construct a decision tree. The decision tree algorithm will always try to \u003cstrong\u003emaximize information gain\u003c/strong\u003e. The entropy of the dataset is calculated using each attribute, and the attribute showing highest information gain is used to create the split at each node. A simple understanding of information gain can be written as:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"Information~Gain  = Entropy_{parent} - Entropy_{child}.[child ~weighted ~average]\" src=\"/equation_images/Information~Gain%20%20=%20Entropy_{parent}%20-%20Entropy_{child}.[child%20~weighted%20~average]\" alt=\"{\" data-equation-content=\"Information~Gain  = Entropy_{parent} - Entropy_{child}.[child ~weighted ~average]\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eA weighted average based on the number of samples in each class is multiplied by the child's entropy, since most datasets have class imbalance. Thus the information gain calculation for each attribute is calculated and compared, and the attribute showing the highest information gain will be selected for the split. Below is a more generalized form of the equation: \u003c/p\u003e\n\n\u003cp\u003eWhen we measure information gain, we're really measuring the difference in entropy from before the split (an untidy sock drawer) to after the split (a group of white socks and underwear, and a group of non-white socks and underwear). Information gain allows us to put a number to exactly how much we've reduced our \u003cem\u003euncertainty\u003c/em\u003e after splitting a dataset \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e on some attribute, \u003cimg class=\"equation_image\" title=\"A\" src=\"https://learning.flatironschool.com/equation_images/A\" alt=\"{\" data-equation-content=\"A\"\u003e.  The equation for information gain is:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large IG(A, S) = H(S) - \\sum{}{p(t)H(t)}  \" src=\"/equation_images/%255Clarge%20IG(A,%20S)%20=%20H(S)%20-%20%255Csum{}{p(t)H(t)}\" alt=\"{\" data-equation-content=\"\\large IG(A, S) = H(S) - \\sum{}{p(t)H(t)}  \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eWhere:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"H(S)\" src=\"https://learning.flatironschool.com/equation_images/H(S)\" alt=\"{\" data-equation-content=\"H(S)\"\u003e is the entropy of set \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e is a subset of the attributes contained in \u003cimg class=\"equation_image\" title=\"A\" src=\"https://learning.flatironschool.com/equation_images/A\" alt=\"{\" data-equation-content=\"A\"\u003e (we represent all subsets \u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e as \u003cimg class=\"equation_image\" title=\"T\" src=\"https://learning.flatironschool.com/equation_images/T\" alt=\"{\" data-equation-content=\"T\"\u003e)\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"p(t)\" src=\"https://learning.flatironschool.com/equation_images/p(t)\" alt=\"{\" data-equation-content=\"p(t)\"\u003e is the proportion of the number of elements in \u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e to the number of elements in \u003cimg class=\"equation_image\" title=\"S\" src=\"https://learning.flatironschool.com/equation_images/S\" alt=\"{\" data-equation-content=\"S\"\u003e\u003c/li\u003e\n\u003cli\u003e\u003cimg class=\"equation_image\" title=\"H(t)\" src=\"https://learning.flatironschool.com/equation_images/H(t)\" alt=\"{\" data-equation-content=\"H(t)\"\u003e is the entropy of a given subset \u003cimg class=\"equation_image\" title=\"t\" src=\"https://learning.flatironschool.com/equation_images/t\" alt=\"{\" data-equation-content=\"t\"\u003e \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the ID3 algorithm, we use entropy to calculate information gain, and then pick the attribute with the largest possible information gain to split our data on at each iteration. \u003c/p\u003e\n\n\u003ch2\u003eEntropy and information gain example\u003c/h2\u003e\n\n\u003cp\u003eSo far, we've focused heavily on the math behind entropy and information gain. This usually makes the calculations look scarier than they actually are. To show that calculating entropy/information gain is actually pretty simple, let's take a look at an example problem -- predicting if we want to play tennis or not, based on the weather, temperature, humidity, and windiness of a given day!\u003c/p\u003e\n\n\u003cp\u003eOur dataset is as follows:\u003c/p\u003e\n\n\u003ctable\u003e\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: center\"\u003eoutlook\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003etemp\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003ehumidity\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003ewindy\u003c/th\u003e\n\u003cth style=\"text-align: center\"\u003eplay\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003eovercast\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003erain\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ecool\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehot\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003enormal\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eY\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eno\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: center\"\u003esunny\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003emild\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003ehigh\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eN\u003c/td\u003e\n\u003ctd style=\"text-align: center\"\u003eyes\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\n\u003cp\u003eLet's apply the formulas we saw earlier to this problem:  \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\Large  H(S) = \\sum{}{-p(c) log_2 p(c)}\" src=\"/equation_images/%255CLarge%20%20H(S)%20=%20%255Csum{}{-p(c)%20log_2%20p(c)}\" alt=\"{\" data-equation-content=\"\\Large  H(S) = \\sum{}{-p(c) log_2 p(c)}\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large C={{yes, no}}\" src=\"/equation_images/%255Clarge%20C={{yes,%20no}}\" alt=\"{\" data-equation-content=\"\\large C={{yes, no}}\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eOut of 14 instances, 9 are classified as yes, and 5 as no. So:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large  p(yes) = -(9/14)log_2(9/14) = 0.28\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20%20p(yes)%20=%20-(9/14)log_2(9/14)%20=%200.28\" alt=\"{\" data-equation-content=\"\\large  p(yes) = -(9/14)log_2(9/14) = 0.28\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large  p(no) = -(5/14)log_2(5/14) = 0.37\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20%20p(no)%20=%20-(5/14)log_2(5/14)%20=%200.37\" alt=\"{\" data-equation-content=\"\\large  p(no) = -(5/14)log_2(5/14) = 0.37\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\large  H(S) = p(yes) + p(no) = 0.65\" src=\"https://learning.flatironschool.com/equation_images/%255Clarge%20%20H(S)%20=%20p(yes)%20+%20p(no)%20=%200.65\" alt=\"{\" data-equation-content=\"\\large  H(S) = p(yes) + p(no) = 0.65\"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eThe current entropy of our dataset is 0.65. In the next lesson, we'll see how we can improve this by subsetting our dataset into different groups by calculating the entropy/information gain of each possible split, and then picking the one that performs best until we have a fully fleshed-out decision tree!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at calculating entropy and information gain measures for building decision trees. We looked at a simple example and saw how to use these measures to select the best split at each node. Next, we calculate these measures in Python, before digging deeper into decision trees. \u003c/p\u003e","frontPage":false},{"exportId":"lecture-calculus-and-cost-functions","title":"üé¨  Lecture: Calculus and Cost Functions","type":"WikiPage","content":"\u003cdiv style=\"padding:56.25% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/681831452?h=1127a46572\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"Calculus and Cost Functions\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\u003cp\u003eIn this lesson, Victor Geislinger reviews some calculus concepts and cost functions along with their use in data science. Topics covered include: derivatives, partial derivatives, integration, differentiation, the difference between integration and differentiation.\u003c/p\u003e\n\u003cp\u003eThe repository for this lecture can be found here: \u003ca class=\"inline_disabled\" style=\"color: #3598db;\" href=\"https://github.com/flatiron-school/ds-calculus-kvo32\"\u003eCalculus and Cost Function Lecture Repository\u003c/a\u003e\u003c/p\u003e","frontPage":false},{"exportId":"support-vector-machines-introduction","title":"Support Vector Machines - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eA Support Vector Machine (SVM) is a type of classifier which modifies the loss function for optimization to not only take into account overall accuracy metrics of the resulting predictions, but also to maximize the decision boundary between the data points. In essence, this further helps tune the classifier as a good balance between underfitting and overfitting.\u003c/p\u003e\n\n\u003ch2\u003eSupport Vector Machines\u003c/h2\u003e\n\n\u003cp\u003eIn addition to optimizing for accuracy, support vector machines add a slack component, trading in accuracy to increase the distance between data points and the decision boundary. This provides an interesting perspective that can help formalize the intuitive visual choices a human would make in balancing precision and generalization to strike a balance between overfitting and underfitting.\u003c/p\u003e\n\n\u003ch2\u003eKernel Functions\u003c/h2\u003e\n\n\u003cp\u003eInitially, you'll explore linear support vector machines that divide data points into their respective groups by drawing hyperplanes using the dimensions from the feature space. In practice, these have limitations and the dataset may not be cleanly separable. As a result, kernel functions are an additional tool that can be used. Essentially, kernels reproject data onto a new parameter space using combinations of existing features. From there, the same process of applying SVMs to this transformed space can then be employed.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eSupport Vector Machines are a powerful algorithm and may have the top performance among the out of the box classifiers from scikit-learn. Moreover, learning to properly tune SVMs is critical. In the upcoming labs and lessons, you'll investigate and apply these concepts.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-svm-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-svm-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-svm-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"gradient-boosting-and-weak-learners","title":"Gradient Boosting and Weak Learners","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-boosting-and-weak-learners\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we'll explore one of the most powerful ensemble methods around -- gradient boosting!\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompare and contrast weak and strong learners and explain the role of weak learners in boosting algorithms\u003c/li\u003e\n\u003cli\u003eDescribe the process of boosting in Adaboost and Gradient Boosting\u003c/li\u003e\n\u003cli\u003eExplain the concept of a learning rate and the role it plays in gradient boosting algorithms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eWeak learners and boosting\u003c/h2\u003e\n\u003cp\u003eThe first ensemble technique we learned about was \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e, which refers to training different models independently on different subsets of data by sampling with replacement. The goal of bagging is to create variability in the ensemble of models. The next ensemble technique we'll learn about is \u003cstrong\u003e\u003cem\u003eBoosting\u003c/em\u003e\u003c/strong\u003e. This technique is at the heart of some very powerful, top-of-class ensemble methods currently used in machine learning, such as \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIn order to understand boosting, let's first examine the cornerstone of boosting algorithms -- \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003eWeak learners\u003c/h3\u003e\n\u003cp\u003eAll the models we've learned so far are \u003cstrong\u003e\u003cem\u003eStrong Learners\u003c/em\u003e\u003c/strong\u003e -- models with the goal of doing as well as possible on the classification or regression task they are given. The term \u003cstrong\u003e\u003cem\u003eWeak Learner\u003c/em\u003e\u003c/strong\u003e refers to simple models that do only slightly better than random chance. Boosting algorithms start with a single weak learner (tree methods are overwhelmingly used here), but technically, any model will do. Boosting works as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTrain a single weak learner\u003c/li\u003e\n\u003cli\u003eFigure out which examples the weak learner got wrong\u003c/li\u003e\n\u003cli\u003eBuild another weak learner that focuses on the areas the first weak learner got wrong\u003c/li\u003e\n\u003cli\u003eContinue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, or the model's performance has plateaued\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn this way, each new weak learner is specifically tuned to focus on the weak points of the previous weak learner(s). The more often an example is missed, the more likely it is that the next weak learner will be the one that can classify that example correctly. In this way, all the weak learners work together to make up a single strong learner.\u003c/p\u003e\n\u003ch2\u003eBoosting and random forests\u003c/h2\u003e\n\u003ch3\u003eSimilarities\u003c/h3\u003e\n\u003cp\u003eBoosting algorithms share some similarities with random forests, as well as some notable differences. Like random forests, boosting algorithms are an ensemble of many different models with high inter-group diversity. Boosting algorithms also aggregate the predictions of each constituent model into an overall prediction. Both algorithms also make use of tree models (although this isn't strictly required, in the case of boosting).\u003c/p\u003e\n\u003ch3\u003eDifferences\u003c/h3\u003e\n\u003ch4\u003e1: Independent vs. iterative\u003c/h4\u003e\n\u003cp\u003eThe difference is in the approach to training the trees. Whereas a random forest trains each tree independently and at the same time, boosting trains each tree iteratively. In a random forest model, how well or poorly a given tree does has no effect on any of the other trees since they are all trained at the same time. Boosting, on the other hand, trains trees one at a time, identifies the weak points for those trees, and then purposefully creates the next round of trees in such a way as to specialize in those weak points.\u003c/p\u003e\n\u003ch4\u003e2: Weak vs. strong\u003c/h4\u003e\n\u003cp\u003eAnother major difference between random forests and boosting algorithms is the overall size of the trees. In a random forest, each tree is a strong learner -- they would do just fine as a decision tree on their own. In boosting algorithms, trees are artificially limited to a very shallow depth (usually only 1 split), to ensure that each model is only slightly better than random chance. For this reason, boosting algorithms are also highly resilient against noisy data and overfitting. Since the individual weak learners are too simple to overfit, it is very hard to combine them in such a way as to overfit the training data as a whole -- especially when they focus on different things, due to the iterative nature of the algorithm.\u003c/p\u003e\n\u003ch4\u003e3: Aggregate predictions\u003c/h4\u003e\n\u003cp\u003eThe final major difference we'll talk about between the two is the way predictions are aggregated. Whereas in a random forest, each tree simply votes for the final result, boosting algorithms usually employ a system of weights to determine how important the input for each tree is. Since we know how well each weak learner performs on the dataset by calculating its performance at each step, we can see which weak learners do better on hard tasks. Think of it like this -- harder problems deserve more weight. If there are many learners in the overall ensemble that can get the same questions right, then that tree isn't super important -- other trees already provide the same value that it does. This tree will have its overall weight reduced. As more and more trees get a hard problem wrong, the \"reward\" for a tree getting that hard problem correct goes higher and higher. This \"reward\" is actually just a higher weight when calculating the overall vote. Intuitively, this makes sense -- trees that can do what few other trees can do are the ones that we should probably listen to more than others, as they are the most likely to get hard examples correct. Since other trees tend to get this wrong, we can expect to see a general split of about 50/50 among the trees that do not \"specialize\" in the hard problems. Since our \"specialized\" tree has more weight, its correct vote will carry more weight than the combined votes of the half of the \"unspecialized\" trees that get it wrong. It is worth noting that the \"specialized\" trees will often do quite poorly on the examples that are easy to predict. However, since these examples are easier, we can expect a strong majority of the trees in our ensemble to get it right, meaning that the combined, collective weight of their agreement will be enough to overrule the trees with higher weights that get it wrong.\u003c/p\u003e\n\u003ch2\u003eUnderstanding Adaboost and Gradient boosting\u003c/h2\u003e\n\u003cp\u003eThere are two main algorithms that come to mind when Data Scientists talk about boosting: \u003cstrong\u003e\u003cem\u003eAdaboost\u003c/em\u003e\u003c/strong\u003e (short for Adaptive Boosting), and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e. Both are generally very effective, but they use different methods to achieve their results.\u003c/p\u003e\n\u003ch3\u003eAdaboost\u003c/h3\u003e\n\u003cp\u003eAdaboost was the first boosting algorithm invented. Although there have been marked improvements made to this algorithm, Adaboost still tends to be quite an effective algorithm! More importantly, it's a good starting place for understanding how boosting algorithms actually work.\u003c/p\u003e\n\u003cp\u003eIn Adaboost, each learner is trained on a subsample of the dataset, much like we saw with \u003cstrong\u003e\u003cem\u003eBagging\u003c/em\u003e\u003c/strong\u003e. Initially, the bag is randomly sampled with replacement. However, each data point in the dataset has a weight assigned. As learners correctly classify an example, that example's weight is reduced. Conversely, when learners get an example wrong, the weight for that sample increases. In each iteration, these weights act as the probability that an item will be sampled into the \"bag\" which will be used to train the next weak learner. As the number of learners grows, you can imagine that the examples that are easy to get correct will become less and less prevalent in the samples used to train each new learner. This is a good thing -- if our ensemble already contains multiple learners that can correctly classify that example, then we don't need more that can do this. Instead, the \"bags\" of data will contain multiple instances of the hard examples, thereby increasing the likelihood that the learner will create a split that focuses on getting the hard example correct.\u003c/p\u003e\n\u003cp\u003eThe following diagram demonstrates how the weights change for each example as classifiers get them right and wrong.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_adaboost.png\" width=\"600\"\u003e\u003c/p\u003e\n\u003cp\u003ePay attention to the colors of the pluses and minuses -- pluses are meant to be in the blue section, and minuses are meant to be in the red. The decision boundary of the tree can be interpreted as the line drawn between the red and blue sections. As you can see above, examples that were misclassified are larger in the next iteration, while examples that were classified correctly are smaller. As we combine the decision boundaries of each new classifier, we end up with a classifier that correctly classifies all of the examples!\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eKey Takeaway:\u003c/em\u003e\u003c/strong\u003e Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive learner.\u003c/p\u003e\n\u003ch3\u003eGradient boosting\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e are a more advanced boosting algorithm that makes use of \u003cstrong\u003e\u003cem\u003eGradient Descent.\u003c/em\u003e\u003c/strong\u003e Much like Adaboost, gradient boosting starts with a weak learner that makes predictions on the dataset. The algorithm then checks this learner's performance, identifying examples that it got right and wrong. However, this is where the gradient boosting algorithm diverges from Adaboost's methodology. The model then calculates the \u003cstrong\u003e\u003cem\u003eResiduals\u003c/em\u003e\u003c/strong\u003e for each data point, to determine how far off the mark each prediction was. The model then combines these residuals with a \u003cstrong\u003e\u003cem\u003eLoss Function\u003c/em\u003e\u003c/strong\u003e to calculate the overall loss. There are many loss functions that are used -- the thing that matters most is that the loss function is \u003cstrong\u003e\u003cem\u003edifferentiable\u003c/em\u003e\u003c/strong\u003e so that we can use calculus to compute the gradient for the loss, given the inputs of the model. We then use the gradients and the loss as predictors to train the next tree against! In this way, we can use \u003cstrong\u003e\u003cem\u003eGradient Descent\u003c/em\u003e\u003c/strong\u003e to minimize the overall loss.\u003c/p\u003e\n\u003cp\u003eSince the loss is most heavily inflated by examples where the model was wrong, gradient descent will push the algorithm towards creating a new learner that will focus on these harder examples. If the next tree gets these right, then the loss goes down! In this way, gradient descent allows us to continually train and improve on the loss for each model to improve the overall performance of the ensemble as a whole by focusing on the \"hard\" examples that cause the loss to be high.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-boosting-and-weak-learners/master/images/new_gradient-boosting.png\"\u003e\u003c/p\u003e\n\u003ch3\u003eLearning rates\u003c/h3\u003e\n\u003cp\u003eOften, we want to artificially limit the \"step size\" we take in gradient descent. Small, controlled changes in the parameters we're optimizing with gradient descent will mean that the overall process is slower, but the parameters are more likely to converge to their optimal values. The learning rate for your model is a small scalar meant to artificially reduce the step size in gradient descent. Learning rate is a tunable parameter for your model that you can set -- large learning rates get closer to the optimal values more quickly, but have trouble landing exactly at the optimal values because the step size is too big for the small distances it needs to travel when it gets close. Conversely, small learning rates means the model will take a longer time to get to the optimal parameters, but when it does get there, it will be extremely close to the optimal values, thereby providing the best overall performance for the model.\u003c/p\u003e\n\u003cp\u003eYou'll often see learning rates denoted by the symbol, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cgamma\"\u003e -- this is the greek letter, \u003cstrong\u003e\u003cem\u003egamma\u003c/em\u003e\u003c/strong\u003e. Don't worry if you're still hazy on the concept of gradient descent -- we'll explore it in much more detail when we start studying deep learning!\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003esklearn\u003c/code\u003e library contains some excellent implementations of Adaboost, as well as several different types of gradient boosting classifiers. These classifiers can be found in the \u003ccode\u003eensemble\u003c/code\u003e module, which you will make use of in the upcoming lesson.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this lesson, we learned about \u003cstrong\u003e\u003cem\u003eWeak Learners\u003c/em\u003e\u003c/strong\u003e, and how they are used in various \u003cstrong\u003e\u003cem\u003eGradient Boosting\u003c/em\u003e\u003c/strong\u003e algorithms. We also learned about two specific algorithms -- \u003cstrong\u003e\u003cem\u003eAdaBoost\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eGradient Boosted Trees\u003c/em\u003e\u003c/strong\u003e, and we compared how they are similar and how they are different!\u003c/p\u003e","frontPage":false},{"exportId":"topic-24-lesson-priorities-live","title":"Topic 24 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.8127%; height: 150px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eLogistic Regression 1\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.9107%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 9.57491%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression - Introduction\" href=\"pages/logistic-regression-introduction\"\u003eLogistic Regression - Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.9107%;\"\u003e\u003ca title=\"Introduction to Supervised Learning\" href=\"pages/introduction-to-supervised-learning\"\u003eIntroduction to Supervised Learning\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Linear to Logistic Regression\" href=\"assignments/ge143c152064afbb74f901a0771d431e0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12071\" data-api-returntype=\"Assignment\"\u003eLinear to Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Fitting a Logistic Regression Model - Lab\" href=\"assignments/g70829cf72bde217bb84b7b85a704288d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12072\" data-api-returntype=\"Assignment\"\u003eFitting a Logistic Regression Model - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression in scikit-learn\" href=\"assignments/gb8692713ef61b8e0a83cffd5b60950ae\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12073\" data-api-returntype=\"Assignment\"\u003eLogistic Regression in scikit-learn\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Logistic Regression in scikit-learn - Lab\" href=\"assignments/gb8cdf52053921f13266a12eab476e9eb\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12074\" data-api-returntype=\"Assignment\"\u003eLogistic Regression in scikit-learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 38.9107%;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Introduction to Logistic Regression\" href=\"quizzes/g8f0af8fb6b9743cf18b550abf04e6f7f\"\u003eQuiz: Introduction to Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 100%; height: 198px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eLogistic Regression 1\u003c/em\u003e Lecture, Before\u0026nbsp;\u003cem\u003eLogistic Regression 2\u0026nbsp;\u003c/em\u003eLecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.9107%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 9.57491%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 29px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression 1 Exit Ticket\" href=\"quizzes/g211cf52a1b09af29717a39b460ae425d\"\u003eLogistic Regression 1 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; text-align: center; height: 29px;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/mle-review\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/mle-review\" data-api-returntype=\"Page\"\u003eMLE Review\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/mle-and-logistic-regression\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/mle-and-logistic-regression\" data-api-returntype=\"Page\"\u003eMLE and Logistic Regression\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca class=\"instructure_file_link inline_disabled\" href=\"pages/gradient-descent-review\" target=\"_blank\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/gradient-descent-review\" data-api-returntype=\"Page\"\u003eGradient Descent Review\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;Low priority\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Gradient Descent - Lab\" href=\"assignments/gd4dcde7ebdef12668ae7dbb3f568182b\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12083\" data-api-returntype=\"Assignment\"\u003eGradient Descent - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;Low priority\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Coding Logistic Regression from Scratch - Lab\" href=\"assignments/g4819ae6efc6a7095e874f1dbb8a1ecb2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12084\" data-api-returntype=\"Assignment\"\u003eCoding Logistic Regression from Scratch - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.9064%; height: 76px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eLogistic Regression 2\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 38.9107%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 9.57491%; height: 29px; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003cstrong\u003e\u003ca title=\"Logistic Regression 2 Exit Ticket\" href=\"quizzes/g0ec8da043796b87f5899023655c90b12\"\u003eLogistic Regression 2 Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 38.9107%; height: 28px;\"\u003e\u003ca title=\"Logistic Regression - Recap\" href=\"pages/logistic-regression-recap\"\u003eLogistic Regression - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 9.57491%; height: 28px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","frontPage":false},{"exportId":"bayesian-classification-introduction","title":"Bayesian Classification - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bayesian-classification-intro-v2-1\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bayesian-classification-intro-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn an earlier section, you learned about Bayesian statistics with plenty of theory and application of Bayes theorem. You'll now take a look at using Bayes theorem to perform some classification tasks. Here, you'll see that the Bayes theorem can be applied to multiple variables simultaneously. \u003c/p\u003e\n\n\u003ch2\u003eBayes Classification\u003c/h2\u003e\n\n\u003cp\u003eNaive Bayes algorithms extend Bayes' formula to multiple variables by assuming that these features are independent of one another, which may not be met, (hence its naivety) it can nonetheless provide strong results in scenarios with clean and well normalized datasets. This then allows you to estimate an overall probability by multiplying the conditional probabilities for each of the independent features.\u003c/p\u003e\n\n\u003cp\u003eBayes' formula extended to multiple features is:  \u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\" src=\"/equation_images/%20%255CLarge%20P(y|x_1,%20x_2,%20...,%20x_n)%20=%20%255Cfrac{P(y)%255Cprod_{i}^{n}P(x_i|y)}{P(x_1,%20x_2,%20...,%20x_n)}\" alt=\"{\" data-equation-content=\" \\Large P(y|x_1, x_2, ..., x_n) = \\frac{P(y)\\prod_{i}^{n}P(x_i|y)}{P(x_1, x_2, ..., x_n)}\"\u003e\u003c/p\u003e \u003cp\u003e\u003c/p\u003e\n\n\u003ch2\u003eDocument Classification\u003c/h2\u003e\n\n\u003cp\u003eAn interesting application of Bayes' theorem is to use \u003cem\u003ebag of words\u003c/em\u003e for document classification. A bag of words representation takes a text document and converts it into a word frequency representation. In this section, you'll use bag of words and Naive Bayes to classify YouTube videos into appropriate topics. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eOver the next few lessons you will learn about another fundamental classification algorithm which has many practical applications. It's time to jump into the wonderful Bayesian world again! This section will help you solidify your understanding of Bayesian stats. \u003c/p\u003e","frontPage":false},{"exportId":"short-video-debugging-with-oop","title":"Short Video: Debugging with OOP","type":"WikiPage","content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802485?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"topic-27-lesson-priorities-live","title":"Topic 27 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.7191%; height: 182px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eK-Nearest Neighbors\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"K-Nearest Neighbors - Introduction\" href=\"pages/k-nearest-neighbors-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/k-nearest-neighbors-introduction\" data-api-returntype=\"Page\"\u003eK-Nearest Neighbors - Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Distance Metrics\" href=\"assignments/g61391248a475ba95d6c3011ff7a40b23\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12086\" data-api-returntype=\"Assignment\"\u003eDistance Metrics\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"Distance Metrics - Lab\" href=\"assignments/ga1388e96c2ed6de2dfc2332638720aeb\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12087\" data-api-returntype=\"Assignment\"\u003eDistance Metrics - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"K-Nearest Neighbors\" href=\"pages/k-nearest-neighbors\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/k-nearest-neighbors\" data-api-returntype=\"Page\"\u003eK-Nearest Neighbors\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"K-Nearest Neighbors - Lab\" href=\"assignments/g96f15a5a4928ab60f394fc9eedfc023d\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12088\" data-api-returntype=\"Assignment\"\u003eK-Nearest Neighbors - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Finding the Best Value for K\" href=\"pages/finding-the-best-value-for-k\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/finding-the-best-value-for-k\" data-api-returntype=\"Page\"\u003eFinding the Best Value for K\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"KNN with scikit-learn\" href=\"pages/knn-with-scikit-learn\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/knn-with-scikit-learn\" data-api-returntype=\"Page\"\u003eKNN with scikit-learn\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"KNN with scikit-learn - Lab\" href=\"assignments/g60e8d59944503f2fff50b17ed9050ce2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/assignments/12089\" data-api-returntype=\"Assignment\"\u003eKNN with scikit-learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: K Nearest Neighbors\" href=\"quizzes/g041301c0d6f44f9e19f87488351cf805\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/quizzes/23170\" data-api-returntype=\"Quiz\"\u003eQuiz: K Nearest Neighbors\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 99.6254%; height: 78px;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eK-Nearest Neighbors\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 41.4167%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 8.44866%; text-align: center; height: 29px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003cstrong\u003e\u003ca title=\"KNN Exit Ticket\" href=\"quizzes/gbf96ad98a717fabb3b4f3675241611ed\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/quizzes/11197\" data-api-returntype=\"Quiz\"\u003eKNN Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 41.4167%;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Nonparametric ML Models - Cumulative Lab\" href=\"quizzes/g1056c04a78eb2b8ee77b244c23859397\"\u003e‚≠êÔ∏è Nonparametric ML Models - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; text-align: center;\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 41.4167%; height: 29px;\"\u003e\u003ca title=\"K-Nearest Neighbors - Recap\" href=\"pages/k-nearest-neighbors-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/346/pages/k-nearest-neighbors-recap\" data-api-returntype=\"Page\"\u003eK-Nearest Neighbors - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 8.44866%; height: 29px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","frontPage":false},{"exportId":"linear-algebra-and-calculus-recap","title":"Linear Algebra and Calculus - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-algebra-and-calculus-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-algebra-and-calculus-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eCongratulations! You have learned the fundamentals of the math at the core of machine learning: linear algebra and calculus.\u003c/p\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003ch3\u003eLinear Algebra\u003c/h3\u003e\n\n\u003cp\u003eThe goal of this part was to provide both a conceptual and computational introduction to linear algebra. Some of the key takeaways include: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eScalars, vectors, matrices, and tensors\n\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003escalar\u003c/em\u003e\u003c/strong\u003e is a single, real number\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003evector\u003c/em\u003e\u003c/strong\u003e is a one-dimensional array of numbers\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003ematrix\u003c/em\u003e\u003c/strong\u003e is a 2-dimensional array of numbers\u003c/li\u003e\n\u003cli\u003eTwo matrices can be added together if they have the same shape\u003c/li\u003e\n\u003cli\u003eScalars can be added to matrices by adding the scalar (number) to each element\u003c/li\u003e\n\u003cli\u003eTo calculate the dot product for matrix multiplication, the first matrix must have the same number of columns as the number of rows in the second matrix\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003etensor\u003c/em\u003e\u003c/strong\u003e is a generalized term for an n-dimensional rectangular grid of numbers. A vector is a one-dimensional (first-order tensor), a matrix is a two-dimensional (second-order tensor), etc.\u003c/li\u003e\n\u003cli\u003eOne use case for vectors and matrices is for representing and solving systems of linear equations \u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003eLinear algebra in Python\n\n\u003cul\u003e\n\u003cli\u003eOperating on \u003cstrong\u003e\u003cem\u003eNumPy\u003c/em\u003e\u003c/strong\u003e data types is substantially more computationally efficient than performing the same operations on native Python data types\u003c/li\u003e\n\u003cli\u003eIt is possible to use linear algebra in NumPy to solve for a linear regression using the \u003cstrong\u003e\u003cem\u003eOLS\u003c/em\u003e\u003c/strong\u003e method\u003c/li\u003e\n\u003cli\u003eOLS is not computationally efficient, so in practice, we usually perform a gradient descent instead to solve a linear regression\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3\u003eCalculus and Gradient Descent\u003c/h3\u003e\n\n\u003cp\u003eThe goal of this part was to learn some of the foundational calculus that underpins the gradient descent algorithm. Some of the key takeaways include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculus\n\n\u003cul\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003ederivative\u003c/em\u003e\u003c/strong\u003e is the \"instantaneous rate of change\" of a function - or it can be thought of as the \"slope of the curve\" at a point in time\u003c/li\u003e\n\u003cli\u003eA derivative can also be thought of as a special case of the rate of change over a period of time - as that period of time is zero. \u003c/li\u003e\n\u003cli\u003eIf you calculate the rate of change over a period of time and keep reducing the period of time, it usually tends to a limit - which is the value of that derivative\u003c/li\u003e\n\u003cli\u003eRules can be used for finding derivatives\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003e\u003cem\u003epower rule\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003econstant factor rule\u003c/em\u003e\u003c/strong\u003e, and \u003cstrong\u003e\u003cem\u003eaddition rule\u003c/em\u003e\u003c/strong\u003e are key tools for calculating derivatives for various kinds of functions\u003c/li\u003e\n\u003cli\u003eThe \u003cstrong\u003e\u003cem\u003echain rule\u003c/em\u003e\u003c/strong\u003e can be a useful tool for calculating the derivate of composite functions\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003eGradient descent\n\n\u003cul\u003e\n\u003cli\u003eA derivative can be useful for identifying local \u003cstrong\u003e\u003cem\u003emaxima\u003c/em\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cem\u003eminima\u003c/em\u003e\u003c/strong\u003e as in both cases, the derivative tends to zero\u003c/li\u003e\n\u003cli\u003eA \u003cstrong\u003e\u003cem\u003ecost curve\u003c/em\u003e\u003c/strong\u003e can be used to plot the values of a cost function (in the case of linear regression) for various values of offset and slope for the best fit line\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003cem\u003eGradient descent\u003c/em\u003e\u003c/strong\u003e can be used to move towards the local minimum on the cost curve and thus the ideal values for the y-intercept and slope to minimize the selected cost function when performing a linear regression\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"mle-review","title":"MLE Review","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-mle-review\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-mle-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've seen MLE (Maximum Likelihood Estimation) when discussing Bayesian statistics, but did you know logistic regression can also be seen from this statistical perspective? In this section, you'll gain a deeper understanding of logistic regression by coding it from scratch and analyzing the statistical motivations backing it. But first take some time to review maximum likelihood estimation.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how to take MLE of a binomial variable \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eMLE\u003c/h2\u003e\n\n\u003cp\u003eMaximum likelihood estimation can often sound academic, confusing, and cryptic when first introduced. It is often presented and introduced with complex integrals of statistical distributions that scare away many readers. Hopefully, this hasn't been your experience. While the mathematics can quickly become complex, the underlying concepts are actually quite intuitive.\u003c/p\u003e\n\n\u003cp\u003eTo demonstrate this, imagine a simple coin flipping example. Let's say that you flip a coin 100 times and get 55 heads. Maximum likelihood estimation attempts to uncover the underlying theoretical probability of this coin landing on heads given your observations. In other words, given the observations, what is the chance that the coin was fair and had a 0.5 chance of landing on heads each time? Or what is the chance that the coin actually had a 0.75 probability of lands of heads, given what we observed? It turns out that the answer to these questions is rather intuitive. If you observe 55 out of 100 coin flips, the underlying probability which maximizes the chance of us observing 55 out of 100 coin flips is 0.55. In this simple example, MLE simply returns the current sample mean as the underlying parameter that makes the observations most probable. Slight deviations to this would be almost as probable but slightly less so, and large deviations from our sample mean should be rare. This intuitively makes some sense; as your sample size increases, you expect the sample mean to converge to the true underlying parameter. MLE takes a flipped perspective, asking what underlying parameter is most probable given the observations.\u003c/p\u003e\n\n\u003ch2\u003eLog-likelihood\u003c/h2\u003e\n\n\u003cp\u003eWhen calculating maximum likelihood, it is common to use the log-likelihood, as taking the logarithm can simplify calculations. For example, taking the logarithm of a set of products allows you to decompose the problem from products into sums. (You may recall from high school mathematics that \u003cimg class=\"equation_image\" title=\"x^{(a+b)} = x^a \\bullet x^b\" src=\"/equation_images/x^{(a+b)}%20=%20x^a%20%255Cbullet%20x^b\" alt=\"{\" data-equation-content=\"x^{(a+b)} = x^a \\bullet x^b\"\u003e. Similarly, taking the logarithm of both sides of a function allows you to transform products into sums. \u003c/p\u003e\n\n\u003ch2\u003eMLE for a binomial variable\u003c/h2\u003e\n\n\u003cp\u003eLet's take a deeper mathematical investigation into the coin flipping example above. \u003c/p\u003e\n\n\u003cp\u003eIn general, if you were to observe \u003cimg class=\"equation_image\" title=\"n\" src=\"https://learning.flatironschool.com/equation_images/n\" alt=\"{\" data-equation-content=\"n\"\u003e flips, you would have observations \u003cimg class=\"equation_image\" title=\"y_1, y_2, ..., y_n\" src=\"https://learning.flatironschool.com/equation_images/y_1,%20y_2,%20...,%20y_n\" alt=\"{\" data-equation-content=\"y_1, y_2, ..., y_n\"\u003e.\u003c/p\u003e\n\n\u003cp\u003eIn maximum likelihood estimation, you are looking to maximize the likelihood:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"L(p) = L(y_1, y_2, ..., y_n | p) = p^y (1-p)^{n-y}\" src=\"/equation_images/L(p)%20=%20L(y_1,%20y_2,%20...,%20y_n%20|%20p)%20=%20p^y%20(1-p)^{n-y}\" alt=\"{\" data-equation-content=\"L(p) = L(y_1, y_2, ..., y_n | p) = p^y (1-p)^{n-y}\"\u003e  where \u003cimg class=\"equation_image\" title=\" y = \\sum_{i=1}^{n}y_i\" src=\"/equation_images/%20y%20=%20%255Csum_{i=1}^{n}y_i\" alt=\"{\" data-equation-content=\" y = \\sum_{i=1}^{n}y_i\"\u003e\u003c/p\u003e\n\n\u003cp\u003eTaking the log of both sides:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"ln[L(p)] = ln[p^y (1-p)^{n-y}] = y ln(p)+(n-y)ln(1-p)\" src=\"/equation_images/ln[L(p)]%20=%20ln[p^y%20(1-p)^{n-y}]%20=%20y%20ln(p)+(n-y)ln(1-p)\" alt=\"{\" data-equation-content=\"ln[L(p)] = ln[p^y (1-p)^{n-y}] = y ln(p)+(n-y)ln(1-p)\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIf \u003cimg class=\"equation_image\" title=\"y = 1, 2, ..., n-1\" src=\"https://learning.flatironschool.com/equation_images/y%20=%201,%202,%20...,%20n-1\" alt=\"{\" data-equation-content=\"y = 1, 2, ..., n-1\"\u003e the derivative of \u003cimg class=\"equation_image\" title=\"ln[L(p)]\" src=\"/equation_images/ln[L(p)]\" alt=\"{\" data-equation-content=\"ln[L(p)]\"\u003e with respect to \u003cimg class=\"equation_image\" title=\"p\" src=\"https://learning.flatironschool.com/equation_images/p\" alt=\"{\" data-equation-content=\"p\"\u003e is:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\frac{d\\,ln[L(p)]}{dp} = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\" src=\"/equation_images/%255Cfrac{d%255C,ln[L(p)]}{dp}%20=%20y%20(%255Cfrac{1}{p})+(n-y)(%255Cfrac{-1}{1-p})\" alt=\"{\" data-equation-content=\"\\frac{d\\,ln[L(p)]}{dp} = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\"\u003e  \u003c/p\u003e\n\n\u003cp\u003eAs you've seen previously, the maximum will then occur when the derivative equals zero:  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"0 = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\" src=\"/equation_images/0%20=%20y%20(%255Cfrac{1}{p})+(n-y)(%255Cfrac{-1}{1-p})\" alt=\"{\" data-equation-content=\"0 = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})\"\u003e\u003c/p\u003e\n\n\u003cp\u003eDistributing, you have\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"0 = \\frac{y}{p} - \\frac{n-y}{1-p}\" src=\"/equation_images/0%20=%20%255Cfrac{y}{p}%20-%20%255Cfrac{n-y}{1-p}\" alt=\"{\" data-equation-content=\"0 = \\frac{y}{p} - \\frac{n-y}{1-p}\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAnd solving for p: \u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\frac{n-y}{1-p} = \\frac{y}{p} \" src=\"/equation_images/%20%255Cfrac{n-y}{1-p}%20=%20%255Cfrac{y}{p}\" alt=\"{\" data-equation-content=\" \\frac{n-y}{1-p} = \\frac{y}{p} \"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"p(n-y) = \\frac{y(1-p)}{p}\" src=\"/equation_images/p(n-y)%20=%20%255Cfrac{y(1-p)}{p}\" alt=\"{\" data-equation-content=\"p(n-y) = \\frac{y(1-p)}{p}\"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"\\frac{n-y}{y} = \\frac{1-p}{p}\" src=\"/equation_images/%255Cfrac{n-y}{y}%20=%20%255Cfrac{1-p}{p}\" alt=\"{\" data-equation-content=\"\\frac{n-y}{y} = \\frac{1-p}{p}\"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"\\frac{n}{y}-1 = \\frac{1}{p}-1\" src=\"/equation_images/%255Cfrac{n}{y}-1%20=%20%255Cfrac{1}{p}-1\" alt=\"{\" data-equation-content=\"\\frac{n}{y}-1 = \\frac{1}{p}-1\"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"\\frac{n}{y} = \\frac{1}{p} \" src=\"/equation_images/%255Cfrac{n}{y}%20=%20%255Cfrac{1}{p}\" alt=\"{\" data-equation-content=\"\\frac{n}{y} = \\frac{1}{p} \"\u003e\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"p = \\frac{y}{n}\" src=\"/equation_images/p%20=%20%255Cfrac{y}{n}\" alt=\"{\" data-equation-content=\"p = \\frac{y}{n}\"\u003e  \u003c/p\u003e\n\n\u003cp\u003eAnd voil√†, you've verified the intuitive solution discussed above; the maximum likelihood for a binomial sample is the observed frequency!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly reviewed maximum likelihood estimation. In the upcoming lesson, you'll see how logistic regression can also be interpreted from this framework, which will help set the stage for you to code a logistic regression function from scratch using NumPy. Continue on to the next lesson to take a look at how this works for logistic regression.\u003c/p\u003e","frontPage":false},{"exportId":"statistical-learning-theory","title":"Statistical Learning Theory","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-stat-learning-theory-v2-5\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eBefore you get into building machine learning models, a basic understanding of statistical learning theory is essential.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the difference between modeling for inference and prediction\u003c/li\u003e\n\u003cli\u003eExplain generalization in the context of statistical modeling\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eStatistical Learning Theory\u003c/h2\u003e\n\n\u003cp\u003eStatistical learning theory is based on the idea of using data along with statistics to provide a framework for learning.\u003c/p\u003e\n\n\u003cp\u003eIn statistical learning theory, the main idea is to construct a \u003cstrong\u003emodel\u003c/strong\u003e to draw certain conclusions from data, and next, to use this model to make \u003cstrong\u003epredictions\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eThis builds on statistical modeling, which represents the relationship between independent and dependent variables as a mathematical equation. For parametric statistical models such as linear regression, this means that the model learns \u003cstrong\u003eparameters\u003c/strong\u003e that will be used for future predictions.\u003c/p\u003e\n\n\u003ch2\u003eInference vs. Prediction\u003c/h2\u003e\n\n\u003cp\u003eThere are two different modeling approaches to statistical modeling: modeling for \u003cstrong\u003e\u003cem\u003einference\u003c/em\u003e\u003c/strong\u003e and modeling for \u003cstrong\u003e\u003cem\u003eprediction\u003c/em\u003e\u003c/strong\u003e. A \"perfect\" model might be useful for both, but often your modeling strategy will need to be calibrated based on the goal of the model.\u003c/p\u003e\n\n\u003ch3\u003eInference\u003c/h3\u003e\n\n\u003cp\u003eWhen you are modeling for inference, you are asking the question:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eWhat is the relationship between \u003ccode\u003eX\u003c/code\u003e and \u003ccode\u003ey\u003c/code\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eand sometimes, if you have good reason to infer a causal relationship:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHow does \u003ccode\u003eX\u003c/code\u003e affect \u003ccode\u003ey\u003c/code\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003ewhere \u003ccode\u003eX\u003c/code\u003e is your collection of independent variables (i.e. features) and \u003ccode\u003ey\u003c/code\u003e is your dependent variable (i.e. target). The focus is on \u003cem\u003eunderstanding\u003c/em\u003e. Most of the history of statistics and all of the linear regression content so far has been focused on this approach.\u003c/p\u003e\n\n\u003cp\u003eWhen modeling for inference, it is important for the model to be \u003cstrong\u003estatistically significant\u003c/strong\u003e and \u003cstrong\u003einterpretable\u003c/strong\u003e, sometimes at the expense of overall model fit. Every feature used in the model should be carefully chosen based on underlying domain understanding.\u003c/p\u003e\n\n\u003ch3\u003ePrediction\u003c/h3\u003e\n\n\u003cp\u003eWhen you are modeling for prediction, you are asking the question:\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eHow well can I use \u003ccode\u003eX\u003c/code\u003e to predict \u003ccode\u003ey\u003c/code\u003e?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ccode\u003eX\u003c/code\u003e is still your collection of independent variables, and \u003ccode\u003ey\u003c/code\u003e is still your dependent variable. But you are less concerned about how and which features impact \u003ccode\u003ey\u003c/code\u003e as opposed to how you can efficiently use them to predict \u003ccode\u003ey\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhen modeling for prediction, it is important for the model to \u003cstrong\u003e\u003cem\u003egeneralize\u003c/em\u003e\u003c/strong\u003e to unseen data. This means that the overall model fit is more important than the coefficients of features or statistical significance, and that you will often use all available features rather than carefully choosing them. Both in terms of the number of features and in terms of the type of model used, predictive models tend to be more \u003cstrong\u003e\u003cem\u003ecomplex\u003c/em\u003e\u003c/strong\u003e than inferential models.\u003c/p\u003e\n\n\u003ch2\u003eModel Generalization\u003c/h2\u003e\n\n\u003cp\u003eThe model learns about the data during the \u003cstrong\u003e\u003cem\u003etraining\u003c/em\u003e\u003c/strong\u003e stage. Examples are presented to the model and the model tweaks its parameters to better understand the data.\u003c/p\u003e\n\n\u003cp\u003eOnce the training is over, the model is unleashed upon new data and then uses what it has learned to make predictions with that data. This is where problems can emerge. If we over-train the model on the training data -- i.e. make the model memorize every detail of the data it is shown -- it will be able to identify all the relevant information in the training data, but will fail miserably when presented with the new data. \u003c/p\u003e\n\n\u003cp\u003eWe then say that the \u003cstrong\u003emodel is not capable of generalizing\u003c/strong\u003e, or that the \u003cstrong\u003emodel is over-fitting the training data\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eLet's take a look at an example of the phenomenon: modeling happiness as a function of wealth. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5/master/images/new_happy.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the top three diagrams, we have data and models (dashed curves). From left to right the models have been trained longer and longer on the training data. The training error curve in the bottom box shows that the training error gets better and better as we train longer (increasing model complexity).\u003c/p\u003e\n\n\u003cp\u003eYou may think that if we train longer we'll get better! Well, yes, but \u003cstrong\u003eonly better at describing the training data\u003c/strong\u003e. The top right box shows a very complex model that hits all the data points. This model does great on the training data, but when presented with new data (examine the prediction error curve in the bottom box) then it does worse! The gap between the training error and prediction error for new data (labeled \"optimism\") is growing as model complexity increases, which means that we are getting \u003cem\u003eworse\u003c/em\u003e at generalizing.\u003c/p\u003e\n\n\u003cp\u003eIn order to create good predictive models in machine learning that are capable of generalizing, one needs to know when to stop training the model so that it doesn't over-fit.\u003c/p\u003e\n\n\u003ch3\u003eModel Validation\u003c/h3\u003e\n\n\u003cp\u003eAs the data which is available to us for modeling is finite, the available data needs to be used very effectively to build and \u003cstrong\u003evalidate\u003c/strong\u003e a model. Model validation is a process of measuring overfitting and indicates the degree of generalizability.\u003c/p\u003e\n\n\u003cp\u003eHere is how we perform validation, in its simplest form:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSplit the data into two parts with a 70/30, 80/20, or a similar split\u003c/li\u003e\n\u003cli\u003eUse the larger part for \u003cstrong\u003etraining\u003c/strong\u003e so the model learns from it\u003c/li\u003e\n\u003cli\u003eUse the smaller part for \u003cstrong\u003e\u003cem\u003etesting\u003c/em\u003e\u003c/strong\u003e the model\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis setup looks like as shown below:\n\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-stat-learning-theory-v2-5/master/images/new_train_test_sets.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is called a \u003cstrong\u003e\u003cem\u003etrain-test split\u003c/em\u003e\u003c/strong\u003e and means that you can compare the model performance on training data vs. testing data using a given \u003cstrong\u003emetric\u003c/strong\u003e. The metric can be R-Squared or it can be an error-based metric like RMSE.\u003c/p\u003e\n\n\u003cp\u003eIf the metric is much better on the training data than the test data, this indicates overfitting. A model that generalizes well will have similar metrics on the two datasets.\u003c/p\u003e\n\n\u003cp\u003eAnother approach to validation is \u003cstrong\u003e\u003cem\u003ecross-validation\u003c/em\u003e\u003c/strong\u003e. This involves splitting the data multiple times and training multiple models, to get more of a distribution of possible metrics rather than relying on metrics from a single train-test split.\u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=rqJ8SrnmWu0\"\u003eYoutube: Introduction to Statistical Learning Theory\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.princeton.edu/%7Ekulkarni/Papers/Journals/j077_2011_KulHar_WileyTutorial.pdf\"\u003eAn Overview of Statistical Learning Theory with examples\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly looked at statistical learning theory and its main components. When modeling for inference rather than prediction, some additional conceptual considerations become important. In particular, predictive models should generalize to unseen data. Model validation is used to measure how well a model will generalize.\u003c/p\u003e","frontPage":false},{"exportId":"short-video-pipelines","title":"Short Video: Pipelines","type":"WikiPage","content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802989?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"introduction-to-support-vector-machines","title":"Introduction to Support Vector Machines","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-introduction-to-support-vector-machines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eBy now you've learned a few techniques for classification. You touched upon it when talking about Naive Bayes, and again when you saw some supervised learning techniques such as logistic regression and decision trees. Now it's time for another popular classification technique -- Support Vector Machines.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDescribe what is meant by margin classifiers\u003c/li\u003e\n\u003cli\u003eDescribe the mathematical components underlying soft and max-margin classifiers\u003c/li\u003e\n\u003cli\u003eCompare and contrast max-margin classifiers and soft-margin classifiers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eThe idea\u003c/h2\u003e\n\u003cp\u003eThe idea behind Support Vector Machines (also referred to as SVMs) is that you perform classification by finding the separation line or (in higher dimensions) \"hyperplane\" that maximizes the distance between two classes. Taking a look at the concept visually helps make sense of the process.\u003c/p\u003e\n\u003cp\u003eImagine you have a dataset containing two classes:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_1.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eIn SVM, you want to find a hyperplane or \"decision boundary\" that divides one class from the other. Which one works best?\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_3.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThis would be a good line:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eWhile this seems intuitive, there are other decision boundaries which also separate the classes. Which one is best? Rather than solely focus on the final accuracy of the model, Support Vector Machines aim to \u003cstrong\u003emaximize the margin\u003c/strong\u003e between the decision boundary and the various data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_4.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eThe margin is defined as the distance between the separating line (hyperplane) and the training set cases that are closest to this hyperplane. These cases define \"support vectors\". The support vectors in this particular case are highlighted in the image below. As you can see, the max-margin hyperplane is the midpoint between the two lines defined by the support vectors.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003ch2\u003eThe Max Margin classifier\u003c/h2\u003e\n\u003cp\u003eWhy would you bother maximizing the margins? Don't these other hyperplanes discriminate just as well? Remember that you are fitting the hyperplane on your training data. Imagine you start looking at your test data, which will slightly differ from your training data.\u003c/p\u003e\n\u003cp\u003eAssuming your test set is big enough and randomly drawn from your entire dataset, you might end up with a test case as shown in the image below. This test case diverts a little bit from the training set cases observed earlier. While the max-margin classifier would classify this test set case correctly, the hyperplane closer to the right would have been classified this case incorrectly. Of course, this is just one example and other test cases will end up in different spots. Nonetheless, the purpose of choosing the max-margin classifier is to minimize the generalization error when applying the model to future unseen data points.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_test2.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eBefore diving into the underlying mathematics, take a look at the image again:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_fin.png\" width=\"400\"\u003e\u003c/p\u003e\n\u003cp\u003eNow you can start exploring the mathematics behind the image. First, define some numeric labels for the two classes. Set the circles to be -1 and the diamonds to be 1. Normally, 0 and 1 are used for class labels but in this particular case using -1 and 1 simplifies the mathematics.\u003c/p\u003e\n\u003cp\u003eNow some terminology: The lines defined by the support vectors are the negative (to the left) and the positive (to the right) hyperplanes, respectively. These hyperplanes are defined by two terms: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e and \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e .\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e term is called the \u003cstrong\u003eweight vector\u003c/strong\u003e and contains the weights that are used in the classification.\u003c/p\u003e\n\u003cp\u003eThe \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b\"\u003e term is called the \u003cstrong\u003ebias\u003c/strong\u003e and functions as an offset term. If there were no bias term, the hyperplane would always go through the origin which would not be very generalizable!\u003c/p\u003e\n\u003cp\u003eThe equation describing the positive hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bpos%7D%20=1\"\u003e\u003c/p\u003e\n\u003cp\u003eand the equation describing the negative hyperplane is: \u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx_%7Bneg%7D%20=-1\"\u003e\u003c/p\u003e\n\u003cp\u003eRemember, your goal is to maximize the separation between the two hyperplanes. To do this, first subtract the negative hyperplane's equation from the positive hyperplane's equation:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T(x_%7Bpos%7D-x_%7Bneg%7D)%20=%202\"\u003e\u003c/p\u003e\n\u003cp\u003eNext, normalize \u003cimg src=\"https://render.githubusercontent.com/render/math?math=w_T\"\u003e by dividing both sides of the equation by its norm, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e :\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7C%20w%20%7C%7C=%20%5Csqrt%7B%5Csum%5Em_%7Bj-1%7Dw_j%5E2%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eDividing the former expression by \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%7C%7Cw%7C%7C\"\u003e yields the equation below. The left side of the resulting equation can be interpreted as the distance between the positive and negative hyperplanes. This is the \u003cstrong\u003emargin\u003c/strong\u003e you're trying to maximize.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7Bw_T(x_%7Bpos%7D-x_%7Bneg%7D)%7D%7B%5ClVert%20w%20%5CrVert%7D%20=%20%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective of the SVM is then maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e under the constraint that the samples are classified correctly. Mathematically,\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThese equations basically say that all negative samples should fall on the left side of the negative hyperplane, whereas all the positive samples should fall on the right of the positive hyperplane. This can also be written in one line as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20(b%20%2b%20w_Tx%5E%7B(i)%7D%20)%5Cgeq%201\"\u003e for each \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i\"\u003e\u003c/p\u003e\n\u003cp\u003eNote that maximizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D\"\u003e means we're minimizing \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5ClVert%20w%20%5CrVert\"\u003e , or, as is done in practice because it seems to be easier to be minimized, \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2\"\u003e .\u003c/p\u003e\n\u003ch2\u003eThe Soft Margin classifier\u003c/h2\u003e\n\u003cp\u003eIntroducing slack variables \u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cxi\"\u003e . The idea for introducing slack variables is that the linear constraints need to be relaxed for data that are not linearly separable, as not relaxing the constraints might lead to the algorithm that doesn't converge.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cgeq%201-%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%201\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=b%20%2b%20w_Tx%5E%7B(i)%7D%20%5Cleq%20-1%2b%5Cxi%5E%7B(i)%7D\"\u003e if \u003cimg src=\"https://render.githubusercontent.com/render/math?math=y%20%5E%7B(i)%7D%20=%20-1\"\u003e\u003c/p\u003e\n\u003cp\u003eFor \u003cimg src=\"https://render.githubusercontent.com/render/math?math=i=%201,%5Cldots%20,N\"\u003e\u003c/p\u003e\n\u003cp\u003eThe objective function (AKA the function you want to minimize) is\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://render.githubusercontent.com/render/math?math=%5Cdfrac%7B1%7D%7B2%7D%5ClVert%20w%20%5CrVert%5E2%2b%20C(%5Csum_i%20%5Cxi%5E%7B(i)%7D)\"\u003e\u003c/p\u003e\n\u003cp\u003eYou're basically adding these slack variables in your objective function, making clear that you want to minimize the amount of slack you allow for. You can tune this with \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e as shown in the above equation. \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will define how much slack we're allowing.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA big value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the left: misclassifications are heavily punished, so the optimization prioritizes classifying correctly over having a big margin.\u003c/li\u003e\n\u003cli\u003eA small value for \u003cimg src=\"https://render.githubusercontent.com/render/math?math=C\"\u003e will lead to the picture on the right: it is OK to have some misclassifications, in order to gain a bigger margin overall. (This can help avoid overfitting to the training data.)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-introduction-to-support-vector-machines/master/images/new_SVM_C.png\"\u003e\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eGreat! You now understand both max-margin classifiers as well as soft-margin classifiers. In the next lab, you'll try to code these fairly straightforward linear classifiers from scratch!\u003c/p\u003e","frontPage":false},{"exportId":"model-tuning-and-pipelines-introduction","title":"Model Tuning and Pipelines - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-pipelines-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have learned the basics of a supervised learning workflow, it's time to get into some more-advanced techniques! In this section you'll learn about tools for tuning model hyperparameters, building pipelines, and persisting your trained model on disk.\u003c/p\u003e\n\n\u003ch2\u003eTuning Model Hyperparameters with GridSearchCV\u003c/h2\u003e\n\n\u003cp\u003eWith non-parametric models such as decision trees and k-nearest neighbors, you have seen that there are various hyperparameters that you can specify when you instantiate the model. For example, the maximum depth of the tree, or the number of neighbors. Often these hyperparameters help to balance the bias-variance trade-off between underfitting and overfitting and are important for finding the optimal model.\u003c/p\u003e\n\n\u003cp\u003eWith so many different hyperparameter combinations to try out, it can be difficult to write clean, readable code. Fortunately there is a tool from scikit-learn called \u003ccode\u003eGridSearchCV\u003c/code\u003e that is specifically designed to search through a \"grid\" of hyperparameters! In this section we'll introduce how to use this tool.\u003c/p\u003e\n\n\u003ch2\u003eMachine Learning Pipelines\u003c/h2\u003e\n\n\u003cp\u003ePipelines are extremely useful for allowing data scientists to quickly and consistently transform data, train machine learning models, and make predictions.\u003c/p\u003e\n\n\u003cp\u003eBy now, you know that the data science process is a flow of activities, from inspecting the data to cleaning it, transforming it, running a model, and discussing the results. Wouldn't it be nice if there was a streamlined process to create nice machine learning workflows? Enter the \u003ccode\u003ePipeline\u003c/code\u003e class in scikit-learn!\u003c/p\u003e\n\n\u003cp\u003eIn this section, you'll learn how you can use a pipeline to integrate several steps of the machine learning workflow. Additionally, you'll compare several classification techniques with each other, and integrate grid search in your pipeline so you can tune several hyperparameters in each of the machine learning models while also avoiding data leakage.\u003c/p\u003e\n\n\u003ch2\u003ePickle and Model Deployment\u003c/h2\u003e\n\n\u003cp\u003eSo far, as soon as you shut down your notebook kernel, your model ceases to exist. If you wanted to use the model to make predictions again, you would need to re-train the model. This is time-consuming and makes your model a lot less useful.\u003c/p\u003e\n\n\u003cp\u003eLuckily there are techniques to \u003cem\u003epickle\u003c/em\u003e your model -- basically, to store the model for later, so that it can be loaded and can make predictions without being trained again. Pickled models are also typically used in the context of model deployment, where your model can be used as the backend of an API!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eThis section only scratches the surface of the advanced modeling tools you might use as a data scientist. Get ready to optimize your workflow and get beyond the basics!\u003c/p\u003e","frontPage":false},{"exportId":"object-oriented-programming-introduction","title":"Object-Oriented Programming - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-oop-intro-v2-4\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section, you'll be introduced to the concept of object-oriented programming (OOP) in Python. OOP has become a foundational practice in much of software development and programming, allowing developers to build upon each other's code in a fluent manner.\u003c/p\u003e\n\n\u003ch2\u003eProgramming Paradigms\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eProgramming paradigms\u003c/em\u003e\u003c/strong\u003e are formal approaches for structuring code to achieve the desired results.\u003c/p\u003e\n\n\u003ch3\u003eWhy Do We Need Them?\u003c/h3\u003e\n\n\u003cp\u003eFor very simple programming tasks, there is essentially only one \"correct\" way to structure the code. For example, if you needed to print the string \"Hello, world!\", this is how you would do it:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Hello, world!\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight plaintext\"\u003e\u003ccode\u003eHello, world!\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBut once your code starts to get more complex, the structure gets less intuitive and obvious. For example, if you needed to reshape some data then display a bar graph, or fit a model then use it to make predictions, how would you design that?\u003c/p\u003e\n\n\u003cp\u003eDeciding on a paradigm and sticking to it helps to guide your code design choices, and helps others to understand what your code is doing.\u003c/p\u003e\n\n\u003ch3\u003eProcedural Programming\u003c/h3\u003e\n\n\u003cp\u003eThe oldest (and probably most intuitive) modern programming paradigm is procedural programming. This involves writing a series of sequential steps to be executed, possibly with the use of techniques for \u003cstrong\u003e\u003cem\u003econtrol flow\u003c/em\u003e\u003c/strong\u003e (e.g. \u003ccode\u003eif\u003c/code\u003e statements) and \u003cstrong\u003e\u003cem\u003emodular procedures\u003c/em\u003e\u003c/strong\u003e (e.g. functions).\u003c/p\u003e\n\n\u003cp\u003eData science code written in a \u003cstrong\u003enotebook\u003c/strong\u003e is almost always following a procedural programming paradigm. It is useful for telling a story with a single thread, but less useful for building libraries or software that runs without human intervention. Once code starts to get more complicated, we start incorporating more-complex paradigms such as functional programming or OOP.\u003c/p\u003e\n\n\u003ch3\u003eFunctional Programming\u003c/h3\u003e\n\n\u003cp\u003e\"Purely functional\" programming, using a language like Haskell or Clojure, means that procedural programming is abandoned entirely -- rather than a series of steps, the program consists only of functions, which in turn can be composed of functions or apply functions.\u003c/p\u003e\n\n\u003cp\u003eIn the development of data science libraries, they tended not to use purely functional programming, but nevertheless incorporated some functional principles.\u003c/p\u003e\n\n\u003cp\u003eFor example, here is the functional interface to Matplotlib:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eplt\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efigure\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"My Graph\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003exlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"x Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eylabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"y Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/raw/master/index_files/index_6_0.png\" alt=\"png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eNote that we created this graph without instantiating any variables. We just imported the library, then called a series of functions to create the desired graph. We could rewrite that code snippet like this, to make that aspect even clearer:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003efigure\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003exlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eylabel\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003efigure\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003cspan class=\"n\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"My Graph\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003exlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"x Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eylabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"y Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/raw/master/index_files/index_8_0.png\" alt=\"png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThis approach is still preferred by some \"old school\" data science practitioners, but it has some issues.\u003c/p\u003e\n\n\u003cp\u003eIt uses \u003cstrong\u003e\u003cem\u003eglobal variables\u003c/em\u003e\u003c/strong\u003e, which can get messy as code gets more complex. When the \u003ccode\u003etitle()\u003c/code\u003e function is called in the above snippet, for example, the internal logic first has to find the current global axes object, then apply the label to that object. For a programmer to understand what axes object that is, they would need to closely follow the steps of the code, since there is no unique variable assigned to it. With no variable assigned, that also means that the code is less flexible and steps must be performed \u003cstrong\u003e\u003cem\u003eone at a time\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3\u003eObject-Oriented Programming (OOP)\u003c/h3\u003e\n\n\u003cp\u003eObject-oriented programming takes these global variables and functions and makes them into \"member variables\" (AKA \u003cstrong\u003e\u003cem\u003eattributes\u003c/em\u003e\u003c/strong\u003e) and \"member functions\" (AKA \u003cstrong\u003e\u003cem\u003emethods\u003c/em\u003e\u003c/strong\u003e). This allows code to be more organized and clear.\u003c/p\u003e\n\n\u003cp\u003eFor example, in the previous functional Matplotlib example, you might ask \u003cem\u003eWhat is \u003ccode\u003etitle()\u003c/code\u003e being called on? Is it the figure or the axes?\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eTo answer this, we could look at the \u003ca href=\"https://github.com/matplotlib/matplotlib/blob/v3.5.1/lib/matplotlib/pyplot.py#L3024-L3027\"\u003eMatplotlib source code\u003c/a\u003e, which shows this:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003etitle\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efontdict\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epad\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003egca\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eset_title\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elabel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efontdict\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003efontdict\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eloc\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epad\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003epad\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003ekwargs\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003egca()\u003c/code\u003e means \"get current axes\", so we can tell that this is being applied to the axes.\u003c/p\u003e\n\n\u003cp\u003eOr if we use the object-oriented Matplotlib interface instead, the answer becomes much clearer, just by looking at our code:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight python\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ematplotlib.pyplot\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eplt\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003efig\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eax\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eplt\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esubplots\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebar\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e5\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e7\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e8\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_title\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"My Graph\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_xlabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"x Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003eax\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eset_ylabel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"y Label\"\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"https://github.com/learn-co-curriculum/dsc-oop-intro-v2-4/raw/master/index_files/index_11_0.png\" alt=\"png\"\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see, the title is being applied to the axes, not the figure. We can tell because the method call is structured like \u003ccode\u003eax.\u0026lt;method name\u0026gt;()\u003c/code\u003e and \u003ccode\u003eax\u003c/code\u003e is our axes variable.\u003c/p\u003e\n\n\u003cp\u003eA key takeaway here is that \u003cstrong\u003e\u003cem\u003eyou can often do the exact same thing using different paradigms\u003c/em\u003e\u003c/strong\u003e. They are just different approaches to structuring code, and different people might prefer different approaches.\u003c/p\u003e\n\n\u003ch2\u003eOOP Topics\u003c/h2\u003e\n\n\u003cp\u003eIn this section, we will cover:\u003c/p\u003e\n\n\u003ch3\u003eClasses and Instances\u003c/h3\u003e\n\n\u003cp\u003eA Python class can be thought of as the blueprint for creating a code object. These objects are known as an instance objects or instances. We'll go over how to create classes as well as instances.\u003c/p\u003e\n\n\u003ch3\u003eMethods and Attributes\u003c/h3\u003e\n\n\u003cp\u003eNext, we'll dive deeper into how to specify and invoke the functions and variables that are \"bound\" to instance objects. This includes the \u003cstrong\u003e\u003cem\u003eencapsulation\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eabstraction\u003c/em\u003e\u003c/strong\u003e principles of OOP.\u003c/p\u003e\n\n\u003ch3\u003eInheritance\u003c/h3\u003e\n\n\u003cp\u003eInheritance means that classes can be defined that take on the traits of other classes. This is especially useful when interacting with complex code libraries.\u003c/p\u003e\n\n\u003ch3\u003eOOP and Scikit-Learn\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"https://github.com/scikit-learn/scikit-learn/raw/main/doc/logos/scikit-learn-logo.png\" alt=\"scikit-learn logo\"\u003e\u003c/p\u003e\n\n\u003cp\u003eScikit-learn is the most popular machine learning library in use today, and its organization relies heavily on object-oriented programming. We'll go over the types of classes used and some of the most common methods and attributes you should know about.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eObject-oriented programming (OOP) is a way of organizing your code that can make many types of applications easier to write by combining related variables/properties and functions/methods into objects containing both behavior and state.\u003c/p\u003e","frontPage":false},{"exportId":"model-tuning-and-pipelines-recap","title":"Model Tuning and Pipelines - Recap","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-pipelines-recap\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-recap\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-pipelines-recap/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eKey Takeaways\u003c/h2\u003e\n\n\u003cp\u003eThe key takeaways from this section include:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eMachine learning \u003cstrong\u003e\u003cem\u003epipelines\u003c/em\u003e\u003c/strong\u003e create a nice workflow to combine data manipulations, preprocessing, and modeling\u003c/li\u003e\n\u003cli\u003eMachine learning pipelines can be used along with \u003cstrong\u003e\u003cem\u003egrid search\u003c/em\u003e\u003c/strong\u003e to evaluate several parameter settings \n\n\u003cul\u003e\n\u003cli\u003eGrid search can considerably blow up computation time when computing for several parameters along with cross-validation\u003c/li\u003e\n\u003cli\u003eSome models are very sensitive to hyperparameter changes, so they should be chosen with care, and even with big grids a good outcome isn't always guaranteed\u003c/li\u003e\n\u003c/ul\u003e\u003c/li\u003e\n\u003cli\u003eMachine learning pipelines can also be \u003cstrong\u003e\u003cem\u003epickled\u003c/em\u003e\u003c/strong\u003e so that they can be used in the future without re-training\u003c/li\u003e\n\u003cli\u003eModel \u003cstrong\u003e\u003cem\u003edeployment\u003c/em\u003e\u003c/strong\u003e can be something as simple as pickling a model, or a more complex approach like a \u003cstrong\u003e\u003cem\u003ecloud function\u003c/em\u003e\u003c/strong\u003e that exposes model predictions through an HTTP API\u003c/li\u003e\n\u003c/ul\u003e","frontPage":false},{"exportId":"topic-26-lesson-priorities-live","title":"Topic 26 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.228%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003eDecision Trees\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 42.2964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 7.69282%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Decision Trees - Introduction\" href=\"pages/decision-trees-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/decision-trees-introduction\" data-api-returntype=\"Page\"\u003eDecision Trees - Introduction\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Introduction to Decision Trees\" href=\"pages/introduction-to-decision-trees\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/introduction-to-decision-trees\" data-api-returntype=\"Page\"\u003eIntroduction to Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Entropy and Information Gain\" href=\"pages/entropy-and-information-gain\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/entropy-and-information-gain\" data-api-returntype=\"Page\"\u003eEntropy and Information Gain\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"ID3 Classification Trees: Perfect Split with Information Gain - Lab\" href=\"assignments/gbfbf401b01ebcd3a4fb3a809ecca4366\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186993\" data-api-returntype=\"Assignment\"\u003eID3 Classification Trees: Perfect Split with Information Gain - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Building Trees using scikit-learn\" href=\"assignments/g322a58b3e3dc9b4bab6c5c9f989be7b6\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186961\" data-api-returntype=\"Assignment\"\u003eBuilding Trees using scikit-learn\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Building Trees using scikit-learn - Lab\" href=\"assignments/gabdf55215fd9529a86ae72cbec0a0c8f\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186962\" data-api-returntype=\"Assignment\"\u003eBuilding Trees using scikit-learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Hyperparameter Tuning and Pruning in Decision Trees\" href=\"pages/hyperparameter-tuning-and-pruning-in-decision-trees\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/hyperparameter-tuning-and-pruning-in-decision-trees\" data-api-returntype=\"Page\"\u003eHyperparameter Tuning and Pruning in Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Hyperparameter Tuning and Pruning in Decision Trees - Lab\" href=\"assignments/gceceeb05c5feb34554c4ac6d68cf5757\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/186992\" data-api-returntype=\"Assignment\"\u003eHyperparameter Tuning and Pruning in Decision Trees - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Regression with CART Trees\" href=\"assignments/g3bef8e1ab0e4730537fdf1fb5e809498\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187037\" data-api-returntype=\"Assignment\"\u003eRegression with CART Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Regression with CART Trees - Lab\" href=\"assignments/g79619abf81fde3603d82f5fc24d233fe\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187038\" data-api-returntype=\"Assignment\"\u003eRegression with CART Trees - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;2nd\u0026quot;}\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Regression Trees and Model Optimization - Lab\" href=\"assignments/gfb58ddbd96ed5ad6bfe046755028f202\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/187036\" data-api-returntype=\"Assignment\"\u003eRegression Trees and Model Optimization - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Quiz: Decision Trees\" href=\"quizzes/gc3c333700dc3d9b001b40f75e7ede459\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30634\" data-api-returntype=\"Quiz\"\u003eQuiz: Decision Trees\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 96.228%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eDecision Trees\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003cth style=\"width: 42.2964%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 7.69282%; text-align: center; height: 27px;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 42.2964%;\"\u003e\u003ca title=\"Short Video: Regression Trees\" href=\"pages/short-video-regression-trees\"\u003eShort Video: Regression Trees\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003cstrong\u003e\u003ca title=\"Decision Trees Exit Ticket\" href=\"quizzes/gc01819b9360073166897477919216e53\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/30630\" data-api-returntype=\"Quiz\"\u003eDecision Trees Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;1st\u0026quot;}\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr style=\"height: 30px;\"\u003e\n\u003ctd style=\"width: 42.2964%; height: 27px;\"\u003e\u003ca title=\"Decision Trees - Recap\" href=\"pages/decision-trees-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/decision-trees-recap\" data-api-returntype=\"Page\"\u003eDecision Trees - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 7.69282%; height: 27px; text-align: center;\" data-sheets-value=\"{\u0026quot;1\u0026quot;:2,\u0026quot;2\u0026quot;:\u0026quot;3rd\u0026quot;}\"\u003e3rd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e","frontPage":false},{"exportId":"blogging-overview","title":"Blogging Overview","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-blogging-overview\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-blogging-overview/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e  \u003ch2\u003eIntroduction\u003c/h2\u003e  \u003cp\u003eIn this lesson, we discuss how to write good blog posts that meet Flatiron School's requirements.\u003c/p\u003e  \u003ch2\u003eObjectives\u003c/h2\u003e  \u003cp\u003eThis lesson covers...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhy blogging is valuable\u003c/li\u003e \u003cli\u003eTopics to blog about\u003c/li\u003e \u003cli\u003eWhat makes for a good blog post\u003c/li\u003e \u003cli\u003eHow to start your blog\u003c/li\u003e \u003cli\u003eFlatiron School blog requirements \u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhy Should I Blog?\u003c/h2\u003e  \u003cp\u003eBlogging has many benefits:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDevelop your written communication skills.\u003c/strong\u003e Your writing ability will be critical to your success when completing job applications and presenting your work to colleagues. Blogging is great practice for identifying and clearly communicating the most important points of any subject.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eDemonstrate your talent to employers.\u003c/strong\u003e Potential employers will review your blog to determine whether to offer you an interview or a job. Some students have even been invited to interview or exempted from technical interviews based on their blogs.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eStrengthen your knowledge.\u003c/strong\u003e Blogging helps you explore new topics, deepen your understanding, and crystallize what you've learned.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e\u003cstrong\u003eHelp your peers and the broader community.\u003c/strong\u003e Have you ever Googled a question you had and found the answer on a blog? Writing blog posts helps others who are following in your footsteps!\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Should I Blog About?\u003c/h2\u003e  \u003cp\u003eHere are some blog topic ideas:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eWhy did you decide to learn data science?\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDescribe how a DS technique works, when you might use it, and its strengths/weaknesses.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eSummarize an End of Phase Project by explaining your problem, the dataset, your methodology, and your results.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eDive into something that you want to learn more about, maybe because you find it challenging or it wasn't covered in the course.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eWrite a tutorial to help aspiring data scientists to implement a tool or method.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003eFind an interesting data science paper and summarize why it is important. This can be a new paper from the past few months, or you can refer to \u003ca href=\"https://docs.google.com/spreadsheets/d/1UYmAT13AAknrOatzLeeAsN4tS7ENjn2fpJNGzOZ67rQ/edit?usp=sharing\"\u003ethis spreadsheet\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eWhat Does A Good Blog Post Look Like?\u003c/h2\u003e  \u003cp\u003eWe recommend you take a look at our \u003ca href=\"https://drive.google.com/drive/folders/1UBiRCRLzVP5CHU3PJNwoMZAe3ajUBm2a?usp=sharing\"\u003eblog templates\u003c/a\u003e and \u003ca href=\"https://docs.google.com/document/d/1eqL8Dsj7dH7s_MRnf_4-3kCiSz72POHTfb-sBRN5Zhs/edit?usp=sharing\"\u003eexamples\u003c/a\u003e to get an idea for what makes a blog post good.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eStrike a balance between providing a meaningful investigation of your topic and being concise. Constrain the scope so it will be interesting and digestible in about 1000-3000 words (this is not a firm limit).\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eUse clear and consistent formatting to make your content accessible and professional-looking.\u003c/p\u003e  \u003cul\u003e \u003cli\u003eWhen presenting code, use code snippets instead of screenshots.\u003c/li\u003e \u003cli\u003eMake URLs into hyperlinks that are easy for readers to click into.\u003c/li\u003e \u003cli\u003eUse headings to provide structure and flow to your post.\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\u003cp\u003eCite and link to resources you used to write your post.\u003c/p\u003e\u003c/li\u003e \u003c/ul\u003e  \u003ch2\u003eHow Do I Start My Blog?\u003c/h2\u003e  \u003cp\u003eIf you already have a professional blog that you'd like to use for your data science content, you can add your posts to that. Otherwise, you will need to start a new blog. If you have a personal blog, you should avoid using it for this purpose so that you can continue using it for personal content without worrying about how it might be perceived by potential employers.\u003c/p\u003e  \u003cp\u003eThere are multiple blogging platforms to choose from that make it easy to start a blog, here are some of our favorites:\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003ca href=\"https://www.blogger.com/\"\u003eBlogger\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://dev.to/\"\u003edev.to\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://medium.com/\"\u003eMedium\u003c/a\u003e\u003c/li\u003e \u003cli\u003e\u003ca href=\"https://wordpress.com/\"\u003eWordpress\u003c/a\u003e\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eDifferent platforms have different pros and cons, so do a little research to decide what is best for you.\u003c/p\u003e  \u003ch2\u003eBlog Requirements\u003c/h2\u003e  \u003cp\u003eTo succeed in your career transition and graduate from Flatiron School, you must complete the following activities. These requirements are designed to give you the best opportunity to deepen your knowledge, practice communication skills, and showcase yourself to potential employers.\u003c/p\u003e  \u003cul\u003e \u003cli\u003e\u003cp\u003eSet up a publicly accessible blog \u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003ePublish at least four blog posts on it, including \u003cstrong\u003eone per Phase for Phases 1-4\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eSubmit URLs to your posts \u003cstrong\u003eby the end of each Phase\u003c/strong\u003e in the Blog Post assignments\u003c/p\u003e  \u003cul\u003e \u003cli\u003eThese assignments are located in the Milestones topics of the Phase 1-4 Canvas courses\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003cli\u003e\n\u003cp\u003eWrite blog posts that...\u003c/p\u003e  \u003cul\u003e \u003cli\u003eDiscuss data science topics\u003c/li\u003e \u003cli\u003eAre composed primarily of original material you wrote\u003c/li\u003e \u003cli\u003eInclude proper attribution\u003c/li\u003e \u003cli\u003eHave high-quality content and formatting\u003c/li\u003e \u003cli\u003eAre something you would proudly show to a potential employer\u003c/li\u003e \u003c/ul\u003e\n\u003c/li\u003e \u003c/ul\u003e  \u003cp\u003eAfter you submit your blog posts, your teacher will grade them as Complete or Incomplete. Your blogs must all be submitted on time and receive Complete grades in order to continue through your program.\u003c/p\u003e  \u003cp\u003e‚ú®Have fun and happy blogging!‚ú®\u003c/p\u003e","frontPage":false},{"exportId":"topic-21-lesson-priorities-live","title":"Topic 21 Lesson Priorities (Live)","type":"WikiPage","content":"\u003cp\u003e\u003cspan style=\"font-size: 18pt;\"\u003eWelcome to Phase 3!\u003c/span\u003e\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98.0716%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete Before \u003cem\u003ePython OOP\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth style=\"width: 82.0057%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 17.7609%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Object-Oriented Programming - Introduction\" href=\"pages/object-oriented-programming-introduction\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/object-oriented-programming-introduction\" data-api-returntype=\"Page\"\u003eObject-Oriented Programming - Introduction\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Classes and Instances\" href=\"assignments/g62af0481da1247e246f90c684e1972d0\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197091\" data-api-returntype=\"Assignment\"\u003eClasses and Instances\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Classes and Instances - Lab\" href=\"assignments/g119766e414e353de35cb25c1e78483a1\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197092\" data-api-returntype=\"Assignment\"\u003eClasses and Instances - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Instance Methods\" href=\"assignments/g69af0808045f764ffe43b978fa3ccaf4\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197097\" data-api-returntype=\"Assignment\"\u003eInstance Methods\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Instance Methods - Lab\" href=\"assignments/gaf65bb50e8026994e8ca240f8354c3d2\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197098\" data-api-returntype=\"Assignment\"\u003eInstance Methods - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"A Deeper Dive into \u0026quot;self\u0026quot;\" href=\"assignments/g381033ca9bd27a5adad694abdecab727\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197088\" data-api-returntype=\"Assignment\"\u003eA Deeper Dive into \"self\"\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Object Attributes - Lab\" href=\"assignments/gfbd4429ffbc0336acf92ade5373a1701\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197103\" data-api-returntype=\"Assignment\"\u003eObject Attributes - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Object Initialization\" href=\"assignments/g4d076a4b2a78d13ff48248cac54840a5\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197104\" data-api-returntype=\"Assignment\"\u003eObject Initialization\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Object Initialization - Lab\" href=\"assignments/ge3bb362ff99b1f27ecce7f46eab3b72c\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197105\" data-api-returntype=\"Assignment\"\u003eObject Initialization - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Quiz: Object-Oriented Programming\" href=\"quizzes/gcade38300ac325cdd762ba3921374c78\"\u003e\u003cstrong\u003eQuiz: Object-Oriented Programming\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98.0716%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003ePython OOP\u003c/em\u003e Lecture, Before \u003cem\u003eOOP with Scikit-Learn\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth style=\"width: 82.0057%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 17.7609%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Python OOP Exit Ticket\" href=\"quizzes/g099556e8c6fde026836f7808d2964805\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33592\" data-api-returntype=\"Quiz\"\u003e\u003cstrong\u003ePython OOP Exit Ticket\u003c/strong\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Inheritance\" href=\"assignments/ge5a4685436b8c307dbd0e02842aa8bb3\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197095\" data-api-returntype=\"Assignment\"\u003eInheritance\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Inheritance - Lab\" href=\"assignments/ga7235058a8fe896a98b13eac29d0a628\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197096\" data-api-returntype=\"Assignment\"\u003eInheritance - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"OOP with Scikit-Learn\" href=\"assignments/gf90dbcad5fd270a68f87e199399d7cf4\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197101\" data-api-returntype=\"Assignment\"\u003eOOP with Scikit-Learn\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"OOP with Scikit-Learn - Lab\" href=\"assignments/gc6044b3c144d2460d591e93a47b50523\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/assignments/197102\" data-api-returntype=\"Assignment\"\u003eOOP with Scikit-Learn - Lab\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98.0716%;\" border=\"1\"\u003e\u003ccaption\u003ePriorities to Complete After \u003cem\u003eOOP with Scikit-Learn\u003c/em\u003e Lecture\u003c/caption\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003cth style=\"width: 82.0057%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003eLesson\u003c/strong\u003e\u003c/th\u003e\n\u003cth style=\"width: 17.7609%; text-align: center;\" scope=\"col\"\u003e\u003cstrong\u003ePriority\u003c/strong\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"OOP with Scikit-Learn Exit Ticket\" href=\"quizzes/g2dc61175cd9d50f14d0654fc91001d78\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33588\" data-api-returntype=\"Quiz\"\u003eOOP with Scikit-Learn Exit Ticket\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"Short Video: Debugging with OOP\" href=\"pages/short-video-debugging-with-oop\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/short-video-debugging-with-oop\" data-api-returntype=\"Page\"\u003eShort Video: Debugging with OOP\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003cstrong\u003e\u003ca title=\"‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\" href=\"quizzes/g0c9a5bdde5f8af00ba4e30f35458c335\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/quizzes/33590\" data-api-returntype=\"Quiz\"\u003e‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\u003c/a\u003e\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e\u003cstrong\u003e1st*\u003c/strong\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"width: 82.0057%;\"\u003e\u003ca title=\"Object-Oriented Programming - Recap\" href=\"pages/object-oriented-programming-recap\" data-api-endpoint=\"https://learning.flatironschool.com/api/v1/courses/5826/pages/object-oriented-programming-recap\" data-api-returntype=\"Page\"\u003eObject-Oriented Programming - Recap\u003c/a\u003e\u003c/td\u003e\n\u003ctd style=\"width: 17.7609%; text-align: center;\"\u003e2nd\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003e*Cumulative labs may be used for pairing exercises and might not be published yet; contact your instructor if you have questions\u003c/strong\u003e\u003c/p\u003e","frontPage":false},{"exportId":"ensemble-methods-introduction","title":"Ensemble Methods - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this section, you'll learn about some of the most powerful machine learning algorithms: ensemble models! This lesson summarizes the topics we'll be covering in this section.\u003c/p\u003e\n\u003ch2\u003eEnsembles\u003c/h2\u003e\n\u003cp\u003eThe idea of ensembles is to bring together multiple models to use them to improve the quality of your predictions when compared to just using a single model. In many real-world problems and Kaggle competitions, ensemble methods tend to outperform any single model.\u003c/p\u003e\n\u003ch3\u003eEnsemble Methods\u003c/h3\u003e\n\u003cp\u003eWe start the section by providing an introduction to the concept of ensemble methods, explaining how they take advantage of the delphic technique (or \"wisdom of crowds\") where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.\u003c/p\u003e\n\u003cp\u003eWe also provide an introduction to the idea of bagging (Bootstrap Aggregation).\u003c/p\u003e\n\u003ch3\u003eRandom Forests\u003c/h3\u003e\n\u003cp\u003eWe then look at random forests - an ensemble method for decision trees that takes advantage of bagging and the subspace sampling method to create a \"forest\" of decision trees that provides consistently better predictions than any single decision tree.\u003c/p\u003e\n\u003ch3\u003eGridsearchCV\u003c/h3\u003e\n\u003cp\u003eWe will also introduce some of the common hyperparameters for tuning decision trees. In this lesson, we look at how you can use GridSearchCV to perform an exhaustive search across multiple hyperparameters and multiple possible values to come up with a better performing model.\u003c/p\u003e\n\u003ch3\u003eGradient Boosting and Weak Learners\u003c/h3\u003e\n\u003cp\u003eNext up, we introduce the concept of boosting which is at the heart of some of the most powerful ensemble methods such as Adaboost and Gradient Boosted Trees.\u003c/p\u003e\n\u003ch3\u003eXGBoost\u003c/h3\u003e\n\u003cp\u003eFinally, we end this section by introducing XGBoost (eXtreme Gradient Boosting) - the top gradient boosting algorithm currently in use.\u003c/p\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eYou will often find yourself using a range of ensemble techniques to improve the performance of your models, so this section will introduce you to the techniques that will help you to improve the quality of your models.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\n\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" title=\"Thumbs up!\" alt=\"thumbs up\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\u003cimg id=\"thumbs-down\" title=\"Thumbs down!\" alt=\"thumbs down\" data-repository=\"dsc-ensemble-methods-section-intro\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-ensemble-methods-section-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\n\u003c/footer\u003e","frontPage":false},{"exportId":"short-video-naive-bayes-by-hand","title":"Short Video: Naive Bayes by Hand","type":"WikiPage","content":"\u003cdiv style=\"padding:62.5% 0 0 0;position:relative;\"\u003e\u003ciframe src=\"https://player.vimeo.com/video/713802877?h=fdecdbfde4\u0026amp;badge=0\u0026amp;autopause=0\u0026amp;player_id=0\u0026amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen=\"\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" title=\"one-hot_encoding_phase2_gd\"\u003e\u003c/iframe\u003e\u003c/div\u003e","frontPage":false},{"exportId":"evaluation-metrics","title":"Evaluation Metrics","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-evaluation-metrics\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluation-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about common \u003cstrong\u003e\u003cem\u003eevaluation metrics\u003c/em\u003e\u003c/strong\u003e used to quantify the performance of classifiers!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eEvaluate classification models using the evaluation metrics appropriate for a specific problem \u003c/li\u003e\n\u003cli\u003eDefine precision and recall \u003c/li\u003e\n\u003cli\u003eDefine accuracy and F1 score \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eEvaluation metrics for classification\u003c/h2\u003e\n\n\u003cp\u003eNow that we've started discussing classification, it's time to examine comparing models to one other and choosing the models that have the best fit. Previously in regression, you were predicting values so it made sense to discuss error as a distance of how far off the estimates were from the actual values. However, in classifying a binary variable you are either correct or incorrect. As a result, we tend to deconstruct this as how many false positives versus false negatives there are in a model. In particular, there are a few different specific measurements when evaluating the performance of a classification algorithm.  \u003c/p\u003e\n\n\u003cp\u003eLet's work through these evaluation metrics to understand what each metric tells us.\u003c/p\u003e\n\n\u003ch2\u003ePrecision and recall\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e are two of the most basic evaluation metrics available to us. \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e measures how precise the predictions are, while \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e indicates what percentage of the classes we're interested in were actually captured by the model. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/images/new_EvalMatrices.png\" alt=\"diagram showing precision and recall using blue and orange circles and dots\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003ch3\u003ePrecision\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how to use information found in a confusion matrix to calculate the precision of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} \" src=\"/equation_images/%20%255Ctext{Precision}%20=%20%255Cfrac{%255Ctext{Number%20of%20True%20Positives}}{%255Ctext{Number%20of%20Predicted%20Positives}}\" alt=\"{\" data-equation-content=\" \\text{Precision} = \\frac{\\text{Number of True Positives}}{\\text{Number of Predicted Positives}} \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eTo reuse a previous analogy of a model that predicts whether or not a person has a certain disease, precision allows us to answer the following question:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the times the model said someone had a disease, how many times did the patient in question actually have the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that a high precision score can be a bit misleading.  For instance, let's say we take a model and train it to make predictions on a sample of 10,000 patients. This model predicts that 6000 patients have the disease when in reality, only 5500 have the disease.  This model would have a precision of 91.6%. Now, let's assume we create a second model that only predicts that a person is sick when it's incredibly obvious.  Out of 10,000 patients, this model only predicts that 5 people in the entire population are sick.  However, each of those 5 times, it is correct.  model 2 would have a precision score of 100%, even though it missed 5,495 cases where the patient actually had the disease! In this way, more conservative models can have a high precision score, but this doesn't necessarily mean that they are the \u003cem\u003ebest performing\u003c/em\u003e model!\u003c/p\u003e\n\n\u003ch3\u003eRecall\u003c/h3\u003e\n\n\u003cp\u003eThe following formula shows how we can use the information found in a confusion matrix to calculate the recall of a model:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} \" src=\"/equation_images/%20%255Ctext{Recall}%20=%20%255Cfrac{%255Ctext{Number%20of%20True%20Positives}}{%255Ctext{Number%20of%20Actual%20Total%20Positives}}\" alt=\"{\" data-equation-content=\" \\text{Recall} = \\frac{\\text{Number of True Positives}}{\\text{Number of Actual Total Positives}} \"\u003e\u003c/p\u003e \u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eFollowing the same disease analogy, recall allows us to ask:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the patients we saw that actually had the disease, what percentage of them did our model correctly identify as having the disease?\"\u003c/p\u003e\n\n\u003cp\u003eNote that recall can be a bit of a tricky statistic because improving our recall score doesn't necessarily always mean a better model overall. For example, our model could easily score 100% for recall by just classifying every single patient that walks through the door as having the disease in question. Sure, it would have many False Positives, but it would also correctly identify every single sick person as having the disease!\u003c/p\u003e\n\n\u003ch3\u003eThe relationship between precision and recall\u003c/h3\u003e\n\n\u003cp\u003eAs you may have guessed, precision and recall have an inverse relationship. As our recall goes up, our precision will go down, and vice versa. If this doesn't seem intuitive, let's examine this through the lens of our disease analogy. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with recall will have a very low threshold for declaring someone as sick because they are most worried about sick patients. Their precision will be quite low, because they classify almost everyone as sick, and don't care when they're wrong -- they only care about making sure that sick people are identified as sick. \u003c/p\u003e\n\n\u003cp\u003eA doctor that is overly obsessed with precision will have a very high threshold for declaring someone as sick, because they only declare someone as sick when they are completely sure that they will be correct if they declare a person as sick. Although their precision will be very high, their recall will be incredibly low, because a lot of people that are sick but don't meet the doctor's threshold will be incorrectly classified as healthy. \u003c/p\u003e\n\n\u003ch3\u003eWhich metric is better?\u003c/h3\u003e\n\n\u003cp\u003eA classic Data Science interview question is to ask \"What is better -- more false positives, or false negatives?\" This is a trick question designed to test your critical thinking on the topics of precision and recall. As you're probably thinking, the answer is \"It depends on the problem!\".  Sometimes, our model may be focused on a problem where False Positives are much worse than False Negatives, or vice versa. For instance, detecting credit card fraud. A False Positive would be when our model flags a transaction as fraudulent, and it isn't.  This results in a slightly annoyed customer. On the other hand, a False Negative might be a fraudulent transaction that the company mistakenly lets through as normal consumer behavior. In this case, the credit card company could be on the hook for reimbursing the customer for thousands of dollars because they missed the signs that the transaction was fraudulent! Although being wrong is never ideal, it makes sense that credit card companies tend to build their models to be a bit too sensitive, because having a high recall saves them more money than having a high precision score.\u003c/p\u003e\n\n\u003cp\u003eTake a few minutes and see if you can think of at least two examples each of situations where a high precision might be preferable to high recall, and two examples where high recall might be preferable to high precision. This is a common interview topic, so it's always handy to have a few examples ready!\u003c/p\u003e\n\n\u003ch2\u003eAccuracy and F1 score\u003c/h2\u003e\n\n\u003cp\u003eThe two most informative metrics that are often cited to describe the performance of a model are \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e. Let's take a look at each and see what's so special about them.\u003c/p\u003e\n\n\u003ch3\u003eAccuracy\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e is probably the most intuitive metric. The formula for accuracy is:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} \" src=\"/equation_images/%20%255Ctext{Accuracy}%20=%20%255Cfrac{%255Ctext{Number%20of%20True%20Positives%20+%20True%20Negatives}}{%255Ctext{Total%20Observations}}\" alt=\"{\" data-equation-content=\" \\text{Accuracy} = \\frac{\\text{Number of True Positives + True Negatives}}{\\text{Total Observations}} \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eAccuracy is useful because it allows us to measure the total number of predictions a model gets right, including both \u003cstrong\u003e\u003cem\u003eTrue Positives\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eTrue Negatives\u003c/em\u003e\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp\u003eSticking with our analogy, accuracy allows us to answer:\u003c/p\u003e\n\n\u003cp\u003e\"Out of all the predictions our model made, what percentage were correct?\"\u003c/p\u003e\n\n\u003cp\u003eAccuracy is the most common metric for classification. It provides a solid holistic view of the overall performance of our model. \u003c/p\u003e\n\n\u003ch3\u003eF1 score\u003c/h3\u003e\n\n\u003cp\u003eThe F1 score is a bit more tricky, but also more informative. F1 score represents the \u003cstrong\u003e\u003cem\u003eHarmonic Mean of Precision and Recall\u003c/em\u003e\u003c/strong\u003e.  In short, this means that the F1 score cannot be high without both precision and recall also being high. When a model's F1 score is high, you know that your model is doing well all around. \u003c/p\u003e\n\n\u003cp\u003eThe formula for F1 score is:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{F1 score} = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall} \" src=\"/equation_images/%20%255Ctext{F1%20score}%20=%202%255C%20%255Cfrac{Precision%255C%20x%255C%20Recall}{Precision%20+%20Recall}\" alt=\"{\" data-equation-content=\" \\text{F1 score} = 2\\ \\frac{Precision\\ x\\ Recall}{Precision + Recall} \"\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eTo demonstrate the effectiveness of F1 score, let's plug in some numbers and compare F1 score with a regular arithmetic average of precision and recall. \u003c/p\u003e\n\n\u003cp\u003eLet's assume that the model has 98% recall and 6% precision.  \u003c/p\u003e\n\n\u003cp\u003eTaking the arithmetic mean of the two, we get: \u003cimg class=\"equation_image\" title=\" \\frac{0.98 + 0.06}{2} = \\frac{1.04}{2} = 0.52 \" src=\"/equation_images/%20%255Cfrac{0.98%20+%200.06}{2}%20=%20%255Cfrac{1.04}{2}%20=%200.52\" alt=\"{\" data-equation-content=\" \\frac{0.98 + 0.06}{2} = \\frac{1.04}{2} = 0.52 \"\u003e\u003c/p\u003e\n\n\u003cp\u003eHowever, using these numbers in the F1 score formula results in:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cimg class=\"equation_image\" title=\" \\text{F1 score} = 2 \\frac{0.98 * 0.06}{0.98 + 0.06} = 2 \\frac{0.0588}{1.04} = 2(0.061152) = 0.122304\" src=\"/equation_images/%20%255Ctext{F1%20score}%20=%202%20%255Cfrac{0.98%20*%200.06}{0.98%20+%200.06}%20=%202%20%255Cfrac{0.0588}{1.04}%20=%202(0.061152)%20=%200.122304\" alt=\"{\" data-equation-content=\" \\text{F1 score} = 2 \\frac{0.98 * 0.06}{0.98 + 0.06} = 2 \\frac{0.0588}{1.04} = 2(0.061152) = 0.122304\"\u003e\u003c/p\u003e or 12.2%!\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eAs you can see, F1 score penalizes models heavily if it skews too hard towards either precision or recall. For this reason, F1 score is generally the most used metric for describing the performance of a model. \u003c/p\u003e\n\n\u003ch2\u003eWhich metric to use?\u003c/h2\u003e\n\n\u003cp\u003eThe metrics that are most important to a project will often be dependent on the business use case or goals for that model. This is why it's \u003cstrong\u003e\u003cem\u003every important\u003c/em\u003e\u003c/strong\u003e to understand why you're doing what you're doing, and how your model will be used in the real world! Otherwise, you may optimize your model for the wrong metric! \u003c/p\u003e\n\n\u003cp\u003eIn general, it is worth noting that it's a good idea to calculate all relevant metrics, when in doubt.  In most classification tasks, you don't know which model will perform best when you start. The common workflow is to train each different type of classifier, and select the best by comparing the performance of each. It's common to make tables like the one below, and highlight the best performer for each metric:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-evaluation-metrics/master/images/performance-comparisons.png\" alt=\"pandas dataframe showing different classification metrics as columns and different models as rows\"\u003e\u003c/p\u003e\n\n\u003ch2\u003eCalculate evaluation metrics with confusion matrices\u003c/h2\u003e\n\n\u003cp\u003eNote that we can only calculate any of the metrics discussed here if we know the \u003cstrong\u003e\u003cem\u003eTrue Positives, True Negatives, False Positives, and False Negatives\u003c/em\u003e\u003c/strong\u003e resulting from the predictions of a model. If we have a confusion matrix, we can easily calculate \u003cstrong\u003e\u003cem\u003ePrecision\u003c/em\u003e\u003c/strong\u003e, \u003cstrong\u003e\u003cem\u003eRecall\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eAccuracy\u003c/em\u003e\u003c/strong\u003e -- and if we know precision and recall, we can easily calculate \u003cstrong\u003e\u003cem\u003eF1 score\u003c/em\u003e\u003c/strong\u003e!\u003c/p\u003e\n\n\u003ch2\u003eClassification reports\u003c/h2\u003e\n\n\u003cp\u003eScikit-learn has a built-in function that will create a \u003cstrong\u003e\u003cem\u003eClassification Report\u003c/em\u003e\u003c/strong\u003e. This classification report even breaks down performance by individual class predictions for your model. You can find the \u003ccode\u003eclassification_report()\u003c/code\u003e function in the \u003ccode\u003esklearn.metrics\u003c/code\u003e module, which takes labels and predictions and returns the precision, recall, F1 score and support (number of occurrences of each label in \u003ccode\u003ey_true\u003c/code\u003e) for the results of a model. \u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you were introduced to several metrics which can be used to evaluate classification models. In the following lab, you'll write functions to calculate each of these manually, as well as explore how you can use existing functions in scikit-learn to quickly calculate and interpret each of these metrics. \u003c/p\u003e","frontPage":false},{"exportId":"classification-metrics-introduction","title":"Classification Metrics - Introduction","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classification-metrics-intro\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classification-metrics-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eClassification models can be more complex to evaluate than regression models. There are more trade-offs involved as well as different metrics that can be used.\u003c/p\u003e\n\n\u003ch2\u003eEvaluating Classifiers\u003c/h2\u003e\n\n\u003cp\u003eWe'll look at the practicalities of evaluating logistic regression models based on precision, recall, and accuracy to evaluate other classifiers.\u003c/p\u003e\n\n\u003cp\u003eWe also take a little time to look at how to plot a confusion matrix for a logistic regression classifier and introduce a couple of key concepts for determining the optimal precision-recall trade-off for a given classifier - Receiver Operating Characteristic (ROC) curves and AUC (the Area Under the Curve).\u003c/p\u003e\n\n\u003ch2\u003eClass Imbalance Problems\u003c/h2\u003e\n\n\u003cp\u003eWe then introduce the concept of class imbalance. Imagine a classifier for cancer where only 1 screened individual in 1000 is sick. You could obtain over 99 percent accuracy by just saying everyone is fine, but that wouldn't be a very useful approach. We look at the ideas of class weights and over/undersampling and how they can be used to work with highly imbalanced classes.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eMany of the concepts around model evaluation will be useful whenever you're trying to solve a classification problem.\u003c/p\u003e","frontPage":false},{"exportId":"gradient-descent-review","title":"Gradient Descent Review","type":"WikiPage","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-review\"\u003e\u003c/div\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-review/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eRecall that gradient descent is a numerical approximation method for finding optimized solutions to problems with no closed form. That is, some mathematical problems are very easy to solve analytically. A trivial example is basic algebra problems which you undoubtedly saw in grade school:\u003cbr\u003e\n\u003cimg class=\"equation_image\" title=\"x + 2 = 10\" src=\"https://learning.flatironschool.com/equation_images/x%20+%202%20=%2010\" alt=\"{\" data-equation-content=\"x + 2 = 10\"\u003e subtracting 2 from both sides you get \u003cimg class=\"equation_image\" title=\"x = 8\" src=\"https://learning.flatironschool.com/equation_images/x%20=%208\" alt=\"{\" data-equation-content=\"x = 8\"\u003e. Similarly, some more complex mathematical problems such as ordinary least squares, our preliminary regression approach, also have closed-form solutions where we can follow a rote procedure and be guaranteed a solution. In other cases, this is not possible and numerical approximation methods are used to find a solution. The first instance that you witnessed of this was adding the L1 and L2 (lasso and ridge, respectively) penalties to OLS regression. In these cases, numerical approximation methods, such as gradient descent, are used in order to find optimal or near-optimal solutions.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the elements of gradient descent in the context of a logistic regression \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eGradient descent\u003c/h2\u003e\n\n\u003cp\u003eGradient descent is grounded in basic calculus theory. Whenever you have a minimum or maximum, the derivative at that point is equal to zero. This is displayed visually in the picture below; the slope of the red tangent lines is equal to the derivative of the curve at that point. As you can see, the slope of all of these horizontal tangent lines will be zero. \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_dxdy0.png\" alt=\"higher-order polynomial graph with horizontal bars at all minima and maxima\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eThe gradient is simply another term for the derivative. Typically, this is the term used when we are dealing with multivariate data. The gradient is the rate of change, which is also the slope of the line tangent.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eBuilding upon this, gradient descent attempts to find the minimum of a function by taking successive steps in the steepest direction downhill.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-review/master/images/new_gradient.png\" alt=\"on the left, a 3d plot of a higher-order polynomial. on the right, an image representing finding a specific minimum\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhile this process guarantees a local minimum, the starting point and step size can affect the outcome. For example, for two different runs of gradient descent, one may lead to the global minimum while the other may lead to a local minimum.\u003c/p\u003e\n\n\u003cp\u003eRecall that the general outline for gradient descent is:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eDefine initial parameters:\n\n\u003col\u003e\n\u003cli\u003ePick a starting point\u003c/li\u003e\n\u003cli\u003ePick a step size \u003cimg class=\"equation_image\" title=\"\\alpha\" src=\"https://learning.flatironschool.com/equation_images/%255Calpha\" alt=\"{\" data-equation-content=\"\\alpha\"\u003e (alpha)\u003c/li\u003e\n\u003cli\u003eChoose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\u003c/li\u003e\n\u003cli\u003e(optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligible difference \u003c/li\u003e\n\u003c/ol\u003e\u003c/li\u003e\n\u003cli\u003eCalculate the gradient at the current point (initially, the starting point)\u003c/li\u003e\n\u003cli\u003eTake a step (of size alpha) in the direction of the gradient\u003c/li\u003e\n\u003cli\u003eRepeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you briefly reviewed that a gradient is the derivative of a function, which is the rate of change at a specific point. You then reviewed the intuition behind gradient descent, as well as some of its pitfalls. Finally, you saw a brief outline of the algorithm itself. In the next lab, you'll practice coding gradient descent and applying that to some simple mathematical functions.\u003c/p\u003e","frontPage":false},{"exportId":"k-nearest-neighbors-introduction","title":"K-Nearest Neighbors - Introduction","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this section you'll look at an intuitive algorithm known as K-Nearest Neighbors (KNN). KNN is an effective classification and regression algorithm that uses nearby points in order to generate a prediction. \u003c/p\u003e\n\n\u003ch2\u003eKNN\u003c/h2\u003e\n\n\u003cp\u003eThe K-Nearest Neighbors algorithm works as follows: \u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eChoose a point \u003c/li\u003e\n\u003cli\u003eFind the K-nearest points\n\n\u003col\u003e\n\u003cli\u003eK is a predefined user constant such as 1, 3, 5, or 11 \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003ePredict a label for the current point:\n\n\u003col\u003e\n\u003cli\u003eClassification - Take the most common class of the k neighbors\u003c/li\u003e\n\u003cli\u003eRegression - Take the average target metric of the k neighbors\u003c/li\u003e\n\u003cli\u003eBoth classification or regression can also be modified to use weighted averages based on the distance of the neighbors \u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eDistance metrics\u003c/h2\u003e\n\n\u003cp\u003eAn incredibly important decision when using the KNN algorithm is determining an appropriate distance metric. This makes a monumental impact to the output of the algorithm. While there are additional distance metrics, such as cosine distance which we will not cover, you'll get a solid introduction to distance metrics by looking at the standard Euclidean distance and its more generic counterpart, Minkowski distance.\u003c/p\u003e\n\n\u003ch2\u003eK-means\u003c/h2\u003e\n\n\u003cp\u003eWhile outside the scope of this section, it is worth mentioning the related K-means algorithm which uses similar principles as KNN but serves as an unsupervised learning clustering algorithm. In the K-means algorithm, K represents the number of clusters rather then the number of neighbors. Unlike KNN, K-means is an iterative algorithm which repeats until convergence. Nonetheless, its underlying principle is the same, in that it groups data points together using a distance metric in order to create homogeneous groupings.\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this brief lesson, you were introduced to the KNN algorithm. From here, you'll jump straight to the details of KNN, practice coding your own implementation and then get an introduction to use pre-built tools within scikit-learn for KNN.\u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-knn-intro\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-knn-intro\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-knn-intro/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false},{"exportId":"hyperparameter-tuning-and-pruning-in-decision-trees","title":"Hyperparameter Tuning and Pruning in Decision Trees","type":"WikiPage","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eHyperparameter tuning relates to how we sample candidate model architectures from the space of all possible hyperparameter values. This is often referred to as \u003cstrong\u003esearching the hyperparameter space for the optimum values\u003c/strong\u003e. In this lesson, we'll look at some of the key hyperparameters for decision trees and how they affect the learning and prediction processes. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eIdentify the role of pruning while training decision trees\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eList the different hyperparameters for tuning decision trees \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eHyperparameter Optimization\u003c/h2\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins.\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eBy contrast, the values of model parameters are derived via training as we have seen previously.\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, Lasso is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm. \u003c/p\u003e\n\n\u003cp\u003eIn this lesson, we'll look at these sorts of optimizations in the context of decision trees and see how these can affect the predictive performance as well as the computational complexity of the tree. \u003c/p\u003e\n\n\u003ch2\u003eTree pruning\u003c/h2\u003e\n\n\u003cp\u003eNow that we know how to grow a decision tree using Python and scikit-learn, let's move on and practice \u003cstrong\u003eoptimizing\u003c/strong\u003e a classifier. We can tweak a few parameters in the decision tree algorithm before the actual learning takes place. \u003c/p\u003e\n\n\u003cp\u003eA decision tree, grown beyond a certain level of complexity leads to overfitting. If we grow our tree and carry on using poor predictors that don't have any impact on the accuracy, we will eventually a) slow down the learning, and b) cause overfitting.  Different tree pruning parameters can adjust the amount of overfitting or underfitting in order to optimize for increased accuracy, precision, and/or recall.\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThis process of trimming decision trees to optimize the learning process is called \"tree pruning\".\u003c/strong\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe can prune our trees using:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eMaximum depth: Reduce the depth of the tree to build a generalized tree. Set the depth of the tree to 3, 5, 10 depending after verification on test data\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum samples leaf with split: Restrict the size of sample leaf\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMinimum leaf sample size: Size in terminal nodes can be fixed to 30, 100, 300 or 5% of total\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum leaf nodes: Reduce the number of leaf nodes\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eMaximum features: Maximum number of features to consider when splitting a node\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet's look at a few hyperparameters and learn about their impact on classifier performance:  \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emax_depth\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe parameter for decision trees that we normally tune first is \u003ccode\u003emax_depth\u003c/code\u003e. This parameter indicates how deep we want our tree to be. If the tree is too deep, it means we are creating a large number of splits in the parameter space and capturing more information about underlying data. This may result in \u003cstrong\u003eoverfitting\u003c/strong\u003e as it will lead to learning granular information from given data, which makes it difficult for our model to generalize on unseen data. \nGenerally speaking, a low training error but a large testing error is a strong indication of this. \u003c/p\u003e\n\n\u003cp\u003eIf, on the other hand, the tree is too shallow, we may run into \u003cstrong\u003eunderfitting\u003c/strong\u003e, i.e., we are not learning enough information about the data and the accuracy of the model stays low for both the test and training samples. The following example shows the training and test AUC scores for a decision tree with depths ranging from 1 to 32.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/depth.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above example, we see that as the tree depth increases, our validation/test accuracy starts to go down after a depth of around 4. But with even greater depths, the training accuracy keeps on rising, as the classifier learns more information from the data. However this information can not be mapped onto unseen examples, hence the validation accuracy falls down constantly. Finding the sweet spot (e.g. depth = 4) in this case would be the first hyperparameter that we need to tune. \u003c/p\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_split\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThe hyperparameter \u003ccode\u003emin_samples_split\u003c/code\u003e is used to set the \u003cstrong\u003eminimum number of samples required to split an internal node\u003c/strong\u003e. This can vary between two extremes, i.e., considering only one sample at each node vs. considering all of the samples at each node - for a given attribute. \u003c/p\u003e\n\n\u003cp\u003eWhen we increase this parameter value, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/split.png\" width=\"500\"\u003e\u003c/p\u003e\n\n\u003cp\u003eIn the above plot, we see that the training and test accuracy stabilize at a certain minimum sample split size, and stays the same even if we carry on increasing the size of the split. This means that we will have a complex model, with similar accuracy than a much simpler model could potentially exhibit. Therefore, it is imperative that we try to identify the optimal sample size during the training phase. \u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: \u003ccode\u003emax_depth\u003c/code\u003e and \u003ccode\u003emin_samples_split\u003c/code\u003e are also both related to the computational cost involved with growing the tree. Large values for these parameters can create complex, dense, and long trees. For large datasets, it may become extremely time-consuming to use default values.  \u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2\u003e\u003ccode\u003emin_samples_leaf\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eThis hyperparameter is used to identify the minimum number of samples that we want a leaf node to contain. When this minimum size is achieved at a node, it does not get split any further.  This parameter is similar to \u003ccode\u003emin_samples_splits\u003c/code\u003e, however, this describes the minimum number of samples at the leaves, the base of the tree.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-tuning-decision-trees/master/images/leaf.png\" width=\"400\"\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above plot shows the impact of this parameter on the accuracy of the classifier. We see that increasing this parameter value after an optimal point reduces accuracy. That is due to underfitting again, as keeping too many samples in our leaf nodes means that there is still a high level of uncertainty in the data. \u003c/p\u003e\n\n\u003cp\u003eThe main difference between the two is that \u003ccode\u003emin_samples_leaf\u003c/code\u003e guarantees a minimum number of samples in a leaf, while \u003ccode\u003emin_samples_split\u003c/code\u003e can create arbitrary small leaves, though \u003ccode\u003emin_samples_split\u003c/code\u003e is more common in practice. These two hyperparameters make the distinction between a leaf (terminal/external node) and an internal node. An internal node will have further splits (also called children), while a leaf is by definition a node without any children (without any further splits).\u003c/p\u003e\n\n\u003cp\u003eFor instance, if \u003ccode\u003emin_samples_split = 5\u003c/code\u003e, and there are 7 samples at an internal node, then the split is allowed. But let's say the split results in two leaves, one with 1 sample, and another with 6 samples. If \u003ccode\u003emin_samples_leaf = 2\u003c/code\u003e, then the split won't be allowed (even if the internal node has 7 samples) because one of the leaves resulted will have less than the minimum number of samples required to be at a leaf node.\u003c/p\u003e\n\n\u003ch3\u003eAre there more hyperparameters?\u003c/h3\u003e\n\n\u003cp\u003eYes, there are! Scikit-learn offers a number of other hyperparameters for further fine-tuning the learning process. \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\"\u003eConsult the official doc\u003c/a\u003e to look at them in detail. The hyperparameters mentioned here are directly related to the complexity which may arise in decision trees and are normally tuned when growing trees. We'll shortly see this in action with a real dataset. \u003c/p\u003e\n\n\u003ch2\u003eAdditional Resources\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview\"\u003eOverview of hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f\"\u003eDemystifying hyperparameter tuning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.displayr.com/machine-learning-pruning-decision-trees/\"\u003ePruning decision trees\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we looked at the idea of optimizing hyperparameters and how pruning plays an important role in restricting the growth of a decision tree. We looked at a few hyperparameters which directly impact the potential overfitting/underfitting in trees. Next, we'll see these in practice using scikit-learn.   \u003c/p\u003e\n\u003cfooter class=\"fis-footer\" style=\"visibility: hidden;\"\u003e\u003cdiv class=\"fis-feedback\"\u003e\n\u003ch5\u003eHow do you feel about this lesson?\u003c/h5\u003e\n\u003cimg id=\"thumbs-up\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs up!\" alt=\"thumbs up\"\u003e\u003cimg id=\"thumbs-down\" data-repository=\"dsc-tuning-decision-trees\" title=\"Thumbs down!\" alt=\"thumbs down\"\u003e\n\u003c/div\u003e\n\u003ch5\u003eHave specific feedback? \u003ca href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees/issues/new\"\u003eTell us here!\u003c/a\u003e\n\u003c/h5\u003e\u003c/footer\u003e","frontPage":false}],"assignments":[{"exportId":"g381033ca9bd27a5adad694abdecab727","title":"A Deeper Dive into self","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-understanding-self\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-understanding-self/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eIn this lesson, you'll learn a little more about \u003ccode\u003eself\u003c/code\u003e in object-oriented programming (OOP) in Python. You've seen a little bit about \u003ccode\u003eself\u003c/code\u003e when you learned about defining and calling instance methods. So far you've seen that \u003ccode\u003eself\u003c/code\u003e is always explicitly defined as the instance method's \u003cstrong\u003efirst parameter\u003c/strong\u003e. You've also seen that instance methods implicitly use the instance object as the \u003cstrong\u003efirst argument\u003c/strong\u003e when you call the method. By convention, you name this first parameter \u003ccode\u003eself\u003c/code\u003e since it is a reference to the object on which you are operating. Let's take a look at some code that uses \u003ccode\u003eself\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExplain the \u003ccode\u003eself\u003c/code\u003e variable and its relation to instance objects\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf1d5c6a7ab52e0d588b4b7db4b395574","title":"Applying Gradient Descent - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-applying-gradient-descent-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson, we derived the functions that we help us descend along our cost functions efficiently.  Remember that this technique is not so different from what we saw with using the derivative to tell us our next step size and direction in two dimensions.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/master/images/slopes.png\" alt=\"RSS with changes to slope\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen descending along our cost curve in two dimensions, we used the slope of the tangent line at each point, to tell us how large of a step to take next.  And with the cost curve being a function of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e, we had to use the gradient to determine each step.  \u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-applying-gradient-descent-lab/master/images/new_gradientdescent.png\" alt=\"gradient descent in 3d with absolute minimum highlighted\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eBut really it's an analogous approach.  Just like we can calculate the use derivative of a function \u003cimg class=\"equation_image\" title=\"f(x)\" src=\"https://learning.flatironschool.com/equation_images/f(x)\" alt=\"{\" data-equation-content=\"f(x)\"\u003e to calculate the slope at a given value of \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e on the graph and thus our next step.  Here, we calculated the partial derivative with respect to both variables, our slope and y-intercept, to calculate the amount to move next in either direction and thus to steer us towards our minimum.   \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate functions to perform a simulation of gradient descent for an actual dataset\u003c/li\u003e\n\u003cli\u003eRepresent RSS as a multivariable function and take partial derivatives to perform gradient descent\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g24fef19b98005baa41a82dd41eea0b54","title":"Bias-Variance Tradeoff","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bias-variance-trade-off\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've seen how you can extend your linear models by including interaction effects as well as polynomial terms. Including these in models comes at a price though: not only do the models become more complex (with more parameter estimates), adding more terms can potentially harm model performance when making predictions. This tradeoff between performance on the training data and performance making predictions is called the bias-variance tradeoff. You'll learn about that in this lesson.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the bias-variance tradeoff in machine learning \u003c/li\u003e\n\u003cli\u003eDiscuss how bias and variance are related to over and underfitting \u003c/li\u003e\n\u003cli\u003eList the three components of error \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3b15a2f839ce8b5455258fa436cec3b0","title":"Bias-Variance Tradeoff - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-bias-variance-trade-off-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-bias-variance-trade-off-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice the concepts you learned in the last lesson, bias-variance tradeoff. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDemonstrate the tradeoff between bias and variance by way of fitting a machine learning model \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7df6fb0d0aa358bcfc133f47e6a121c8","title":"Building an Object-Oriented Simulation - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-object-oriented-simulation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3103e11723b224df8b8f7fa054ac87a5","title":"Building an SVM from Scratch - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-from-scratch-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g45fda3bf4a0608d78bd3d255c954a28b","title":"Building an SVM using scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-building-an-svm-using-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g322a58b3e3dc9b4bab6c5c9f989be7b6","title":"Building Trees using scikit-learn","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-decision-trees-with-sklearn-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-with-sklearn-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, we will cover decision trees (for classification) in Python, using scikit-learn and pandas. The emphasis will be on the basics and understanding the resulting decision tree. Scikit-learn provides a consistent interface for running different classifiers/regressors. For classification tasks, evaluation is performed using the same measures as we have seen before. Let's look at our example from earlier lessons and grow a tree to find our solution. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse scikit-learn to fit a decision tree classification model \u003c/li\u003e\n\u003cli\u003ePlot a decision tree using Python \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gabdf55215fd9529a86ae72cbec0a0c8f","title":"Building Trees using scikit-learn - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-decision-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eFollowing the simple example you saw in the previous lesson, you'll now build a decision tree for a more complex dataset. This lab covers all major areas of standard machine learning practice, from data acquisition to evaluation of results. We'll continue to use the Scikit-learn and Pandas libraries to conduct this analysis, following the same structure we saw in the previous lesson.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse scikit-learn to fit a decision tree classification model \u003c/li\u003e\n\u003cli\u003eUse entropy and information gain to identify the best attribute to split on at each node \u003c/li\u003e\n\u003cli\u003ePlot a decision tree using Python \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g62af0481da1247e246f90c684e1972d0","title":"Classes and Instances","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classes-and-instances\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll take a look at class and instance objects in Python and how to create them. A Python class can be thought of as the blueprint for creating a code object (or \u003cstrong\u003einstance object\u003c/strong\u003e). It has both the layout for new objects as well as the ability to create those objects. When you \u003cstrong\u003einitialize\u003c/strong\u003e or make a new instance object from a class, you are essentially pressing a button on an assembly line that instantly rolls out a new instance object. For example, if you were dealing with a \u003ccode\u003eCar\u003c/code\u003e class, you would get a brand new car from the assembly line. In cases where you want to create multiple objects, you can see how this functionality would be extremely useful.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe a class and how it can be used to create objects \u003c/li\u003e\n\u003cli\u003eDescribe an instance object \u003c/li\u003e\n\u003cli\u003eCreate an instance of a class \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g119766e414e353de35cb25c1e78483a1","title":"Classes and Instances - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-classes-and-instances-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-classes-and-instances-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eOkay, you've learned how to declare classes and create instances in the last lesson. Now it's time to put these new skills to the test!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate an instance of a class\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3a698bf676d43a6258d675063aefc220","title":"Class Imbalance Problems","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-class-imbalance-problems\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eYou've learned about precision, recall, accuracy, f1 score, ROC curves, and AUC as metrics for evaluating the performance of classifiers. With this, you've seen how measuring the performance of classification algorithms is substantially different from that of regression. For example, we briefly discussed a scenario where only 2 in 1000 cases were labeled 'positive'. In such drastically cases, even a naive classifier that simply always predicts a 'negative' label would be 99.8% accurate. Moreover, such scenarios are relatively common in areas such as medical conditions or credit card fraud. This is known as the 'class imbalance' problem. As such, there has been a lot of work and research regarding class imbalance problems and methods for tuning classification algorithms to better fit these scenarios.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe why class imbalance can lead to problems in machine learning\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eList the different methods of fixing class imbalance issues \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc7eb5852a1d8a6a65e79c5a0b1b487fa","title":"Class Imbalance Problems - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-class-imbalance-problems-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-class-imbalance-problems-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you've gone over some techniques for tuning classification models on imbalanced datasets, it's time to practice those techniques. In this lab, you'll investigate credit card fraud and attempt to tune a model to flag suspicious activity.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse sampling techniques to address a class imbalance problem within a dataset \u003c/li\u003e\n\u003cli\u003eCreate a visualization of ROC curves and use it to assess a model\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4819ae6efc6a7095e874f1dbb8a1ecb2","title":"Coding Logistic Regression From Scratch - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-coding-logistic-regression-from-scratch\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-coding-logistic-regression-from-scratch/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBuild a logistic regression model from scratch using gradient descent \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gee82ca01b6cbad81fe60f2bfd74f5c6f","title":"Confusion Matrices","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-confusion-matrices\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-confusion-matrices/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn how to construct and interpret a \u003cstrong\u003e\u003cem\u003eConfusion Matrix\u003c/em\u003e\u003c/strong\u003e to evaluate the performance of a classifier!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the four quadrants of a confusion matrix \u003c/li\u003e\n\u003cli\u003eInterpret a confusion matrix\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCreate a confusion matrix using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g39a0387412e39beed4be8f195a098689","title":"Decision Trees Checkpoint","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc0e76e1a6e29d946d6f18e9aacad1c3d","title":"Derivatives: Conclusion","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-conclusion\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-conclusion/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eData science is all about finding good models to understand patterns in your data. You'll find yourself performing optimizations all the time. Examples are: maximizing model likelihoods and minimizing errors. Essentially, you'll perform a lot of minimizations and maximizations along the way when creating machine learning models. This is where derivatives come in very handy!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe how minima and maxima are related to machine learning and optimization\u003c/li\u003e\n\u003cli\u003eCalculate minima and maxima mathematically\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3096b7ec24bf7a02135c7a720a776e2d","title":"Derivatives of Non-Linear Functions","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-of-non-linear-functions\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-of-non-linear-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson, we saw that the derivative was the rate of change and that the derivative of a straight line is a constant. Let's explore non-linear functions and their derivatives in this lesson!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate the derivative of a non-linear function\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6a9260d7fd3edc533d3b4ae5aabcd3e6","title":"Derivatives: the Chain Rule","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-chain-rule/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g61391248a475ba95d6c3011ff7a40b23","title":"Distance Metrics","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-distance-metrics\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about various kinds of distance metrics that you can use as a way to quantify similarity!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate Manhattan distance between two points\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCalculate Euclidean distance between two points\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eCompare and contrast Manhattan, Euclidean, and Minkowski distance \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga1388e96c2ed6de2dfc2332638720aeb","title":"Distance Metrics - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-distance-metrics-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-distance-metrics-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll calculate various distances between multiple points using the distance metrics you learned about!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate Manhattan distance between two points \u003c/li\u003e\n\u003cli\u003eCalculate Euclidean distance between two points\u003c/li\u003e\n\u003cli\u003eCalculate Minkowski distance between two points\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g236db4ca49d5baf1e702753a44377c26","title":"Document Classification with Naive Bayes","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-document-classification-with-naive-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll investigate another implementation of the Bayesian framework in order to classify YouTube videos into the appropriate topic. The dataset you'll be investigating again comes from Kaggle. For further information, you can check out the original dataset here: \u003ca href=\"https://www.kaggle.com/extralime/math-lectures\"\u003ehttps://www.kaggle.com/extralime/math-lectures\u003c/a\u003e .\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement document classification using Naive Bayes \u003c/li\u003e\n\u003cli\u003eExplain how to code a bag of words representation\u003c/li\u003e\n\u003cli\u003eExplain why it is necessary to use Laplacian smoothing correction\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gad6d501b5a276265442395228b9f671b","title":"Document Classification with Naive Bayes - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-document-classification-with-naive-bayes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-document-classification-with-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll practice implementing the Naive Bayes algorithm on your own.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement document classification using Naive Bayes\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g163b0dfb05702dcac7b6cd220709bd40","title":"Evaluating Logistic Regression Models - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-evaluating-logistic-regression-models-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-evaluating-logistic-regression-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn regression, you are predicting continous values so it makes sense to discuss error as a distance of how far off our estimates were. When classifying a binary variable, however, a model is either correct or incorrect. As a result, we tend to quantify this in terms of how many false positives versus false negatives we come across. In particular, we examine a few different specific measurements when evaluating the performance of a classification algorithm. In this lab, you'll review precision, recall, accuracy, and F1 score in order to evaluate our logistic regression models.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement evaluation metrics from scratch using Python \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf751cd1332473f2e094bab2de0075b78","title":"Extensions to Linear Models - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-extensions-to-linear-models-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-extensions-to-linear-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice many concepts you have learned so far, from adding interactions and polynomials to your model to regularization!\u003c/p\u003e\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eBuild a linear regression model with interactions and polynomial features \u003c/li\u003e\n\u003cli\u003eUse feature selection to obtain the optimal subset of features in a dataset\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf5dc93ee053ea7f248ee15a7b680c8fe","title":"Feature Selection Methods","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-feature-selection-methods\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-feature-selection-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about the different techniques you can use to only use features that are most relevant to your model.\u003c/p\u003e\n\n\u003ch3\u003eObjectives\u003c/h3\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse feature selection to obtain the optimal subset of features in a dataset \u003c/li\u003e\n\u003cli\u003eIdentify when it is appropriate to use certain methods of feature selection \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g70829cf72bde217bb84b7b85a704288d","title":"Fitting a Logistic Regression Model - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-fitting-a-logistic-regression-model-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-fitting-a-logistic-regression-model-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson you were given a broad overview of logistic regression. This included an introduction to two separate packages for creating logistic regression models. In this lab, you'll be investigating fitting logistic regressions with \u003ccode\u003estatsmodels\u003c/code\u003e. For your first foray into logistic regression, you are going to attempt to build a model that classifies whether an individual survived the \u003ca href=\"https://www.kaggle.com/c/titanic/data\"\u003eTitanic\u003c/a\u003e shipwreck or not (yes, it's a bit morbid).\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement logistic regression with \u003ccode\u003estatsmodels\u003c/code\u003e \u003c/li\u003e\n\u003cli\u003eInterpret the statistical results associated with model parameters\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc08f06121b1cb6900720308cdce032e8","title":"Gaussian Naive Bayes","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gaussian-naive-bayes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eExpanding Bayes theorem to account for multiple observations and conditional probabilities drastically increases predictive power. In essence, it allows you to develop a belief network taking into account all of the available information regarding the scenario. In this lesson, you'll take a look at one particular implementation of a multinomial naive Bayes algorithm: Gaussian Naive Bayes.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain the Gaussian Naive Bayes algorithm\u003c/li\u003e\n\u003cli\u003eImplement the Gaussian Naive Bayes (GNB) algorithm using SciPy and NumPy\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb77f66d2830c911912d6ef073be94510","title":"Gaussian Naive Bayes - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gaussian-naive-bayes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gaussian-naive-bayes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you've seen how to employ multinomial Bayes for classification, its time to practice implementing the process yourself. You'll also get a chance to investigate the impacts of using true probabilities under the probability density function as opposed to the point estimate on the curve itself.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eIndependently code and implement the Gaussian Naive Bayes algorithm\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g59ccab92e9380e16e1a668e43a6307ec","title":"Gradient Boosting - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-boosting-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-boosting-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll learn how to use both Adaboost and Gradient Boosting classifiers from scikit-learn!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse AdaBoost to make predictions on a dataset \u003c/li\u003e\n\u003cli\u003eUse Gradient Boosting to make predictions on a dataset \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3929ccf412b073ec731206e2dce84453","title":"Gradient Descent in 3D","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-in-3d\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-in-3d/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003ePreviously, we talked about how to think about gradient descent when moving along a 3D cost curve.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-gradient-descent-in-3d/master/images/new_gradientdescent.png\" width=\"600\"\u003e\u003c/p\u003e\n\n\u003cp\u003eWe know that moving along the 3D cost curve above means changing the \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e variables of a regression line like the one below.  And we do so with the purpose of having our line better match our data. In this section, you'll learn about \u003cem\u003epartial derivatives\u003c/em\u003e which will make you achieve this. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine a partial derivative\u003c/li\u003e\n\u003cli\u003eInterpret visual representations of gradient descent in more than two dimensions\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gd4dcde7ebdef12668ae7dbb3f568182b","title":"Gradient Descent - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll continue to formalize your knowledge of gradient descent by coding the algorithm yourself. In the upcoming labs, you'll apply similar procedures to implement logistic regression on your own.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement gradient descent from scratch to minimize OLS\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g508c94e5f4410b774066661cb6893554","title":"Gradient Descent: Step Sizes","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-step-sizes\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last section, you took a first look at the process for improving regression lines.  You began with some data then used a simple regression line in the form \u003cimg class=\"equation_image\" title=\"\\hat{y}= mx + b \" src=\"https://learning.flatironschool.com/equation_images/%255Chat%7By%7D=%20mx%20+%20b\" alt=\"{\" data-equation-content=\"\\hat{y}= mx + b \"\u003e to predict an output, given an input.  Finally, you measured the accuracy of your regression line by calculating the differences between the outputs predicted by the regression line and the actual values. In this lesson, you'll look at how we can make your approach more efficient.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eDefine step sizes in the context of gradient descent\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eDefine a learning rate, and its relationship to step size when performing gradient descent\u003c/p\u003e\u003c/li\u003e\n\u003cli\u003e\u003cp\u003ePlot visualizations of the process of gradient descent\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gafebbe6171e387ed7e205bba80badcc0","title":"Gradient Descent: Step Sizes - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-step-sizes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-step-sizes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice applying gradient descent.  As you know, gradient descent begins with an initial regression line and moves to a \"best fit\" regression line by changing values of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e and evaluating the RSS.  So far, we have illustrated this technique by changing the values of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and evaluating the RSS.  In this lab, you will work through applying this technique by changing the value of \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e instead.  Let's get started.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse gradient descent to find the optimal parameters for a linear regression model\u003c/li\u003e\n\u003cli\u003eDescribe how to use an RSS curve to find the optimal parameters for a linear regression model\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc5264c4c0c07fc228d3a316350dc8504","title":"Gradient to Cost Function","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-to-cost-function-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-to-cost-function-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous lesson, we learned the mathematical definition of a gradient.  We saw that the gradient of a function was a combination of our partial derivatives with respect to each variable of that function.  We saw the direction of gradient descent was simply to move in the negative direction of the gradient.  For example, if the direction of ascent of a function is a move up and to the right, the descent is down and to the left. In this lesson, we will apply gradient descent to our cost function to see how we can move towards a best fit regression line by changing variables of \u003cimg class=\"equation_image\" title=\"m\" src=\"https://learning.flatironschool.com/equation_images/m\" alt=\"{\" data-equation-content=\"m\"\u003e and \u003cimg class=\"equation_image\" title=\"b\" src=\"https://learning.flatironschool.com/equation_images/b\" alt=\"{\" data-equation-content=\"b\"\u003e.  \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRepresent RSS as a multivariable function and take partial derivatives to perform gradient descent\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g24774d5f3764ee7c2cc27699b8e85222","title":"GridSearchCV - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gridsearchcv-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gridsearchcv-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll explore how to use scikit-learn's \u003ccode\u003eGridSearchCV\u003c/code\u003e class to exhaustively search through every combination of hyperparameters until we find optimal values for a given model.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDesign a parameter grid for use with scikit-learn's GridSearchCV \u003c/li\u003e\n\u003cli\u003eUse GridSearchCV to increase model performance through parameter tuning \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gceceeb05c5feb34554c4ac6d68cf5757","title":"Hyperparameter Tuning and Pruning in Decision Trees - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-decision-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-decision-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you will use the titanic dataset to see the impact of tree pruning and hyperparameter tuning on the predictive performance of a decision tree classifier. Pruning reduces the size of decision trees by removing nodes of the tree that do not provide much predictive power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDetermine the optimal hyperparameters for a decision tree model and evaluate the model performance\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gbfbf401b01ebcd3a4fb3a809ecca4366","title":"ID3 Classification Trees: Perfect Split with Information Gain - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ID3-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ID3-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we will simulate the example from the previous lesson in Python. You will write functions to calculate entropy and IG which will be used for calculating these uncertainty measures and deciding upon creating a split using information gain while growing an ID3 classification tree. You will also write a general function that can be used for other (larger) problems as well. So let's get on with it.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eWrite functions for calculating entropy and information gain measures\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eUse entropy and information gain to identify the attribute that results in the best split at each node\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge5a4685436b8c307dbd0e02842aa8bb3","title":"Inheritance","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-inheritance\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn about how you can use inheritance to create relationships between \u003cstrong\u003e\u003cem\u003eSuperclasses\u003c/em\u003e\u003c/strong\u003e and \u003cstrong\u003e\u003cem\u003eSubclasses\u003c/em\u003e\u003c/strong\u003e to further save you from writing redundant code!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a domain model using OOP \u003c/li\u003e\n\u003cli\u003eUse inheritance to write nonredundant code \u003c/li\u003e\n\u003cli\u003eDescribe the relationship between subclasses and superclasses \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga7235058a8fe896a98b13eac29d0a628","title":"Inheritance - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-inheritance-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-inheritance-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll use what you've learned about inheritance to model a zoo using superclasses, subclasses, and maybe even an abstract superclass!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a domain model using OOP \u003c/li\u003e\n\u003cli\u003eUse inheritance to write nonredundant code \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g69af0808045f764ffe43b978fa3ccaf4","title":"Instance Methods","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-instance-methods\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you know what classes and instances are, we can talk about instance methods. Instance methods are almost the same as regular functions in Python. The key difference is that an instance method is defined inside of a class and bound to instance objects of that class. Instance methods can be thought of as an attribute of an instance object. The difference between an instance method and another attribute of an instance is that instance methods are \u003cem\u003ecallable\u003c/em\u003e, meaning they execute a block of code. This may seem a bit confusing, but try to think about instance methods as functions defined in a class that are really just attributes of an instance object from that class.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare instance methods and attributes\u003c/li\u003e\n\u003cli\u003eDefine and call an instance method\u003c/li\u003e\n\u003cli\u003eDefine instance attributes\u003c/li\u003e\n\u003cli\u003eExplain the \u003ccode\u003eself\u003c/code\u003e variable and its relation to instance objects\u003c/li\u003e\n\u003cli\u003eCreate an instance of a class \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gaf65bb50e8026994e8ca240f8354c3d2","title":"Instance Methods - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-instance-methods-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-instance-methods-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the last lesson, you learned about instance methods -- what they are and how to define them. In this lab, you are going to flesh out the \u003ccode\u003eDriver\u003c/code\u003e and \u003ccode\u003ePassenger\u003c/code\u003e classes by writing your own instance methods for these classes.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate an instance of a class \u003c/li\u003e\n\u003cli\u003eDefine and call an instance method\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf75f69ba85f77765f91d254d7d93222e","title":"Introduction to Cross-Validation","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-cross-validation\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eCross-validation is another model validation strategy, which addresses one of the limitations of the train-test split strategy.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the process of cross-validation\u003c/li\u003e\n\u003cli\u003ePerform cross-validation on a model\u003c/li\u003e\n\u003cli\u003eCompare and contrast model validation strategies\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g5c9102ed5e9c336f43723273d0ebd59b","title":"Introduction to Cross-Validation - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-cross-validation-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-cross-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll be able to practice your cross-validation skills!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform cross validation on a model\u003c/li\u003e\n\u003cli\u003eCompare and contrast model validation strategies\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9777bfbacb7532d96452a0bc04a904b5","title":"Introduction to Derivatives","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-intro\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the linear regression section, you learned about the basic notion of mathematical functions. Now, imagine that you used the number of bedrooms as a predictor and house rental price as the target variable, you can formulate this as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\text{price} = f(\\text{number of bedrooms})\" src=\"https://learning.flatironschool.com/equation_images/%255Ctext%7Bprice%7D%20=%20f(%255Ctext%7Bnumber%20of%20bedrooms%7D)\" alt=\"{\" data-equation-content=\"\\text{price} = f(\\text{number of bedrooms})\"\u003e\u003c/p\u003e or, alternatively\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\" y = f(\\text{x})\" src=\"https://learning.flatironschool.com/equation_images/%20y%20=%20f(%255Ctext%7Bx%7D)\" alt=\"{\" data-equation-content=\" y = f(\\text{x})\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eNow let's say the price of the apartment is set in a very simplified way, and there is a perfectly linear relationship between the apartment size and the rental price. Say that the price goes up by 500 USD/month for every bedroom an apartment has. In that case, we can express the price as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg class=\"equation_image\" title=\"\\text{price} = 500 * \\text{number of bedrooms}\" src=\"https://learning.flatironschool.com/equation_images/%255Ctext%7Bprice%7D%20=%20500%20*%20%255Ctext%7Bnumber%20of%20bedrooms%7D\" alt=\"{\" data-equation-content=\"\\text{price} = 500 * \\text{number of bedrooms}\"\u003e\u003c/p\u003e or \u003cp\u003e\u003cimg class=\"equation_image\" title=\"y = f(x) = 500 * x = 500x\" src=\"https://learning.flatironschool.com/equation_images/y%20=%20f(x)%20=%20500%20*%20x%20=%20500x\" alt=\"{\" data-equation-content=\"y = f(x) = 500 * x = 500x\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003c/p\u003e\n\n\u003cp\u003eNote that there is no intercept here! Now, we want to dive deeper into how the rental price changes as the number of bedrooms changes. This is what derivatives are all about!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what a derivative means in the context of a real-world example\u003c/li\u003e\n\u003cli\u003eCalculate the derivative of a linear function\u003c/li\u003e\n\u003cli\u003eDefine derivatives as the instantaneous rate of change of a function\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ga3c10c23d3d9e1151dccb8cb9437a1f4","title":"Introduction to Derivatives - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-derivatives-intro-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-derivatives-intro-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we will practice our knowledge of derivatives. Remember that our key formula for derivatives, is \n\u003cimg class=\"equation_image\" title=\"f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}\" src=\"https://learning.flatironschool.com/equation_images/f'(x)%20=%20%255Cdfrac%7B%255CDelta%20y%7D%7B%255CDelta%20x%7D%20=%20%20%255Cdfrac%7Bf(x%20+%20%255CDelta%20x)%20-%20f(x)%7D%7B%255CDelta%20x%7D\" alt=\"{\" data-equation-content=\"f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}\"\u003e.  So in driving towards this formula, we will do the following: \u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eLearn how to represent linear and nonlinear functions in code \u003c/li\u003e\n\u003cli\u003eThen, because our calculation of a derivative relies on seeing the output at an initial value and the output at that value plus \u003cimg class=\"equation_image\" title=\"\\Delta x\" src=\"https://learning.flatironschool.com/equation_images/%255CDelta%20x\" alt=\"{\" data-equation-content=\"\\Delta x\"\u003e, we need an \u003ccode\u003eoutput_at\u003c/code\u003e function\u003c/li\u003e\n\u003cli\u003eThen we will be able to code the \u003cimg class=\"equation_image\" title=\"\\Delta f\" src=\"https://learning.flatironschool.com/equation_images/%255CDelta%20f\" alt=\"{\" data-equation-content=\"\\Delta f\"\u003e function that sees the change in output between the initial \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e and that initial \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e plus the \u003cimg class=\"equation_image\" title=\"\\Delta x\" src=\"https://learning.flatironschool.com/equation_images/%255CDelta%20x\" alt=\"{\" data-equation-content=\"\\Delta x\"\u003e \u003c/li\u003e\n\u003cli\u003eFinally, we will calculate the derivative at a given \u003cimg class=\"equation_image\" title=\"x\" src=\"https://learning.flatironschool.com/equation_images/x\" alt=\"{\" data-equation-content=\"x\"\u003e value, \u003ccode\u003ederivative_at\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse python functions to demonstrate derivatives of functions\u003c/li\u003e\n\u003cli\u003eDescribe what a derivative means in the context of a real-world example\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0eaaa285661d545beda76b6a1d97b302","title":"Introduction to Gradient Descent","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-gradient-descent-intro\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-gradient-descent-intro/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIt's possible to solve for the optimal values of a regression using closed-form Ordinary Least Squares programming when there are a limited number of features, but this process might become computationally expensive when there are many features. Therefore, iterative algorithms like the \u003cem\u003egradient descent\u003c/em\u003e algorithm are the basis of many models in statistics and machine learning!\u003c/p\u003e\n\n\u003cp\u003eYou previously saw how after choosing the slope and y-intercept values of a regression line, we can calculate the residual sum of squares (RSS) and related root mean squared error. We can use either the RSS or RMSE to calculate the accuracy of a line. In this lesson, we'll use the RSS to iteratively find the best fit line for our problem at hand!\u003c/p\u003e\n\n\u003cp\u003eOnce calculating the accuracy of a line, we are pretty close to improving upon a line by minimizing the RSS.  This is the task of the gradient descent technique.  But before learning about gradient descent, let's review and ensure that we understand how to evaluate how our line fits our data.  \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe what gradient descent is, and its relationship to minima\u003c/li\u003e\n\u003cli\u003eDescribe a cost curve and what it means to move along it\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9e64eee30e747fc4c6498bd573227ec2","title":"Kernels in scikit-learn - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-kernels-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g96f15a5a4928ab60f394fc9eedfc023d","title":"K-Nearest Neighbors - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-k-nearest-neighbors-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-k-nearest-neighbors-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll build a simple version of a \u003cstrong\u003e\u003cem\u003eK-Nearest Neigbors classifier\u003c/em\u003e\u003c/strong\u003e from scratch, and train it to make predictions on a dataset!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eImplement a basic KNN algorithm from scratch\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g60e8d59944503f2fff50b17ed9050ce2","title":"KNN with scikit-learn - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-knn-with-scikit-learn-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-knn-with-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll learn how to use scikit-learn's implementation of a KNN classifier on the classic Titanic dataset from Kaggle!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eConduct a parameter search to find the optimal value for K \u003c/li\u003e\n\u003cli\u003eUse a KNN classifier to generate predictions on a real-world dataset \u003c/li\u003e\n\u003cli\u003eEvaluate the performance of a KNN model\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge143c152064afbb74f901a0771d431e0","title":"Linear to Logistic regression","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linear-to-logistic-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linear-to-logistic-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll be introduced to the logistic regression model. You'll start with an introductory example using linear regression, which you've seen before, to act as a segue into logistic regression. After that, you'll learn about the formal notation of logistic regression models. Then, you'll conclude this lesson by looking at a real-world example.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the need for logistic regression\u003c/li\u003e\n\u003cli\u003eInterpret the parameters of a logistic regression model\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7f84091a5e76258e0ccb89ca7bd645d5","title":"Logistic Regression Checkpoint","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb8692713ef61b8e0a83cffd5b60950ae","title":"Logistic Regression in scikit-learn","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-in-scikit-learn\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eGenerally, the process for fitting a logistic regression model using scikit-learn is very similar to that which you previously saw for \u003ccode\u003estatsmodels\u003c/code\u003e. One important exception is that scikit-learn will not display statistical measures such as the p-values associated with the various features. This is a shortcoming of scikit-learn, although scikit-learn has other useful tools for tuning models which we will investigate in future lessons.\u003c/p\u003e\n\n\u003cp\u003eThe other main process of model building and evaluation which we didn't discuss previously is performing a train-test split. As we saw in linear regression, model validation is an essential part of model building as it helps determine how our model will generalize to future unseen cases. After all, the point of any model is to provide future predictions where we don't already know the answer but have other informative data (\u003ccode\u003eX\u003c/code\u003e).\u003c/p\u003e\n\n\u003cp\u003eWith that, let's take a look at implementing logistic regression in scikit-learn using dummy variables and a proper train-test split.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFit a logistic regression model using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb8cdf52053921f13266a12eab476e9eb","title":"Logistic Regression in scikit-learn - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-in-scikit-learn-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-in-scikit-learn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you are going to fit a logistic regression model to a dataset concerning heart disease. Whether or not a patient has heart disease is indicated in the column labeled \u003ccode\u003e'target'\u003c/code\u003e. 1 is for positive for heart disease while 0 indicates no heart disease.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFit a logistic regression model using scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge33ea8e6409baf98644fba31c69392ac","title":"Logistic Regression Model Comparisons - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-logistic-regression-model-comparisons-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-model-comparisons-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll further investigate how to tune your own logistic regression implementation, as well as that of scikit-learn in order to produce better models.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare the different inputs with logistic regression models and determine the optimal model \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g75744bd363ff7dae6e80149164e7db83","title":"Machine Learning Fundamentals Checkpoint","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1f9f8e83538aa1ccd15d2d9b0de0394c","title":"Matrix Multiplication - Code Along","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linalg-mat-multiplication-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-mat-multiplication-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eUnderstanding matrix operations is very important for a deeper understanding of linear algebra. We know matrices are used throughout the field of machine learning in the description of algorithms and representation of data. In this lesson, we shall discover how to manipulate matrices in Python and Numpy.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompute the dot product for matrices and vectors \u003c/li\u003e\n\u003cli\u003eCalculate a cross product using Numpy \u003c/li\u003e\n\u003cli\u003eDefine a cross product\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfbd4429ffbc0336acf92ade5373a1701","title":"Object Attributes - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-object-attributes-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-attributes-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice defining classes and instance methods. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine and call an instance method\u003c/li\u003e\n\u003cli\u003eDefine and access instance attributes\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4d076a4b2a78d13ff48248cac54840a5","title":"Object Initialization","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-object-initialization\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you've begun to see OOP and class structures, it's time to investigate the \u003ccode\u003e__init__\u003c/code\u003e method more. The \u003ccode\u003e__init__\u003c/code\u003e method allows classes to have default behaviors and attributes. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate instance variables in the \u003ccode\u003e__init__\u003c/code\u003e method\u003c/li\u003e\n\u003cli\u003eUse default arguments in the \u003ccode\u003e__init__\u003c/code\u003e method \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge3bb362ff99b1f27ecce7f46eab3b72c","title":"Object Initialization - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-object-initialization-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-initialization-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice defining classes with \u003ccode\u003e__init__\u003c/code\u003e methods. You'll define two classes, \u003ccode\u003eDriver\u003c/code\u003e and \u003ccode\u003ePassenger\u003c/code\u003e in the cells below. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate instance variables in the \u003ccode\u003e__init__\u003c/code\u003e method\u003c/li\u003e\n\u003cli\u003eUse default arguments in the \u003ccode\u003e__init__\u003c/code\u003e method\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb9ac29340ae2e7a63aeadabeacc31527","title":"Object Oriented Attributes with Functions","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb22c89bceacbf963e06408ab0cbd3424","title":"Object Oriented Attributes With Functions - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-attributes-with-functions-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g620f162c9e68c4adb9e507b7e852183f","title":"Object-Oriented Programming Checkpoint","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1755c0c703e3acfea9f4882b8959a352","title":"Object Oriented Shopping Cart - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-object-oriented-shopping-cart-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf90dbcad5fd270a68f87e199399d7cf4","title":"OOP with Scikit-Learn","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-oop-sklearn\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs you learn more about machine learning algorithms, there are typically two components. First, the conceptual underlying logic of the algorithm -- how it works to process inputs and generate outputs. Second, the scikit-learn implementation of the algorithm -- how to use it in practice.\u003c/p\u003e\n\n\u003cp\u003eBefore diving into specific examples of various scikit-learn models, it is helpful to understand the general structure they follow. Specifically, we'll go over some key classes, methods, and attributes common to scikit-learn.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecall the distinction between mutable and immutable types\u003c/li\u003e\n\u003cli\u003eDefine the four main inherited object types in scikit-learn\u003c/li\u003e\n\u003cli\u003eInstantiate scikit-learn transformers and models\u003c/li\u003e\n\u003cli\u003eInvoke scikit-learn methods\u003c/li\u003e\n\u003cli\u003eAccess scikit-learn attributes\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc6044b3c144d2460d591e93a47b50523","title":"OOP with Scikit-Learn - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-oop-sklearn-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-oop-sklearn-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have learned some of the basics of object-oriented programming with scikit-learn, let's practice applying it!\u003c/p\u003e\n\n\u003ch2\u003eObjectives:\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you will practice:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecall the distinction between mutable and immutable types\u003c/li\u003e\n\u003cli\u003eDefine the four main inherited object types in scikit-learn\u003c/li\u003e\n\u003cli\u003eInstantiate scikit-learn transformers and models\u003c/li\u003e\n\u003cli\u003eInvoke scikit-learn methods\u003c/li\u003e\n\u003cli\u003eAccess scikit-learn attributes\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gaad3ce28f6a3f4a5c4b8da821048c163","title":"Phase 3 Blog Post","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Blog Post here. \u003c/span\u003e\u003cspan\u003eRefer to the \u003c/span\u003e\u003ca title=\"Blogging Overview\" href=\"pages/blogging-overview\"\u003eBlogging Overview\u003c/a\u003e\u003cspan\u003e to learn about how to write good blog posts that\u003c/span\u003e\u003cspan style=\"font-family: inherit; font-size: 1rem;\"\u003e meet Flatiron School‚Äôs requirements.\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g88357763d5f22d659df758ff649797ea","title":"Phase 3 Code Challenge","type":"Assignment","content":"","submissionTypes":"an external tool","graded":true,"pointsPossible":16.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb72bb79ecbf4e6a1d3f538f126ac6d8c","title":"Phase 3 Project - GitHub Repository URL","type":"Assignment","content":"\u003cp\u003e\u003cspan\u003ePlease put the URL to your Phase 3 Project GitHub Repository here.\u0026nbsp;\u003c/span\u003e\u003c/p\u003e","submissionTypes":"a website url","graded":true,"pointsPossible":0.0,"dueAt":"2022-10-07T23:59:59-04:00","lockAt":null,"unlockAt":null},{"exportId":"g0576788fe1f7929f3f8b2444c8156ef3","title":"Pickle","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pickle\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickle/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003ePickle is an invaluable tool for saving objects.  In this lesson you will learn how to use it on various different Python data types.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDescribe the circumstances in which you would want to use a pickle file\u003c/li\u003e\n\u003cli\u003eWrite a pickle file\u003c/li\u003e\n\u003cli\u003eRead a pickle file\u003c/li\u003e\n\u003cli\u003eUse the \u003ccode\u003ejoblib\u003c/code\u003e library to pickle and load a scikit-learn class\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8492efd41e732bf37959b70989a9dcb6","title":"Pickling and Deploying Pipelines","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pickling-pipelines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickling-pipelines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pickling-pipelines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow that you have learned about scikit-learn pipelines and model pickling, it's time to bring it all together in a professional ML workflow!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUnderstand the purpose of deploying a machine learning model\u003c/li\u003e\n\u003cli\u003eUnderstand the cloud function approach to model deployment\u003c/li\u003e\n\u003cli\u003ePickle a scikit-learn pipeline\u003c/li\u003e\n\u003cli\u003eCreate a cloud function\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9460c3b0812bc3c864b5800d801a5245","title":"Pipelines in scikit-learn - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-pipelines-lab-v2-1\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-pipelines-lab-v2-1/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you will work with the \u003ca href=\"https://archive.ics.uci.edu/ml/datasets/wine+quality\"\u003eWine Quality Dataset\u003c/a\u003e. The goal of this lab is not to teach you a new classifier or even show you how to improve the performance of your existing model, but rather to help you streamline your machine learning workflows using scikit-learn pipelines. Pipelines let you keep your preprocessing and model building steps together, thus simplifying your cognitive load. You will see for yourself why pipelines are great by building the same KNN model twice in different ways. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cul\u003e\n\u003cli\u003eConstruct pipelines in scikit-learn \u003c/li\u003e\n\u003cli\u003eUse pipelines in combination with \u003ccode\u003eGridSearchCV()\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfd090dd8ae4044fef856f70185578e8f","title":"Properties of Dot Product - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-dot-product-properties-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfd0d759c515d090524907fb11efc4ad1","title":"Pure Python vs. Numpy - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-python-vs-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb884ce65901c5bbc5ad0879aa9be7100","title":"Refactoring Your Code to Use Pipelines","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-refactoring-with-pipelines\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-refactoring-with-pipelines\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-refactoring-with-pipelines/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you will learn how to use the core features of scikit-learn pipelines to refactor existing machine learning preprocessing code into a portable pipeline format.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eRecall the benefits of using pipelines\u003c/li\u003e\n\u003cli\u003eDescribe the difference between a \u003ccode\u003ePipeline\u003c/code\u003e, a \u003ccode\u003eFeatureUnion\u003c/code\u003e, and a \u003ccode\u003eColumnTransformer\u003c/code\u003e in scikit-learn\u003c/li\u003e\n\u003cli\u003eIteratively refactor existing preprocessing code into a pipeline\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gf03dc6212b7f20a3571de75b90fb5ccc","title":"Regression Analysis using Linear Algebra and NumPy - Code Along","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linalg-regression-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous sections, you learned that in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between data entities (variables). Linear regression is an important predictive analytical tool in the data scientist's toolbox. Here, you'll try and develop a basic intuition for regression from a linear algebra perspective using vectors and matrix operations. This lesson covers least-squares regression with matrix algebra without digging deep into the geometric dimensions. \u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"http://math.mit.edu/%7Egs/linearalgebra/ila0403.pdf\"\u003eYou can find a deeper mathematical and geometric explanation of the topic here\u003c/a\u003e. In this lesson, we'll try to keep things more data-oriented.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eApply linear algebra to fit a function to data, describing linear mappings between input and output variables\u003c/li\u003e\n\u003cli\u003eIndicate how linear algebra is related to regression modeling\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7481fbdcbe748b61b8266678416afd43","title":"Regression Model Validation","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-model-validation\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003ePreviously you've evaluated a multiple linear regression model by calculating metrics based on the fit of the training data. In this lesson you'll learn why it's important to split your data in a train and a test set if you want to evaluate a model used for prediction.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform a train-test split\u003c/li\u003e\n\u003cli\u003ePrepare training and testing data for modeling\u003c/li\u003e\n\u003cli\u003eCompare training and testing errors to determine if model is over or underfitting\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8878f371f80e2be164db9ca56270ac6f","title":"Regression Model Validation - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-model-validation-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-model-validation-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll be able to validate your Ames Housing data model using a train-test split.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform a train-test split\u003c/li\u003e\n\u003cli\u003ePrepare training and testing data for modeling\u003c/li\u003e\n\u003cli\u003eCompare training and testing errors to determine if model is over or underfitting\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfb58ddbd96ed5ad6bfe046755028f202","title":"Regression Trees and Model Optimization - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tuning-regression-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tuning-regression-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll see how to apply regression analysis using CART trees while making use of some hyperparameter tuning to improve our model. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003ePerform the full process of cleaning data, tuning hyperparameters, creating visualizations, and evaluating decision tree models \u003c/li\u003e\n\u003cli\u003eDetermine the optimal hyperparameters for a decision tree model and evaluate the performance of decision tree models\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g3bef8e1ab0e4730537fdf1fb5e809498","title":"Regression with CART Trees","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-cart-trees-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAs we've learned, a decision tree is a supervised machine learning model that can be used both for classification and regression tasks. We have seen that a decision tree uses a tree structure to predict an output class for a given input example in a classification task. For regression analysis, each path in the tree from the root node to a leaf node represents a decision path that ends in a predicted value. In this lesson, we shall see how regression is performed in using a decision tree regressor using a simple example.  \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Kindly visit the \u003ca href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\"\u003eofficial documentation\u003c/a\u003e for the regressor tree function used in this lesson. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eExplain recursive partitioning \u003c/li\u003e\n\u003cli\u003eFit a decision tree regression model with scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g79619abf81fde3603d82f5fc24d233fe","title":"Regression with CART Trees - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-regression-cart-trees-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-regression-cart-trees-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll make use of what we learned in the previous lesson to build a model for the \u003ca href=\"https://www.kaggle.com/harinir/petrol-consumption\"\u003ePetrol Consumption Dataset\u003c/a\u003e from Kaggle. This model will be used to predict gasoline consumption for a bunch of examples, based on features about the drivers.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eFit a decision tree regression model with scikit-learn\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gce6931adc68361b10a894c60cafbf6f7","title":"Regression with Linear Algebra - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-linalg-regression-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll apply regression analysis using simple matrix manipulations to fit a model to given data, and then predict new values for previously unseen data. You'll follow the approach highlighted in the previous lesson where you used NumPy to build the appropriate matrices and vectors and solve for the \u003cimg class=\"equation_image\" title=\"\\beta\" src=\"https://learning.flatironschool.com/equation_images/%255Cbeta\" alt=\"{\" data-equation-content=\"\\beta\"\u003e (unknown variables) vector. The beta vector will be used with test data to make new predictions. You'll also evaluate the model fit.\nIn order to make this experiment interesting, you'll use NumPy at every single stage of this experiment, i.e., loading data, creating matrices, performing train-test split, model fitting, and evaluation.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse matrix algebra to calculate the parameter values of a linear regression\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g80f8d50fa5bf6e890812cf951e88626b","title":"Ridge and Lasso Regression","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ridge-and-lasso-regression\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eAt this point, you've seen a number of criteria and algorithms for fitting regression models to data. You've seen the simple linear regression using ordinary least squares, and its more general regression of polynomial functions. You've also seen how we can overfit models to data using polynomials and interactions. With all of that, you began to explore other tools to analyze this general problem of overfitting versus underfitting, all this using training and test splits, bias and variance, and cross validation.\u003c/p\u003e\n\n\u003cp\u003eNow you're going to take a look at another way to tune the models you create. These methods all modify the mean squared error function that you are optimizing against. The modifications will add a penalty for large coefficient weights in the resulting model.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine lasso regression \u003c/li\u003e\n\u003cli\u003eDefine ridge regression \u003c/li\u003e\n\u003cli\u003eDescribe why standardization is necessary before ridge and lasso regression \u003c/li\u003e\n\u003cli\u003eCompare and contrast lasso, ridge, and non-regularized regression \u003c/li\u003e\n\u003cli\u003eUse lasso and ridge regression with scikit-learn \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7ff4049e3a4478aad77ba3a1b80487cc","title":"Ridge and Lasso Regression - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-ridge-and-lasso-regression-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ridge-and-lasso-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice your knowledge of ridge and lasso regression!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse lasso and ridge regression with scikit-learn \u003c/li\u003e\n\u003cli\u003eCompare and contrast lasso, ridge and non-regularized regression \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g4bde6387fe8446d8a78599461096f6fa","title":"ROC Curves and AUC","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-roc-curves-and-auc\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThis lesson will introduce ROC: Receiver Operating Characteristic curves and AUC: Area Under [the] Curve.\u003c/p\u003e\n\n\u003cp\u003eSome of the accuracy scores you've encountered thus far probably seem pretty impressive; an 80% accuracy seems pretty darn good on the first try! What you have to keep in mind is that for binary classification, you are bound to be right sometimes, even just by random guessing. For example, a person should be roughly 50% accurate in guessing whether or not a coin lands on heads. This also can lead to issues when tuning models down the road. If you have a skewed dataset with rare events (such as a disease or winning the lottery) where there are only 2 positive cases in 1000, then even a trivial algorithm that classifies everything as 'not a member' will achieve an accuracy of 99.8% (998 out of 1000 times it was correct). So remember that an 80% accuracy must be taken into account in a larger context. AUC is an alternative comprehensive metric to confusion matrices, and ROC graphs allow us to determine optimal precision-recall tradeoff balances specific to the problem you are looking to solve.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine ROC curves and AUC\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eExplain how ROC and AUC are used to evaluate and choose models \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g7dd679196a289cb69567a310143e0a69","title":"ROC Curves and AUC - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-roc-curves-and-auc-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-roc-curves-and-auc-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll practice drawing ROC graphs, calculating AUC, and interpreting these results. In doing so, you will also further review logistic regression, by briefly fitting a model as in a standard data science pipeline.\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a visualization of ROC curves and use it to assess a model \u003c/li\u003e\n\u003cli\u003eEvaluate classification models using the evaluation metrics appropriate for a specific problem \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g240536dee629c9b857d210808dac71ec","title":"Rules for Derivatives","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-rules-for-derivatives\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn the previous lesson, we calculated the derivative by changing our delta to see the convergence around a number as reflected in the table above.  However, mathematicians have derived shortcuts to calculate the derivative. \nYou'll learn about these shortcuts in this lesson!\u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCalculate derivatives of more complex functions by using power rules, constant factor and the addition rule\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"geae69a859277d4635ccd9f62b82d1703","title":"Rules for Derivatives - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-rules-for-derivatives-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfea549a153a77fb9593814c7ea567a9d","title":"Scalars, Vectors, Matrices, and Tensors - Code Along","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-scalars-vectors-matrices-tensors-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-scalars-vectors-matrices-tensors-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll be introduced to the basic mathematical entities used in linear algebra. We'll also look at how these entities are created (and later manipulated) in Python using NumPy. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCompare scalars, vectors, matrices, and tensors \u003c/li\u003e\n\u003cli\u003eCreate vectors and matrices using Numpy and Python\u003c/li\u003e\n\u003cli\u003eUse the transpose method to transpose Numpy matrices \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8345898d9a24b82069d1f41709ee0fdd","title":"Solving Systems of Linear Equations with NumPy - Code Along","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lineq-numpy-codealong\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lesson, you'll learn how to solve a system of linear equations using matrix algebra and Numpy.  You'll also learn about the identity matrix and inverse matrices, which have some unique properties that can be used to solve for unknown values in systems of linear equations. You'll also learn how to create these using Numpy. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eDefine the identity matrix and its dot product \u003c/li\u003e\n\u003cli\u003eDefine the inverse of a matrix \u003c/li\u003e\n\u003cli\u003eCalculate the inverse of a matrix in order to solve linear problems \u003c/li\u003e\n\u003cli\u003eUse matrix algebra and Numpy to solve a system of linear equations given a real-life example \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g601c542580c65fb6de1d04b8bb0a30ef","title":"Solving Systems of Linear Equations with NumPy - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lineq-numpy-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lineq-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eNow you've gathered all the required skills needed to solve systems of linear equations. You saw why there was a need to calculate inverses of matrices, followed by matrix multiplication to figure out the values of unknown variables. \u003c/p\u003e\n\n\u003cp\u003eThe exercises in this lab present some problems that can be converted into a system of linear equations. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eYou will be able to:\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eUse matrix algebra and NumPy to solve a system of linear equations given a real-life example \u003c/li\u003e\n\u003cli\u003eUse NumPy's linear algebra solver to solve for systems of linear equations\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g6cafbb465925915959d5223bf0942753","title":"Systems of Linear Equations - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-lingalg-linear-equations-quiz\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-lingalg-linear-equations-quiz/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eThe following scenarios present problems that can be solved as a system of equations while performing substitutions and eliminations as you saw in the previous lesson.\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eSolve these problems by hand, showing all the steps to work out the unknown variable values \u003c/li\u003e\n\u003cli\u003eVerify your answers by showing the calculated values satisfy all equations\u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcbdd65aa18e5b390121d84ab623eec90","title":"Tree Ensembles and Random Forests - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-tree-ensembles-random-forests-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-tree-ensembles-random-forests-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, we'll create some popular tree ensemble models such as a bag of trees and random forest to predict a person's salary based on information about them. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will: \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eTrain a random forest model using \u003ccode\u003escikit-learn\u003c/code\u003e\u003cbr\u003e\n\u003c/li\u003e\n\u003cli\u003eAccess, visualize, and interpret feature importances from an ensemble model \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g607f57af8b1267deb9b5c49c5c1b7ea2","title":"Vector Addition and Broadcasting in NumPy - Code Along","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-addition-codealong/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0760252c10be6de4644b3386a48bc65b","title":"Vectors and Matrices in Numpy - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-linalg-vector-matrices-numpy-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfc4cabc49a2b5825bcd2280f966121f2","title":"Visualizing Confusion Matrices - Lab","type":"Assignment","content":"\u003cdiv id=\"git-data-element\" data-org=\"learn-co-curriculum\" data-repo=\"dsc-visualizing-confusion-matrices-lab\"\u003e\u003c/div\u003e\n\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-visualizing-confusion-matrices-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\u003cp\u003eIn this lab, you'll build upon the previous lesson on confusion matrices and visualize a confusion matrix using \u003ccode\u003ematplotlib\u003c/code\u003e. \u003c/p\u003e\n\n\u003ch2\u003eObjectives\u003c/h2\u003e\n\n\u003cp\u003eIn this lab you will:  \u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003eCreate a confusion matrix from scratch \u003c/li\u003e\n\u003cli\u003eCreate a confusion matrix using scikit-learn \u003c/li\u003e\n\u003cli\u003eCraft functions that visualize confusion matrices \u003c/li\u003e\n\u003c/ul\u003e","submissionTypes":"an external tool","graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"ge3ed4d45a85151cc7115a93f81746615","title":"XGBoost - Lab","type":"Assignment","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-xgboost-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003c/header\u003e\n\u003cp\u003eComplete this lab on your local computer.\u003c/p\u003e","submissionTypes":null,"graded":true,"pointsPossible":0.0,"dueAt":null,"lockAt":null,"unlockAt":null}],"discussion_topics":[],"quizzes":[{"exportId":"gb3db5cbf0bcdb11aabfa93891bfac5b0","title":"Calculus and Cost Functions Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"g07c50903bfd70ff6e7919d625878acc2","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc53fdbad8c7fa83c61a7b59363bb80bf","title":"Gradient Descent Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"gc49c8298a982b1ac6bd1c70fa4c89133","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g63baf40c53d2206ad7b65a1501c08cbf","title":" Inference vs. Prediction Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"g4287b0c35fcf1baaaa7444da6d55b434","questionCount":7,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g62b3e5e4cbc8970111617a88cdab7801","title":"Linear Algebra Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"g12d88cc7c0173532794428b3f141a2df","questionCount":7,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g211cf52a1b09af29717a39b460ae425d","title":"Logistic Regression 1 Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"gd6d3c3e505fd36534136395582d2a713","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0ec8da043796b87f5899023655c90b12","title":"Logistic Regression 2 Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"ga65380dbf7600a45461766bdf5eabbb9","questionCount":9,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g307f021a855d355fe1b11925047cb804","title":"‚≠êÔ∏è Logistic Regression - Cumulative Lab","type":"Quizzes::Quiz","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-logistic-regression-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure what to do, refer to the instructions in \u003ca title=\"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\" href=\"quizzes/g9d5b89d09608ee0da642cc68cf19da0e\"\u003e‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\u003c/a\u003e\u003c/p\u003e","assignmentExportId":"gaee4522395e47c76dbf067b362bf6eff","questionCount":1,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g9d5b89d09608ee0da642cc68cf19da0e","title":"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab","type":"Quizzes::Quiz","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-ml-fundamentals-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure what to do, refer to the instructions in \u003ca title=\"‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\" href=\"quizzes/g0c9a5bdde5f8af00ba4e30f35458c335\"\u003e‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab\u003c/a\u003e\u003c/p\u003e","assignmentExportId":"g5ebdcf9e39a987128ef740ce30aa0d03","questionCount":1,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g343df895fe21388a003ee4f3f256d3e0","title":"Model Validation and Data Leakage Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"g403c51f0a915f7bf1d4798f721ac611c","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g1056c04a78eb2b8ee77b244c23859397","title":"‚≠êÔ∏è Nonparametric ML Models - Cumulative Lab","type":"Quizzes::Quiz","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nonparametric-models-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-nonparametric-models-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure what to do, refer to \u003ca title=\"‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\" href=\"quizzes/g9d5b89d09608ee0da642cc68cf19da0e\"\u003e‚≠êÔ∏è Machine Learning Fundamentals - Cumulative Lab\u003c/a\u003e\u003c/p\u003e","assignmentExportId":"g319dbdc14fa91ed73faa8241ef6a5199","questionCount":1,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g2dc61175cd9d50f14d0654fc91001d78","title":"OOP with Scikit-Learn Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"g2a8a68e2b1d6f93063734ba508d417fa","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g0c9a5bdde5f8af00ba4e30f35458c335","title":"‚≠êÔ∏è Preprocessing with scikit-learn - Cumulative Lab","type":"Quizzes::Quiz","content":"\u003cheader class=\"fis-header\" style=\"visibility: hidden;\"\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sklearn-preprocessing-lab\" target=\"_blank\"\u003e\u003cimg id=\"repo-img\" title=\"Open GitHub Repo\" alt=\"GitHub Repo\"\u003e\u003c/a\u003e\u003ca class=\"fis-git-link\" href=\"https://github.com/learn-co-curriculum/dsc-sklearn-preprocessing-lab/issues/new\" target=\"_blank\"\u003e\u003cimg id=\"issue-img\" title=\"Create New Issue\" alt=\"Create New Issue\"\u003e\u003c/a\u003e\u003c/header\u003e\n\u003cp\u003eWork on this lab on your local computer. If you're not sure how to clone the source code, refer to the instructions here: \u003ca href=\"https://github.com/learn-co-curriculum/dsc-running-jupyter-locally\" target=\"_blank\"\u003ehttps://github.com/learn-co-curriculum/dsc-running-jupyter-locally\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003eSubmission Instructions\u003c/h3\u003e\n\u003cp\u003eWhen you are finished with the lab, complete the following steps to submit your work:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSave the changes to the notebook by clicking the Save icon, shown below highlighted in red\u003cbr\u003e\u003cimg src=\"https://learning.flatironschool.com/courses/5539/file_contents/viewer/files/Uploaded%20Media/Screen%20Shot%202021-07-28%20at%205.41.06%20PM.png\" alt=\"Screen Shot 2021-07-28 at 5.41.06 PM.png\"\u003e\u0026nbsp;\u003c/li\u003e\n\u003cli\u003eClose the notebook browser tab(s)\u003c/li\u003e\n\u003cli\u003eShut down the notebook server by typing control-C in the terminal window where it is currently running\u003c/li\u003e\n\u003cli\u003eCommit your changes in Git by typing \u003cbr\u003e\u003ccode\u003egit commit -am \"Finished lab\"\u003c/code\u003e \u003cbr\u003ein the terminal and hitting Enter\u003c/li\u003e\n\u003cli\u003ePush your changes to GitHub by typing \u003cbr\u003e\u003ccode\u003egit push origin master\u003c/code\u003e \u003cbr\u003ein the terminal and hitting Enter\u003c/li\u003e\n\u003cli\u003eOpen the GitHub view of your fork of the lab in the browser. For example, if your username were \u003ccode\u003ehoffm386\u003c/code\u003e, you would go to \u003ca href=\"https://github.com/hoffm386/dsc-data-serialization-lab\" target=\"_blank\"\u003ehttps://github.com/hoffm386/dsc-data-serialization-lab\u003c/a\u003e in the browser for this particular lab. Click on \u003ccode\u003eindex.ipynb\u003c/code\u003e and double-check that your code updates are there. (The updates will not be in the README, only in the \u003ccode\u003e.ipynb\u003c/code\u003e file.)\u003c/li\u003e\n\u003cli\u003eSubmit the link to your fork of the lab in the textbox on Canvas\u003cbr\u003e\u0026nbsp;\u003cimg src=\"https://learning.flatironschool.com/courses/5539/file_contents/viewer/files/Uploaded%20Media/Screen%20Shot%202021-08-24%20at%206.42.54%20PM.png\" alt=\"Screen Shot 2021-08-24 at 6.42.54 PM.png\" width=\"319\" height=\"320\"\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003eTroubleshooting\u003c/h3\u003e\n\u003cp\u003e\u003cspan style=\"background-color: #fbeeb8;\"\u003eIf you are able to submit the URL successfully, you do not need to follow the below steps!\u003c/span\u003e\u003c/p\u003e\n\u003ch4\u003eNot a Git Repository\u003c/h4\u003e\n\u003cp\u003eIf you try to run \u003ccode\u003egit commit -am \"Finished lab\"\u003c/code\u003e and get the error message \u003ccode\u003efatal: not a git repository\u003c/code\u003e, double-check that you are running the code from the correct directory. If you type \u003ccode\u003epwd\u003c/code\u003e in the terminal and hit Enter, the path that is printed out should include the directory of the lab ‚Äî in this case, \u003ccode\u003edsc-data-serialization-lab\u003c/code\u003e. For example, a valid path would be \u003ccode\u003e/Users/myname/Development/DS/dsc-data-serialization-lab\u003c/code\u003e, since that ends with the lab directory, whereas \u003ccode\u003e/Users/myname/Development/DS/\u003c/code\u003e would not be a valid path. Use commands like \u003ccode\u003els\u003c/code\u003e and \u003ccode\u003ecd\u003c/code\u003e to navigate to the appropriate directory, then continue with the steps above, starting with step 4.\u003c/p\u003e\n\u003ch4\u003ePermission Denied\u003c/h4\u003e\n\u003cp\u003eIf you try to run \u003ccode\u003egit push origin master\u003c/code\u003e and get a \u003ccode\u003ePermission denied\u003c/code\u003e error message, you are likely trying to push to the curriculum version of the lab, not your personal fork. Follow these steps to fix this:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIn the browser, go to the curriculum repository for this lab by clicking the \u003cimg src=\"viewer/files/Uploaded%20Media/GitHub-Mark-32px.png\" alt=\"GitHub octocat logo\"\u003e\u0026nbsp;icon above\u003c/li\u003e\n\u003cli\u003eClick the Fork button. If you already have a fork, this will take you to it. If you haven't made a fork yet, this will make the fork and take you to it\u003c/li\u003e\n\u003cli\u003eOn the page of your fork, copy the clone link. For example, \u003ca href=\"https://github.com/hoffm386/dsc-data-serialization-lab.git\" target=\"_blank\"\u003ehttps://github.com/hoffm386/dsc-data-serialization-lab.git\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eBack in the terminal where you were trying to run \u003ccode\u003egit push\u003c/code\u003e, type \u003cbr\u003e\u003ccode\u003egit remote add myfork \u0026lt;URL\u0026gt;\u003c/code\u003e \u003cbr\u003eWhere \u003ccode\u003e\u0026lt;URL\u0026gt;\u003c/code\u003e is replaced with the clone link you copied. For example, \u003ccode\u003egit remote add myfork https://github.com/hoffm386/dsc-data-serialization-lab.git\u003c/code\u003e. Then hit Enter. This means you have created a connection between your local repository and your fork\u003c/li\u003e\n\u003cli\u003eNow, push your code to your fork by typing \u003cbr\u003e\u003ccode\u003egit push myfork master\u003c/code\u003e \u003cbr\u003ein the terminal and hitting Enter\u003c/li\u003e\n\u003cli\u003eProceed with the steps above, starting with step 6\u003c/li\u003e\n\u003c/ol\u003e","assignmentExportId":"gaefb27c067a8c8a58e822cf9b93b1236","questionCount":1,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g072d490545d2ff5e9675b66373a4d996","title":"Quiz: Bayes Classification","type":"Quizzes::Quiz","content":"","assignmentExportId":"g42112b49608a7d1b46550bab97255d53","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gfbc831faf7b76124ddf50175a5d466d8","title":"Quiz: Classification Metrics","type":"Quizzes::Quiz","content":"","assignmentExportId":"g878a100f8835986ec288696d10002046","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc3c333700dc3d9b001b40f75e7ede459","title":"Quiz: Decision Trees","type":"Quizzes::Quiz","content":"","assignmentExportId":"g2773bcc7ce99bf018cdde53dbe3c8ce2","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g01b43860f608fdfa4d009588e4178c7e","title":"Quiz: Ensemble Methods","type":"Quizzes::Quiz","content":"","assignmentExportId":"g75d013b6043be830c29dacb4eb7a48ce","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g8f0af8fb6b9743cf18b550abf04e6f7f","title":"Quiz: Introduction to Logistic Regression","type":"Quizzes::Quiz","content":"","assignmentExportId":"g66f71747bc93011e9c6c3c1f31f08f37","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g041301c0d6f44f9e19f87488351cf805","title":"Quiz: K Nearest Neighbors","type":"Quizzes::Quiz","content":"","assignmentExportId":"g68b1b16cb45b0c2602ed03edb7f3ef56","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gc405785850874cf6a53b7923e4458e4d","title":"Quiz: Linear Algebra","type":"Quizzes::Quiz","content":"","assignmentExportId":"g4a3f882d37f3810875ee6486149a8c35","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gde2a7ea9ada6166586c9cf787822bfba","title":"Quiz: Machine Learning Fundamentals","type":"Quizzes::Quiz","content":"","assignmentExportId":"g9fcf8c2214c578d100cc0bec5e35577c","questionCount":5,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gcade38300ac325cdd762ba3921374c78","title":"Quiz: Object-Oriented Programming","type":"Quizzes::Quiz","content":"","assignmentExportId":"g78dbcc31faa2ce4fe50021d6665dec53","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb7d6a7cdf1300cc8cb899aa98c8bc385","title":"Quiz: Regularization","type":"Quizzes::Quiz","content":"","assignmentExportId":"gb2c59f56f7a0911bf496fbe6b43a0eaf","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"gb7c0c54f1a48881f9ef20382bb2f26a5","title":"Quiz: Support Vector Machines","type":"Quizzes::Quiz","content":"","assignmentExportId":"g60e839b12dfaa060967cee6c080f0eda","questionCount":5,"timeLimit":null,"attempts":-1,"graded":true,"pointsPossible":5.0,"dueAt":null,"lockAt":null,"unlockAt":null},{"exportId":"g15774678425387b9fbeab8a27d51575f","title":"Regularization Exit Ticket","type":"Quizzes::Quiz","content":"","assignmentExportId":"gc88411b07bef5b5aa7ea7b53912ae392","questionCount":8,"timeLimit":null,"attempts":1,"graded":true,"pointsPossible":1.0,"dueAt":null,"lockAt":null,"unlockAt":null}],"files":[{"type":"folder","name":"Uploaded Media","size":null,"files":[{"type":"file","name":"GitHub-Mark-32px.png","size":1714,"files":null}]},{"type":"folder","name":"course_image","size":null,"files":[]}]}